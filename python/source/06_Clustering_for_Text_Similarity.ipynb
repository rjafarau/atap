{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d155d0c9-07de-4d4a-89c3-a50d79d0b6f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a11977d5-34d4-4904-a314-5833f284e914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pathlib\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca23613f-6d7c-4260-b654-4dcd1bedd9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c83731c8-b2fc-4424-ba16-06c6f31d7f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d538c387-3143-4868-b580-1e3afb69a05a",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90114fd6-844a-4c88-b89e-92af8c3a8817",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = pathlib.Path('~/project').expanduser()\n",
    "DATA_DIR = PROJECT_DIR / 'data'\n",
    "CORPUS_ROOT = DATA_DIR / 'sample'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb79b384-59ae-4948-a6b0-bf78e96a9d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.api import CorpusReader\n",
    "from nltk.corpus.reader.api import CategorizedCorpusReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b6e5e7-b384-4f1d-83c9-76cbc7534969",
   "metadata": {},
   "source": [
    "# PickledCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4e5c430-9a8b-4dd6-be27-a8d8da6aa5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_PATTERN = r'(?!\\.)[a-z_\\s]+/[a-f0-9]+\\.json'\n",
    "PKL_PATTERN = r'(?!\\.)[a-z_\\s]+/[a-f0-9]+\\.pickle'\n",
    "CAT_PATTERN = r'([a-z_\\s]+)/.*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "901a579c-59f7-410c-a8b7-d1325fb6f81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PickledCorpusReader(CategorizedCorpusReader, CorpusReader):\n",
    "\n",
    "    def __init__(self, root, fileids=PKL_PATTERN, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the corpus reader.  Categorization arguments\n",
    "        (``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to\n",
    "        the ``CategorizedCorpusReader`` constructor.  The remaining arguments\n",
    "        are passed to the ``CorpusReader`` constructor.\n",
    "        \"\"\"\n",
    "        # Add the default category pattern if not passed into the class.\n",
    "        if not any(key.startswith('cat_') for key in kwargs.keys()):\n",
    "            kwargs['cat_pattern'] = CAT_PATTERN\n",
    "\n",
    "        CategorizedCorpusReader.__init__(self, kwargs)\n",
    "        CorpusReader.__init__(self, root, fileids)\n",
    "\n",
    "    def _resolve(self, fileids, categories):\n",
    "        \"\"\"\n",
    "        Returns a list of fileids or categories depending on what is passed\n",
    "        to each internal corpus reader function. This primarily bubbles up to\n",
    "        the high level ``docs`` method, but is implemented here similar to\n",
    "        the nltk ``CategorizedPlaintextCorpusReader``.\n",
    "        \"\"\"\n",
    "        if fileids is not None and categories is not None:\n",
    "            raise ValueError(\"Specify fileids or categories, not both\")\n",
    "\n",
    "        if categories is not None:\n",
    "            return self.fileids(categories)\n",
    "        return fileids\n",
    "\n",
    "    def docs(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns the document loaded from a pickled object for every file in\n",
    "        the corpus. Similar to the BaleenCorpusReader, this uses a generator\n",
    "        to acheive memory safe iteration.\n",
    "        \"\"\"\n",
    "        # Resolve the fileids and the categories\n",
    "        fileids = self._resolve(fileids, categories)\n",
    "\n",
    "        # Create a generator, loading one document into memory at a time.\n",
    "        for path, enc, fileid in self.abspaths(fileids, True, True):\n",
    "            with open(path, 'rb') as f:\n",
    "                yield pickle.load(f)\n",
    "\n",
    "    def paras(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of paragraphs where each paragraph is a list of\n",
    "        sentences, which is in turn a list of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for doc in self.docs(fileids, categories):\n",
    "            for paragraph in doc:\n",
    "                yield paragraph\n",
    "\n",
    "    def sents(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of sentences where each sentence is a list of\n",
    "        (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for paragraph in self.paras(fileids, categories):\n",
    "            for sentence in paragraph:\n",
    "                yield sentence\n",
    "\n",
    "    def tagged(self, fileids=None, categories=None):\n",
    "        for sent in self.sents(fileids, categories):\n",
    "            for token, tag in sent:\n",
    "                yield token, tag\n",
    "\n",
    "    def words(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for token, tag in self.tagged(fileids, categories):\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a485c3bf-1474-429f-8a16-e3a6dbf71575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58,748 vocabulary 1,624,862 word count\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "reader = PickledCorpusReader(root=CORPUS_ROOT.as_posix())\n",
    "words  = Counter(reader.words())\n",
    "\n",
    "print(f\"{len(words.keys()):,} vocabulary {sum(words.values()):,} word count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "877a5203-a04e-4dba-a745-17a809732563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 'books' contains 71 docs and 41,438 words\n",
      "- 'business' contains 389 docs and 222,182 words\n",
      "- 'cinema' contains 100 docs and 69,153 words\n",
      "- 'cooking' contains 30 docs and 37,854 words\n",
      "- 'data_science' contains 41 docs and 31,354 words\n",
      "- 'design' contains 55 docs and 18,260 words\n",
      "- 'do_it_yourself' contains 122 docs and 28,050 words\n",
      "- 'gaming' contains 128 docs and 70,778 words\n",
      "- 'news' contains 1,159 docs and 850,688 words\n",
      "- 'politics' contains 149 docs and 88,853 words\n",
      "- 'sports' contains 118 docs and 68,884 words\n",
      "- 'tech' contains 176 docs and 97,368 words\n"
     ]
    }
   ],
   "source": [
    "for category in reader.categories():\n",
    "\n",
    "    n_docs = len(reader.fileids(categories=[category]))\n",
    "    n_words = sum(1 for word in reader.words(categories=[category]))\n",
    "\n",
    "    print(\"- '{}' contains {:,} docs and {:,} words\".format(category, n_docs, n_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58839040-0091-4540-9e14-966ab7bb5a53",
   "metadata": {},
   "source": [
    "# Unsupervised Learning on Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2309f3-7910-471c-b0bc-0176c4ffeaad",
   "metadata": {},
   "source": [
    "# Clustering by Document Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c564bcb8-f600-4034-8743-18d13500a4b9",
   "metadata": {},
   "source": [
    "## Distance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c152cc-5d99-4134-91d7-847741ffe254",
   "metadata": {},
   "source": [
    "## Partitive Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e75ef7-a417-486f-a462-ba30d713707a",
   "metadata": {},
   "source": [
    "### k-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6bd8d61-8be3-4145-8921-ea0e8bac232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.cluster import KMeansClusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d298b83-ff40-45bf-8445-b7d5d0bcd5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(nltk.corpus.stopwords.words('english'))\n",
    "lemmatizer = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c97fb4e7-a153-4251-a242-abef23341a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_punct(token):\n",
    "    # Is every character punctuation?\n",
    "    return all(\n",
    "        unicodedata.category(char).startswith('P')\n",
    "        for char in token\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d4c6dbc-b6f3-4aee-ae44-c43d29ff2c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wnpos(tag):\n",
    "    # Return the WordNet POS tag from the Penn Treebank tag\n",
    "    return {\n",
    "        'N': wn.NOUN,\n",
    "        'V': wn.VERB,\n",
    "        'R': wn.ADV,\n",
    "        'J': wn.ADJ\n",
    "    }.get(tag[0], wn.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c7f1fa0-d1cb-450a-8caa-f5bd42ce9973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(document, stopwords=STOPWORDS):\n",
    "    \"\"\"\n",
    "    Removes stopwords and punctuation, lowercases, lemmatizes\n",
    "    \"\"\"\n",
    "\n",
    "    for token, tag in document:\n",
    "        token = token.lower().strip()\n",
    "\n",
    "        if is_punct(token) or (token in stopwords):\n",
    "            continue\n",
    "\n",
    "        yield lemmatizer.lemmatize(token, wnpos(tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd514fb1-3973-444a-b61d-15f9469bb685",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansTopics(object):\n",
    "\n",
    "    def __init__(self, corpus, k=10):\n",
    "        \"\"\"\n",
    "        corpus is a corpus object, e.g. an HTMLCorpusReader()\n",
    "        or an HTMLPickledCorpusReader() object\n",
    "\n",
    "        k is the number of clusters\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.model = None\n",
    "        self.vocab = list(\n",
    "            set(normalize(corpus.words(categories=['news'])))\n",
    "        )\n",
    "\n",
    "    def vectorize(self, document):\n",
    "        \"\"\"\n",
    "        Vectorizes a document consisting of a list of part of speech\n",
    "        tagged tokens using the segmentation and tokenization methods.\n",
    "\n",
    "        One-hot encode the set of documents\n",
    "        \"\"\"\n",
    "        features = set(normalize(document))\n",
    "        return np.array(\n",
    "            [token in features for token in self.vocab],\n",
    "            np.short\n",
    "        )\n",
    "\n",
    "    def cluster(self, corpus):\n",
    "        \"\"\"\n",
    "        Fits the K-Means model to the given data.\n",
    "        \"\"\"\n",
    "        self.model = KMeansClusterer(\n",
    "            num_means=self.k,\n",
    "            distance=nltk.cluster.util.cosine_distance, \n",
    "            avoid_empty_clusters=True\n",
    "        )\n",
    "        self.model.cluster([\n",
    "            self.vectorize(corpus.words(fileid))\n",
    "            for fileid in corpus.fileids(categories=['news'])\n",
    "        ])\n",
    "\n",
    "    def classify(self, document):\n",
    "        \"\"\"\n",
    "        Pass through to the internal model classify\n",
    "        \"\"\"\n",
    "        return self.model.classify(self.vectorize(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b71d0cc-2a8a-4e68-a95c-178e98630c79",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_103/168741197.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPickledCorpusReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCORPUS_ROOT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_posix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclusterer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeansTopics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mclusterer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_103/2503441878.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, k)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         self.vocab = list(\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'news'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         )\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_103/1714504206.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(document, stopwords)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "corpus = PickledCorpusReader(root=CORPUS_ROOT.as_posix())\n",
    "\n",
    "clusterer = KMeansTopics(corpus, k=7)\n",
    "clusterer.cluster(corpus)\n",
    "\n",
    "# Classify documents in the new corpus by cluster affinity\n",
    "groups  = [\n",
    "    (clusterer.classify(corpus.words(fileid)), fileid)\n",
    "    for fileid in corpus.fileids(categories=['news'])\n",
    "]\n",
    "\n",
    "# Group documents in corpus by cluster and display them\n",
    "groups.sort(key=itemgetter(0))\n",
    "for group, items in groupby(groups, key=itemgetter(0)):\n",
    "    for cluster, fname in items:\n",
    "        print(\"Cluster {}: {}\".format(cluster+1,fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5977dc8-5949-428f-ba2c-68d4b961016e",
   "metadata": {},
   "source": [
    "### Optimizing k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b78c038-9ec6-4a10-bd64-1f02dde40222",
   "metadata": {},
   "source": [
    "### Handling uneven geometries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d15f2c-493b-47e4-9016-bf5a6a7bc3b2",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6dd214-ef12-4ad5-8499-78b1e909eded",
   "metadata": {},
   "source": [
    "### Agglomerative clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854533e9-d2f5-439c-864f-5947a73940fe",
   "metadata": {},
   "source": [
    "# Modeling Document Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aef3ab-998f-4409-956a-7ff71643fa72",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f8c64f-c6f6-4852-a743-0e26c32cf9c8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### In Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b02c56-5481-441d-9961-5888d4619d86",
   "metadata": {},
   "source": [
    "### The Gensim way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc8c7f1-0222-41fa-b70e-c4a5fb67f2f4",
   "metadata": {},
   "source": [
    "### Visualizing topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dd32f4-70b6-43dc-96ff-d38f533edd7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e435866-eb8f-4a66-9bed-cf106dd08e7c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### In Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb13fba0-f254-469a-a7ec-c0915d8aa1bd",
   "metadata": {},
   "source": [
    "### The Gensim way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ebffde-fd7c-453c-a42b-138c9ae1a28b",
   "metadata": {},
   "source": [
    "## Non-Negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2f1a6e-d62c-4995-8504-1959d84ead40",
   "metadata": {},
   "source": [
    "### In Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba9da19-1412-425d-a3e8-b74a2ad94a16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
