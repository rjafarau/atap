{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d155d0c9-07de-4d4a-89c3-a50d79d0b6f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a11977d5-34d4-4904-a314-5833f284e914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import pathlib\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca23613f-6d7c-4260-b654-4dcd1bedd9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c83731c8-b2fc-4424-ba16-06c6f31d7f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d538c387-3143-4868-b580-1e3afb69a05a",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90114fd6-844a-4c88-b89e-92af8c3a8817",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = pathlib.Path('~/project').expanduser()\n",
    "DATA_DIR = PROJECT_DIR / 'data'\n",
    "CORPUS_ROOT = DATA_DIR / 'sample'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb79b384-59ae-4948-a6b0-bf78e96a9d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.api import CorpusReader\n",
    "from nltk.corpus.reader.api import CategorizedCorpusReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b6e5e7-b384-4f1d-83c9-76cbc7534969",
   "metadata": {},
   "source": [
    "# PickledCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4e5c430-9a8b-4dd6-be27-a8d8da6aa5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_PATTERN = r'(?!\\.)[a-z_\\s]+/[a-f0-9]+\\.json'\n",
    "PKL_PATTERN = r'(?!\\.)[a-z_\\s]+/[a-f0-9]+\\.pickle'\n",
    "CAT_PATTERN = r'([a-z_\\s]+)/.*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "901a579c-59f7-410c-a8b7-d1325fb6f81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PickledCorpusReader(CategorizedCorpusReader, CorpusReader):\n",
    "\n",
    "    def __init__(self, root, fileids=PKL_PATTERN, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the corpus reader.  Categorization arguments\n",
    "        (``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to\n",
    "        the ``CategorizedCorpusReader`` constructor.  The remaining arguments\n",
    "        are passed to the ``CorpusReader`` constructor.\n",
    "        \"\"\"\n",
    "        # Add the default category pattern if not passed into the class.\n",
    "        if not any(key.startswith('cat_') for key in kwargs.keys()):\n",
    "            kwargs['cat_pattern'] = CAT_PATTERN\n",
    "\n",
    "        CategorizedCorpusReader.__init__(self, kwargs)\n",
    "        CorpusReader.__init__(self, root, fileids)\n",
    "\n",
    "    def _resolve(self, fileids, categories):\n",
    "        \"\"\"\n",
    "        Returns a list of fileids or categories depending on what is passed\n",
    "        to each internal corpus reader function. This primarily bubbles up to\n",
    "        the high level ``docs`` method, but is implemented here similar to\n",
    "        the nltk ``CategorizedPlaintextCorpusReader``.\n",
    "        \"\"\"\n",
    "        if fileids is not None and categories is not None:\n",
    "            raise ValueError(\"Specify fileids or categories, not both\")\n",
    "\n",
    "        if categories is not None:\n",
    "            return self.fileids(categories)\n",
    "        return fileids\n",
    "\n",
    "    def docs(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns the document loaded from a pickled object for every file in\n",
    "        the corpus. Similar to the BaleenCorpusReader, this uses a generator\n",
    "        to acheive memory safe iteration.\n",
    "        \"\"\"\n",
    "        # Resolve the fileids and the categories\n",
    "        fileids = self._resolve(fileids, categories)\n",
    "\n",
    "        # Create a generator, loading one document into memory at a time.\n",
    "        for path, enc, fileid in self.abspaths(fileids, True, True):\n",
    "            with open(path, 'rb') as f:\n",
    "                yield pickle.load(f)\n",
    "\n",
    "    def paras(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of paragraphs where each paragraph is a list of\n",
    "        sentences, which is in turn a list of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for doc in self.docs(fileids, categories):\n",
    "            for paragraph in doc:\n",
    "                yield paragraph\n",
    "\n",
    "    def sents(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of sentences where each sentence is a list of\n",
    "        (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for paragraph in self.paras(fileids, categories):\n",
    "            for sentence in paragraph:\n",
    "                yield sentence\n",
    "\n",
    "    def words(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for sent in self.sents(fileids, categories):\n",
    "            for token, tag in sent:\n",
    "                yield token, tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a485c3bf-1474-429f-8a16-e3a6dbf71575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77,930 vocabulary 1,624,862 word count\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "reader = PickledCorpusReader(root=CORPUS_ROOT.as_posix())\n",
    "words  = Counter(reader.words())\n",
    "\n",
    "print(f\"{len(words.keys()):,} vocabulary {sum(words.values()):,} word count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "877a5203-a04e-4dba-a745-17a809732563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 'books' contains 71 docs and 41,438 words\n",
      "- 'business' contains 389 docs and 222,182 words\n",
      "- 'cinema' contains 100 docs and 69,153 words\n",
      "- 'cooking' contains 30 docs and 37,854 words\n",
      "- 'data_science' contains 41 docs and 31,354 words\n",
      "- 'design' contains 55 docs and 18,260 words\n",
      "- 'do_it_yourself' contains 122 docs and 28,050 words\n",
      "- 'gaming' contains 128 docs and 70,778 words\n",
      "- 'news' contains 1,159 docs and 850,688 words\n",
      "- 'politics' contains 149 docs and 88,853 words\n",
      "- 'sports' contains 118 docs and 68,884 words\n",
      "- 'tech' contains 176 docs and 97,368 words\n"
     ]
    }
   ],
   "source": [
    "for category in reader.categories():\n",
    "\n",
    "    n_docs = len(reader.fileids(categories=[category]))\n",
    "    n_words = sum(1 for word in reader.words(categories=[category]))\n",
    "\n",
    "    print(\"- '{}' contains {:,} docs and {:,} words\".format(category, n_docs, n_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58839040-0091-4540-9e14-966ab7bb5a53",
   "metadata": {},
   "source": [
    "# Unsupervised Learning on Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2309f3-7910-471c-b0bc-0176c4ffeaad",
   "metadata": {},
   "source": [
    "# Clustering by Document Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c564bcb8-f600-4034-8743-18d13500a4b9",
   "metadata": {},
   "source": [
    "## Distance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c152cc-5d99-4134-91d7-847741ffe254",
   "metadata": {},
   "source": [
    "## Partitive Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e75ef7-a417-486f-a462-ba30d713707a",
   "metadata": {},
   "source": [
    "### k-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6bd8d61-8be3-4145-8921-ea0e8bac232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.cluster import KMeansClusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d298b83-ff40-45bf-8445-b7d5d0bcd5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(nltk.corpus.stopwords.words('english'))\n",
    "lemmatizer = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c97fb4e7-a153-4251-a242-abef23341a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_punct(token):\n",
    "    # Is every character punctuation?\n",
    "    return all(\n",
    "        unicodedata.category(char).startswith('P')\n",
    "        for char in token\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d4c6dbc-b6f3-4aee-ae44-c43d29ff2c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wnpos(tag):\n",
    "    # Return the WordNet POS tag from the Penn Treebank tag\n",
    "    return {\n",
    "        'N': wn.NOUN,\n",
    "        'V': wn.VERB,\n",
    "        'R': wn.ADV,\n",
    "        'J': wn.ADJ\n",
    "    }.get(tag[0], wn.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c7f1fa0-d1cb-450a-8caa-f5bd42ce9973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(document, stopwords=STOPWORDS):\n",
    "    \"\"\"\n",
    "    Removes stopwords and punctuation, lowercases, lemmatizes\n",
    "    \"\"\"\n",
    "\n",
    "    for token, tag in document:\n",
    "        token = token.lower().strip()\n",
    "\n",
    "        if is_punct(token) or (token in stopwords):\n",
    "            continue\n",
    "\n",
    "        yield lemmatizer.lemmatize(token, wnpos(tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd514fb1-3973-444a-b61d-15f9469bb685",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansTopics(object):\n",
    "\n",
    "    def __init__(self, corpus, k=10):\n",
    "        \"\"\"\n",
    "        corpus is a corpus object, e.g. an HTMLCorpusReader()\n",
    "        or an HTMLPickledCorpusReader() object\n",
    "\n",
    "        k is the number of clusters\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.model = None\n",
    "        self.vocab = list(\n",
    "            set(normalize(corpus.words(categories=['news'])))\n",
    "        )\n",
    "\n",
    "    def vectorize(self, document):\n",
    "        \"\"\"\n",
    "        Vectorizes a document consisting of a list of part of speech\n",
    "        tagged tokens using the segmentation and tokenization methods.\n",
    "\n",
    "        One-hot encode the set of documents\n",
    "        \"\"\"\n",
    "        features = set(normalize(document))\n",
    "        return np.array(\n",
    "            [token in features for token in self.vocab],\n",
    "            np.short\n",
    "        )\n",
    "\n",
    "    def cluster(self, corpus):\n",
    "        \"\"\"\n",
    "        Fits the K-Means model to the given data.\n",
    "        \"\"\"\n",
    "        self.model = KMeansClusterer(\n",
    "            num_means=self.k,\n",
    "            distance=nltk.cluster.util.cosine_distance, \n",
    "            avoid_empty_clusters=True,\n",
    "            rng=random.Random(42)\n",
    "        )\n",
    "        self.model.cluster([\n",
    "            self.vectorize(corpus.words(fileid))\n",
    "            for fileid in corpus.fileids(categories=['news'])\n",
    "        ])\n",
    "\n",
    "    def classify(self, document):\n",
    "        \"\"\"\n",
    "        Pass through to the internal model classify\n",
    "        \"\"\"\n",
    "        return self.model.classify(self.vectorize(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e4d7253-8150-43f4-9247-cfbbd0a13ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = PickledCorpusReader(root=CORPUS_ROOT.as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbb3a1d3-52c6-45ac-9ae6-0dcff23516d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.8 s, sys: 245 ms, total: 15 s\n",
      "Wall time: 17.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture\n",
    "clusterer = KMeansTopics(corpus, k=7)\n",
    "clusterer.cluster(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d2c43b9-495a-41db-adae-75d09a05ad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify documents in the new corpus by cluster affinity\n",
    "groups  = [\n",
    "    (clusterer.classify(corpus.words(fileid)), fileid)\n",
    "    for fileid in corpus.fileids(categories=['news'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7229fe62-1eef-4478-9d78-a8c4a7a75e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1: news/56d63af0c1808113ffb88745.pickle\n",
      "Cluster 2: news/56d6e677c1808118f8de4438.pickle\n",
      "Cluster 3: news/56d62570c1808113ffb87557.pickle\n",
      "Cluster 4: news/56d63a76c1808113ffb8841c.pickle\n",
      "Cluster 5: news/56d62554c1808113ffb87492.pickle\n",
      "Cluster 6: news/56d64c7ac1808115036122b4.pickle\n",
      "Cluster 7: news/56d6255dc1808113ffb874f0.pickle\n"
     ]
    }
   ],
   "source": [
    "# Group documents in corpus by cluster and display them\n",
    "groups.sort(key=itemgetter(0))\n",
    "for group, items in groupby(groups, key=itemgetter(0)):\n",
    "    for cluster, fname in items:\n",
    "        print(f\"Cluster {cluster + 1}: {fname}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcea944-9d4a-4873-a332-0ce625550fc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ALTERNATIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "413c854b-3a1b-4f7c-896e-025884c179fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3485cf93-7ecf-418c-8b82-210ba15dae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, language='english'):\n",
    "        self.language = language\n",
    "        self.stopwords = set(nltk.corpus.stopwords.words(language))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def is_punct(self, token):\n",
    "        return all(\n",
    "            unicodedata.category(char).startswith('P') for char in token\n",
    "        )\n",
    "\n",
    "    def is_stopword(self, token):\n",
    "        return token.lower() in self.stopwords\n",
    "\n",
    "    def normalize(self, document):\n",
    "        return [\n",
    "            self.lemmatize(token, tag).lower()\n",
    "            for paragraph in document\n",
    "            for sentence in paragraph\n",
    "            for (token, tag) in sentence\n",
    "            if not self.is_punct(token)\n",
    "            and not self.is_stopword(token)\n",
    "        ]\n",
    "\n",
    "    def lemmatize(self, token, pos_tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(pos_tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "    def fit(self, documents, labels=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        return [' '.join(self.normalize(doc)) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9dc5d104-44e3-42c2-8f9a-ecd04a981e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4ae16c1-a6fd-4842-93b3-39399b942f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCountVectorizer(CountVectorizer):\n",
    "    \"\"\"Customize transform() method\"\"\"\n",
    "    \n",
    "    def transform(self, raw_documents):\n",
    "        \"\"\"Returns list of numpy arrays\"\"\"\n",
    "        return list(super().transform(raw_documents).toarray())\n",
    "    \n",
    "    def fit_transform(self, raw_documents, y=None):\n",
    "        \"\"\"Fit and returns list of numpy arrays\"\"\"\n",
    "        return list(super().fit_transform(raw_documents, y).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f791819f-dee1-4ac9-8092-58014b9e2fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.cluster import KMeansClusterer\n",
    "from sklearn.base import BaseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d5a921e-bc7a-43ac-8c63-7b6d20da633a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansClusters(BaseEstimator):\n",
    "    \"\"\"sklearn wrapper for nltk.cluster.KMeansClusterer\"\"\"\n",
    "    \n",
    "    def __init__(self, k, distance, random_state=None):\n",
    "        \"\"\"\n",
    "        k is the number of clusters\n",
    "        model is the implementation of Kmeans\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.distance = distance\n",
    "        self.random_state = random_state\n",
    "        self.model = KMeansClusterer(num_means=self.k,\n",
    "                                     distance=self.distance,\n",
    "                                     rng=random.Random(self.random_state),\n",
    "                                     avoid_empty_clusters=True)\n",
    "        \n",
    "    def fit(self, documents, labels=None):\n",
    "        \"\"\"\n",
    "        Fits the K-Means model to one-hot vectorized documents.\n",
    "        \"\"\"\n",
    "        self.model.cluster(documents)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, documents):\n",
    "        \"\"\"\n",
    "        Predicts the closest cluster for each document in documents.\n",
    "        \"\"\"\n",
    "        return list(map(self.model.classify, documents))\n",
    "    \n",
    "    def fit_predict(self, documents, labels=None):\n",
    "        \"\"\"Calls fit() and predict() methods\"\"\"\n",
    "        return self.fit(documents).predict(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31c28d9e-7e8e-4386-894d-581bc07f2953",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2227fba0-3a7a-493e-94bd-8867eb51e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('normalizer', TextNormalizer()),\n",
    "    ('vectorizer', CustomCountVectorizer(lowercase=False,\n",
    "                                         binary=True,\n",
    "                                         dtype='short')),\n",
    "    ('clusters', KMeansClusters(k=7,\n",
    "                                distance=nltk.cluster.util.cosine_distance,\n",
    "                                random_state=42)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8cabc05b-bc14-4ba9-8aa1-2ad7c2ae8204",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = PickledCorpusReader(root=CORPUS_ROOT.as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79b71d16-2f5e-46f7-b623-2f3e395d784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = reader.docs(categories=['news'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7198288d-0463-4daa-b790-755a1ac9172b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.4 s, sys: 151 ms, total: 11.6 s\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture\n",
    "clusters = pipeline.fit_predict(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d9e23fd-6c66-4970-bc70-6c476a3b00cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify documents in the new corpus by cluster affinity\n",
    "fileids = corpus.fileids(categories=['news'])\n",
    "clusters = pipeline.predict(corpus.docs(fileids=fileids))\n",
    "groups = list(zip(clusters, fileids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bec3d3b6-13ed-4031-8bd6-3db47321a715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1: news/56d63af0c1808113ffb88745.pickle\n",
      "Cluster 2: news/56d7503fc18081081a6e1e15.pickle\n",
      "Cluster 3: news/56d62570c1808113ffb87557.pickle\n",
      "Cluster 4: news/56d75003c18081081a6e1b5e.pickle\n",
      "Cluster 5: news/56d62554c1808113ffb87492.pickle\n",
      "Cluster 6: news/56d64c7ac1808115036122b4.pickle\n",
      "Cluster 7: news/56d6255dc1808113ffb874f0.pickle\n"
     ]
    }
   ],
   "source": [
    "# Group documents in corpus by cluster and display them\n",
    "groups.sort(key=itemgetter(0))\n",
    "for group, items in groupby(groups, key=itemgetter(0)):\n",
    "    for cluster, fname in items:\n",
    "        print(f\"Cluster {cluster + 1}: {fname}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5977dc8-5949-428f-ba2c-68d4b961016e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Optimizing k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b78c038-9ec6-4a10-bd64-1f02dde40222",
   "metadata": {},
   "source": [
    "### Handling uneven geometries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d15f2c-493b-47e4-9016-bf5a6a7bc3b2",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6dd214-ef12-4ad5-8499-78b1e909eded",
   "metadata": {},
   "source": [
    "### Agglomerative clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854533e9-d2f5-439c-864f-5947a73940fe",
   "metadata": {},
   "source": [
    "# Modeling Document Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aef3ab-998f-4409-956a-7ff71643fa72",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f8c64f-c6f6-4852-a743-0e26c32cf9c8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### In Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b02c56-5481-441d-9961-5888d4619d86",
   "metadata": {},
   "source": [
    "### The Gensim way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc8c7f1-0222-41fa-b70e-c4a5fb67f2f4",
   "metadata": {},
   "source": [
    "### Visualizing topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dd32f4-70b6-43dc-96ff-d38f533edd7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e435866-eb8f-4a66-9bed-cf106dd08e7c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### In Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb13fba0-f254-469a-a7ec-c0915d8aa1bd",
   "metadata": {},
   "source": [
    "### The Gensim way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ebffde-fd7c-453c-a42b-138c9ae1a28b",
   "metadata": {},
   "source": [
    "## Non-Negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2f1a6e-d62c-4995-8504-1959d84ead40",
   "metadata": {},
   "source": [
    "### In Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba9da19-1412-425d-a3e8-b74a2ad94a16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
