{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bf23ce4-d2ab-441d-a53e-efca4c547540",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5937cc10-4816-4636-9274-85053c693fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef32cac0-6e99-410a-9c1c-7fea632f090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4be3d51d-338c-4fe3-a479-8f96fab684e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd7eb6b-59a4-4304-9326-d96251702e09",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0451ed36-33c4-4139-9c39-2fc1ad19a5f4",
   "metadata": {},
   "source": [
    "# Grammar-Based Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754674ea-265d-4d65-843c-3c1799eaf8b5",
   "metadata": {},
   "source": [
    "## Context-Free Grammars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cb46382-835f-4d86-baaf-303546791094",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAMMAR = \"\"\"\n",
    "    S -> NNP VP\n",
    "    VP -> V PP\n",
    "    PP -> P NP\n",
    "    NP -> DT N\n",
    "    NNP -> 'Gwen' | 'George'\n",
    "    V -> 'looks' | 'burns'\n",
    "    P -> 'in' | 'for'\n",
    "    DT -> 'the'\n",
    "    N -> 'castle' | 'ocean'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15b3f75a-6b63-4528-92dd-47ffc9af6fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = nltk.CFG.fromstring(GRAMMAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6eb60453-9343-4aba-91ed-30cf5afbaafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 13 productions (start state = S)\n",
      "    S -> NNP VP\n",
      "    VP -> V PP\n",
      "    PP -> P NP\n",
      "    NP -> DT N\n",
      "    NNP -> 'Gwen'\n",
      "    NNP -> 'George'\n",
      "    V -> 'looks'\n",
      "    V -> 'burns'\n",
      "    P -> 'in'\n",
      "    P -> 'for'\n",
      "    DT -> 'the'\n",
      "    N -> 'castle'\n",
      "    N -> 'ocean'\n"
     ]
    }
   ],
   "source": [
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca2b5634-46eb-4546-a290-258cf5a5fa83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c409830e-9c7f-4b99-b8a3-992f3390e51a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[S -> NNP VP,\n",
       " VP -> V PP,\n",
       " PP -> P NP,\n",
       " NP -> DT N,\n",
       " NNP -> 'Gwen',\n",
       " NNP -> 'George',\n",
       " V -> 'looks',\n",
       " V -> 'burns',\n",
       " P -> 'in',\n",
       " P -> 'for',\n",
       " DT -> 'the',\n",
       " N -> 'castle',\n",
       " N -> 'ocean']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.productions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e7fc77-a07c-4bdf-9e60-5a1f1437f774",
   "metadata": {},
   "source": [
    "## Syntactic Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73c123ab-8936-4c34-b56d-e04b03f3e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chunk.regexp import RegexpParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08bbc2d8-cb2f-42c3-bfd0-7abf0a1c5fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAMMAR = r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}'\n",
    "chunker = RegexpParser(GRAMMAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdca5255-7e40-410a-bbfa-5bb18866cfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"\"\"\n",
    "    Dusty Baker proposed a simple solution to the Washington Nationals early-season bullpen \n",
    "    troubles Monday afternoon and it had nothing to do with his maligned group of relievers.\n",
    "\"\"\"\n",
    "tokens = nltk.word_tokenize(sent)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "chunked = chunker.parse(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1dcdff1-55ea-44ce-af86-a7c7e5fc6764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (KT Dusty/NNP Baker/NNP)\n",
      "  proposed/VBD\n",
      "  a/DT\n",
      "  (KT simple/JJ solution/NN)\n",
      "  to/TO\n",
      "  the/DT\n",
      "  (KT Washington/NNP Nationals/NNP)\n",
      "  (KT\n",
      "    early-season/JJ\n",
      "    bullpen/NN\n",
      "    troubles/NNS\n",
      "    Monday/NNP\n",
      "    afternoon/NN)\n",
      "  and/CC\n",
      "  it/PRP\n",
      "  had/VBD\n",
      "  (KT nothing/NN)\n",
      "  to/TO\n",
      "  do/VB\n",
      "  with/IN\n",
      "  his/PRP$\n",
      "  maligned/VBN\n",
      "  (KT group/NN of/IN relievers/NNS)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(chunked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab04e4ea-bd48-4730-b207-2e1bfc5240ec",
   "metadata": {},
   "source": [
    "## Extracting Keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0215e3ba-9ad6-47c5-96e9-b2a6523ae3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAMMAR = r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}'\n",
    "GOODTAGS = frozenset(['JJ','JJR','JJS','NN','NNP','NNS','NNPS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca5c9d1d-575e-4b16-8899-f04c4c8b7b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyphraseExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Wraps a PickledCorpusReader consisting of pos-tagged documents.\n",
    "    \"\"\"\n",
    "    def __init__(self, grammar=GRAMMAR):\n",
    "        self.grammar = GRAMMAR\n",
    "        self.chunker = nltk.RegexpParser(self.grammar)\n",
    "    \n",
    "    def normalize(self, sentence):\n",
    "        \"\"\"\n",
    "        Removes punctuation from a tokenized/tagged sentence and\n",
    "        lowercases words.\n",
    "        \"\"\"\n",
    "        return [(token.lower(), tag)\n",
    "                for (token, tag) in sentence\n",
    "                if not all(unicodedata.category(char).startswith('P')\n",
    "                           for char in token)]\n",
    "    \n",
    "    def extract_keyphrases(self, document):\n",
    "        \"\"\"\n",
    "        For a document, parse sentences using our chunker created by\n",
    "        our grammar, converting the parse tree into a tagged sequence.\n",
    "        Yields extracted phrases.\n",
    "        \"\"\"\n",
    "        for paragraph in document:\n",
    "            for sentence in paragraph:\n",
    "                sentence = self.normalize(sentence)\n",
    "                if not sentence:\n",
    "                    continue\n",
    "                chunks = nltk.tree2conlltags(\n",
    "                    self.chunker.parse(sentence)\n",
    "                )\n",
    "                keyphrases = [\n",
    "                    ' '.join(word for word, pos, chunk in group)\n",
    "                    for key, group in itertools.groupby(\n",
    "                        chunks, lambda term: term[-1] != 'O'\n",
    "                    ) if key\n",
    "                ]\n",
    "                for keyphrase in keyphrases:\n",
    "                    yield keyphrase\n",
    "    \n",
    "    def fit(self, documents, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        for document in documents:\n",
    "            yield list(self.extract_keyphrases(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31185caf-5117-49f2-adbf-a37be9463db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyphrase_extractor = KeyphraseExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "874a91d6-c852-4824-a70e-37735ebfa285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dusty baker',\n",
       "  'simple solution',\n",
       "  'washington nationals early-season bullpen troubles monday afternoon',\n",
       "  'nothing',\n",
       "  'group of relievers']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(keyphrase_extractor.fit_transform([[[tagged]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8098261-21f8-4ad9-991a-af394a88771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from reader import PickledCorpusReader\n",
    "\n",
    "# corpus = PickledCorpusReader('../corpus')\n",
    "# docs = corpus.docs()\n",
    "\n",
    "# phrase_extractor = KeyphraseExtractor()\n",
    "# keyphrases = list(phrase_extractor.fit_transform(docs))\n",
    "# print(keyphrases[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188512be-0fea-42ea-8f71-2c25f473906c",
   "metadata": {},
   "source": [
    "## Extracting Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fc7747-2ab9-4204-8ad4-d93a862e6e1f",
   "metadata": {},
   "source": [
    "# n-Gram Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075cd1cf-baea-415f-a627-f93f1519a526",
   "metadata": {},
   "source": [
    "## An n-Gram-Aware CorpusReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0edf47e-2f0d-4bed-94ac-4ad2386f88d6",
   "metadata": {},
   "source": [
    "## Choosing the Right n-Gram Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157ff500-678a-4d70-b408-74f0b4818845",
   "metadata": {},
   "source": [
    "## Significant Collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2902dd-8dca-4c8f-a2b0-e86dcbcdf220",
   "metadata": {},
   "source": [
    "# n-Gram Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d9cb7c-6efb-475d-883a-d5dcf7ddf865",
   "metadata": {},
   "source": [
    "## Frequency and Conditional Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6390e135-14ce-410e-89c5-ce22040e0aab",
   "metadata": {},
   "source": [
    "## Estimating Maximum Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aea820-78df-4488-9e73-6c75a36ee1b7",
   "metadata": {},
   "source": [
    "## Unknown Words: Back-off and Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e258e2-6290-4a08-8b16-3784d0d1cb6d",
   "metadata": {},
   "source": [
    "## Language Generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
