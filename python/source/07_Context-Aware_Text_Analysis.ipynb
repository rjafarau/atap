{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bf23ce4-d2ab-441d-a53e-efca4c547540",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5937cc10-4816-4636-9274-85053c693fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import pathlib\n",
    "import itertools\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef32cac0-6e99-410a-9c1c-7fea632f090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4be3d51d-338c-4fe3-a479-8f96fab684e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4575d69c-ec1d-4ff5-a109-7a1ad16c5dc7",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf84ab41-094f-4e13-a39f-9efd7036b541",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = pathlib.Path('~/project').expanduser()\n",
    "DATA_DIR = PROJECT_DIR / 'data'\n",
    "CORPUS_ROOT = DATA_DIR / 'sample'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc28f12-95d2-464a-a283-35ac21a247be",
   "metadata": {},
   "source": [
    "# PickledCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7393f98-f8cf-4b4e-84d8-1bfa6e59cc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_PATTERN = r'(?!\\.)[a-z_\\s]+/[a-f0-9]+\\.json'\n",
    "PKL_PATTERN = r'(?!\\.)[a-z_\\s]+/[a-f0-9]+\\.pickle'\n",
    "CAT_PATTERN = r'([a-z_\\s]+)/.*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91a653b3-b826-4384-8e1d-41d453931956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.api import CorpusReader\n",
    "from nltk.corpus.reader.api import CategorizedCorpusReader\n",
    "\n",
    "class PickledCorpusReader(CategorizedCorpusReader, CorpusReader):\n",
    "\n",
    "    def __init__(self, root, fileids=PKL_PATTERN, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the corpus reader.  Categorization arguments\n",
    "        (``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to\n",
    "        the ``CategorizedCorpusReader`` constructor.  The remaining arguments\n",
    "        are passed to the ``CorpusReader`` constructor.\n",
    "        \"\"\"\n",
    "        # Add the default category pattern if not passed into the class.\n",
    "        if not any(key.startswith('cat_') for key in kwargs.keys()):\n",
    "            kwargs['cat_pattern'] = CAT_PATTERN\n",
    "\n",
    "        CategorizedCorpusReader.__init__(self, kwargs)\n",
    "        CorpusReader.__init__(self, root, fileids)\n",
    "\n",
    "    def _resolve(self, fileids, categories):\n",
    "        \"\"\"\n",
    "        Returns a list of fileids or categories depending on what is passed\n",
    "        to each internal corpus reader function. This primarily bubbles up to\n",
    "        the high level ``docs`` method, but is implemented here similar to\n",
    "        the nltk ``CategorizedPlaintextCorpusReader``.\n",
    "        \"\"\"\n",
    "        if fileids is not None and categories is not None:\n",
    "            raise ValueError(\"Specify fileids or categories, not both\")\n",
    "\n",
    "        if categories is not None:\n",
    "            return self.fileids(categories)\n",
    "        return fileids\n",
    "    \n",
    "    def feeds(self):\n",
    "        data = self.open('feeds.json')\n",
    "        return json.load(data)\n",
    "\n",
    "    def docs(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns the document loaded from a pickled object for every file in\n",
    "        the corpus. Similar to the BaleenCorpusReader, this uses a generator\n",
    "        to acheive memory safe iteration.\n",
    "        \"\"\"\n",
    "        # Resolve the fileids and the categories\n",
    "        fileids = self._resolve(fileids, categories)\n",
    "\n",
    "        # Create a generator, loading one document into memory at a time.\n",
    "        for path, enc, fileid in self.abspaths(fileids, True, True):\n",
    "            with open(path, 'rb') as f:\n",
    "                yield pickle.load(f)\n",
    "\n",
    "    def paras(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of paragraphs where each paragraph is a list of\n",
    "        sentences, which is in turn a list of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for doc in self.docs(fileids, categories):\n",
    "            for paragraph in doc:\n",
    "                yield paragraph\n",
    "\n",
    "    def sents(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of sentences where each sentence is a list of\n",
    "        (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for paragraph in self.paras(fileids, categories):\n",
    "            for sentence in paragraph:\n",
    "                yield sentence\n",
    "    \n",
    "    def tagged_words(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for sent in self.sents(fileids, categories):\n",
    "            for token, tag in sent:\n",
    "                yield token, tag\n",
    "\n",
    "    def words(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of tokens.\n",
    "        \"\"\"\n",
    "        for sent in self.sents(fileids, categories):\n",
    "            for token, tag in sent:\n",
    "                yield token\n",
    "    \n",
    "    def describe(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Performs a single pass of the corpus and\n",
    "        returns a dictionary with a variety of metrics\n",
    "        concerning the state of the corpus.\n",
    "        \"\"\"\n",
    "        started = time.time()\n",
    "        \n",
    "        # Structures to perform counting.\n",
    "        counts  = nltk.FreqDist()\n",
    "        tokens  = nltk.FreqDist()\n",
    "\n",
    "        # Perform single pass over paragraphs, tokenize and count\n",
    "        for para in self.paras(fileids, categories):\n",
    "            counts['paras'] += 1\n",
    "            \n",
    "            for sent in para:\n",
    "                counts['sents'] += 1\n",
    "                \n",
    "                for word, tag in sent:\n",
    "                    counts['words'] += 1\n",
    "                    tokens[word] += 1\n",
    "        \n",
    "        # Compute the number of files and categories in the corpus\n",
    "        n_fileids = len(self._resolve(fileids, categories) or self.fileids())\n",
    "        n_topics  = len(self.categories(self._resolve(fileids, categories)))\n",
    "\n",
    "        # Return data structure with information\n",
    "        return {\n",
    "            'files':  n_fileids,\n",
    "            'topics': n_topics,\n",
    "            'paras':  counts['paras'],\n",
    "            'sents':  counts['sents'],\n",
    "            'words':  counts['words'],\n",
    "            'vocab':  len(tokens),\n",
    "            'lexdiv': counts['words'] / len(tokens),\n",
    "            'ppdoc':  counts['paras'] / n_fileids,\n",
    "            'sppar':  counts['sents'] / counts['paras'],\n",
    "            'secs':   time.time() - started,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2258e392-ed0d-4513-a1ac-af4dec197cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58,748 vocabulary 1,624,862 word count\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "corpus = PickledCorpusReader(root=CORPUS_ROOT.as_posix())\n",
    "words  = Counter(corpus.words())\n",
    "\n",
    "print(f\"{len(words.keys()):,} vocabulary {sum(words.values()):,} word count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0451ed36-33c4-4139-9c39-2fc1ad19a5f4",
   "metadata": {},
   "source": [
    "# Grammar-Based Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754674ea-265d-4d65-843c-3c1799eaf8b5",
   "metadata": {},
   "source": [
    "## Context-Free Grammars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cb46382-835f-4d86-baaf-303546791094",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAMMAR = \"\"\"\n",
    "    S -> NNP VP\n",
    "    VP -> V PP\n",
    "    PP -> P NP\n",
    "    NP -> DT N\n",
    "    NNP -> 'Gwen' | 'George'\n",
    "    V -> 'looks' | 'burns'\n",
    "    P -> 'in' | 'for'\n",
    "    DT -> 'the'\n",
    "    N -> 'castle' | 'ocean'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15b3f75a-6b63-4528-92dd-47ffc9af6fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = nltk.CFG.fromstring(GRAMMAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eb60453-9343-4aba-91ed-30cf5afbaafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 13 productions (start state = S)\n",
      "    S -> NNP VP\n",
      "    VP -> V PP\n",
      "    PP -> P NP\n",
      "    NP -> DT N\n",
      "    NNP -> 'Gwen'\n",
      "    NNP -> 'George'\n",
      "    V -> 'looks'\n",
      "    V -> 'burns'\n",
      "    P -> 'in'\n",
      "    P -> 'for'\n",
      "    DT -> 'the'\n",
      "    N -> 'castle'\n",
      "    N -> 'ocean'\n"
     ]
    }
   ],
   "source": [
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca2b5634-46eb-4546-a290-258cf5a5fa83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c409830e-9c7f-4b99-b8a3-992f3390e51a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[S -> NNP VP,\n",
       " VP -> V PP,\n",
       " PP -> P NP,\n",
       " NP -> DT N,\n",
       " NNP -> 'Gwen',\n",
       " NNP -> 'George',\n",
       " V -> 'looks',\n",
       " V -> 'burns',\n",
       " P -> 'in',\n",
       " P -> 'for',\n",
       " DT -> 'the',\n",
       " N -> 'castle',\n",
       " N -> 'ocean']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.productions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e7fc77-a07c-4bdf-9e60-5a1f1437f774",
   "metadata": {},
   "source": [
    "## Syntactic Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73c123ab-8936-4c34-b56d-e04b03f3e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chunk.regexp import RegexpParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08bbc2d8-cb2f-42c3-bfd0-7abf0a1c5fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAMMAR = r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}'\n",
    "chunker = RegexpParser(GRAMMAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdca5255-7e40-410a-bbfa-5bb18866cfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"\"\"\n",
    "    Dusty Baker proposed a simple solution to the Washington Nationals early-season bullpen \n",
    "    troubles Monday afternoon and it had nothing to do with his maligned group of relievers.\n",
    "\"\"\"\n",
    "tokens = nltk.word_tokenize(sent)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "chunked = chunker.parse(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1dcdff1-55ea-44ce-af86-a7c7e5fc6764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (KT Dusty/NNP Baker/NNP)\n",
      "  proposed/VBD\n",
      "  a/DT\n",
      "  (KT simple/JJ solution/NN)\n",
      "  to/TO\n",
      "  the/DT\n",
      "  (KT Washington/NNP Nationals/NNP)\n",
      "  (KT\n",
      "    early-season/JJ\n",
      "    bullpen/NN\n",
      "    troubles/NNS\n",
      "    Monday/NNP\n",
      "    afternoon/NN)\n",
      "  and/CC\n",
      "  it/PRP\n",
      "  had/VBD\n",
      "  (KT nothing/NN)\n",
      "  to/TO\n",
      "  do/VB\n",
      "  with/IN\n",
      "  his/PRP$\n",
      "  maligned/VBN\n",
      "  (KT group/NN of/IN relievers/NNS)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(chunked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab04e4ea-bd48-4730-b207-2e1bfc5240ec",
   "metadata": {},
   "source": [
    "## Extracting Keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0215e3ba-9ad6-47c5-96e9-b2a6523ae3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAMMAR = r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}'\n",
    "GOODTAGS = frozenset(['JJ','JJR','JJS','NN','NNP','NNS','NNPS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca5c9d1d-575e-4b16-8899-f04c4c8b7b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyphraseExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Wraps a PickledCorpusReader consisting of pos-tagged documents.\n",
    "    \"\"\"\n",
    "    def __init__(self, grammar=GRAMMAR):\n",
    "        self.grammar = GRAMMAR\n",
    "        self.chunker = nltk.RegexpParser(self.grammar)\n",
    "    \n",
    "    def normalize(self, sentence):\n",
    "        \"\"\"\n",
    "        Removes punctuation from a tokenized/tagged sentence and\n",
    "        lowercases words.\n",
    "        \"\"\"\n",
    "        return [(token.lower(), tag)\n",
    "                for (token, tag) in sentence\n",
    "                if not all(unicodedata.category(char).startswith('P')\n",
    "                           for char in token)]\n",
    "    \n",
    "    def extract_keyphrases(self, document):\n",
    "        \"\"\"\n",
    "        For a document, parse sentences using our chunker created by\n",
    "        our grammar, converting the parse tree into a tagged sequence.\n",
    "        Yields extracted phrases.\n",
    "        \"\"\"\n",
    "        for paragraph in document:\n",
    "            for sentence in paragraph:\n",
    "                sentence = self.normalize(sentence)\n",
    "                if not sentence:\n",
    "                    continue\n",
    "                chunks = nltk.tree2conlltags(\n",
    "                    self.chunker.parse(sentence)\n",
    "                )\n",
    "                keyphrases = [\n",
    "                    ' '.join(word for word, pos, chunk in group)\n",
    "                    for key, group in itertools.groupby(\n",
    "                        chunks, lambda term: term[-1] != 'O'\n",
    "                    ) if key\n",
    "                ]\n",
    "                for keyphrase in keyphrases:\n",
    "                    yield keyphrase\n",
    "    \n",
    "    def fit(self, documents, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        for document in documents:\n",
    "            yield list(self.extract_keyphrases(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f59f2b4-fe55-490d-ba8a-e07caf8ff844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lonely city', 'heart piercing wisdom', 'loneliness', 'laing', 'everyone', 'feast later', 'point', 'own hermetic existence in new york', 'danger', 'thankfully', 'lonely city', 'cry for connection', 'overcrowded overstimulated world', 'blueprint of urban loneliness', 'emotion', 'calls', 'city', 'npr jason heller', 'olivia laing', 'lonely city', 'exploration of loneliness', 'others experiences in new york city', 'rumpus', 'review', 'lonely city', 'related posts']\n"
     ]
    }
   ],
   "source": [
    "corpus = PickledCorpusReader(root=CORPUS_ROOT.as_posix())\n",
    "docs = corpus.docs()\n",
    "\n",
    "keyphrase_extractor = KeyphraseExtractor()\n",
    "keyphrases = list(keyphrase_extractor.fit_transform(docs))\n",
    "print(keyphrases[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188512be-0fea-42ea-8f71-2c25f473906c",
   "metadata": {},
   "source": [
    "## Extracting Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b6480d1-ade5-4a2c-8b49-11314a9ee603",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOODLABELS = frozenset(['PERSON', 'ORGANIZATION', 'FACILITY', 'GPE', 'GSP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22e7e8b9-52ba-4a56-be4f-7f7d64dca328",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, labels=GOODLABELS, **kwargs):\n",
    "        self.labels = labels\n",
    "\n",
    "    def get_entities(self, document):\n",
    "        return [\n",
    "            ' '.join(word.lower() for word, tag in tree)\n",
    "            for paragraph in document\n",
    "            for sentence in paragraph\n",
    "            for tree in nltk.ne_chunk(sentence)\n",
    "            if hasattr(tree, 'label')\n",
    "            and tree.label() in self.labels\n",
    "        ]\n",
    "\n",
    "    def fit(self, documents, labels=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        for document in documents:\n",
    "            yield self.get_entities(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22811337-441b-4ea2-822f-78ab18334ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lonely city', 'loneliness', 'laing', 'new york', 'lonely city', 'npr', 'jason heller', 'olivia laing', 'lonely city', 'new york city', 'rumpus', 'lonely city', 'related']\n"
     ]
    }
   ],
   "source": [
    "corpus = PickledCorpusReader(root=CORPUS_ROOT.as_posix())\n",
    "docs = corpus.docs()\n",
    "\n",
    "entity_extractor = EntityExtractor()\n",
    "entities = list(entity_extractor.fit_transform(docs))\n",
    "print(entities[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fc7747-2ab9-4204-8ad4-d93a862e6e1f",
   "metadata": {},
   "source": [
    "# n-Gram Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075cd1cf-baea-415f-a627-f93f1519a526",
   "metadata": {},
   "source": [
    "## An n-Gram-Aware CorpusReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0edf47e-2f0d-4bed-94ac-4ad2386f88d6",
   "metadata": {},
   "source": [
    "## Choosing the Right n-Gram Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157ff500-678a-4d70-b408-74f0b4818845",
   "metadata": {},
   "source": [
    "## Significant Collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2902dd-8dca-4c8f-a2b0-e86dcbcdf220",
   "metadata": {},
   "source": [
    "# n-Gram Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d9cb7c-6efb-475d-883a-d5dcf7ddf865",
   "metadata": {},
   "source": [
    "## Frequency and Conditional Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6390e135-14ce-410e-89c5-ce22040e0aab",
   "metadata": {},
   "source": [
    "## Estimating Maximum Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aea820-78df-4488-9e73-6c75a36ee1b7",
   "metadata": {},
   "source": [
    "## Unknown Words: Back-off and Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e258e2-6290-4a08-8b16-3784d0d1cb6d",
   "metadata": {},
   "source": [
    "## Language Generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
