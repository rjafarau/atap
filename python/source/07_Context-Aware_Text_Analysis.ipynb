{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bf23ce4-d2ab-441d-a53e-efca4c547540",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5937cc10-4816-4636-9274-85053c693fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import math\n",
    "import pickle\n",
    "import pathlib\n",
    "import itertools\n",
    "import functools\n",
    "import collections\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef32cac0-6e99-410a-9c1c-7fea632f090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4be3d51d-338c-4fe3-a479-8f96fab684e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4575d69c-ec1d-4ff5-a109-7a1ad16c5dc7",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf84ab41-094f-4e13-a39f-9efd7036b541",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = pathlib.Path('~/project').expanduser()\n",
    "DATA_DIR = PROJECT_DIR / 'data'\n",
    "CORPUS_ROOT = DATA_DIR / 'sample'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc28f12-95d2-464a-a283-35ac21a247be",
   "metadata": {},
   "source": [
    "# PickledCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7393f98-f8cf-4b4e-84d8-1bfa6e59cc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_PATTERN = r'(?!\\.)[a-z_\\s]+/[a-f0-9]+\\.json'\n",
    "PKL_PATTERN = r'(?!\\.)[a-z_\\s]+/[a-f0-9]+\\.pickle'\n",
    "CAT_PATTERN = r'([a-z_\\s]+)/.*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91a653b3-b826-4384-8e1d-41d453931956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.api import CorpusReader\n",
    "from nltk.corpus.reader.api import CategorizedCorpusReader\n",
    "\n",
    "class PickledCorpusReader(CategorizedCorpusReader, CorpusReader):\n",
    "\n",
    "    def __init__(self, root, fileids=PKL_PATTERN, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the corpus reader.  Categorization arguments\n",
    "        (``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to\n",
    "        the ``CategorizedCorpusReader`` constructor.  The remaining arguments\n",
    "        are passed to the ``CorpusReader`` constructor.\n",
    "        \"\"\"\n",
    "        # Add the default category pattern if not passed into the class.\n",
    "        if not any(key.startswith('cat_') for key in kwargs.keys()):\n",
    "            kwargs['cat_pattern'] = CAT_PATTERN\n",
    "\n",
    "        CategorizedCorpusReader.__init__(self, kwargs)\n",
    "        CorpusReader.__init__(self, root, fileids)\n",
    "\n",
    "    def _resolve(self, fileids, categories):\n",
    "        \"\"\"\n",
    "        Returns a list of fileids or categories depending on what is passed\n",
    "        to each internal corpus reader function. This primarily bubbles up to\n",
    "        the high level ``docs`` method, but is implemented here similar to\n",
    "        the nltk ``CategorizedPlaintextCorpusReader``.\n",
    "        \"\"\"\n",
    "        if fileids is not None and categories is not None:\n",
    "            raise ValueError(\"Specify fileids or categories, not both\")\n",
    "\n",
    "        if categories is not None:\n",
    "            return self.fileids(categories)\n",
    "        return fileids\n",
    "    \n",
    "    def feeds(self):\n",
    "        data = self.open('feeds.json')\n",
    "        return json.load(data)\n",
    "\n",
    "    def docs(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns the document loaded from a pickled object for every file in\n",
    "        the corpus. Similar to the BaleenCorpusReader, this uses a generator\n",
    "        to acheive memory safe iteration.\n",
    "        \"\"\"\n",
    "        # Resolve the fileids and the categories\n",
    "        fileids = self._resolve(fileids, categories)\n",
    "\n",
    "        # Create a generator, loading one document into memory at a time.\n",
    "        for path, enc, fileid in self.abspaths(fileids, True, True):\n",
    "            with open(path, 'rb') as f:\n",
    "                yield pickle.load(f)\n",
    "\n",
    "    def tagged_paras(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of paragraphs where each paragraph is a list of\n",
    "        sentences, which is in turn a list of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for doc in self.docs(fileids, categories):\n",
    "            for tagged_para in doc:\n",
    "                yield tagged_para\n",
    "    \n",
    "    def paras(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of paragraphs where each paragraph is a list of\n",
    "        sentences, which is in turn a list of tokens.\n",
    "        \"\"\"\n",
    "        for tagged_para in self.tagged_paras(fileids, categories):\n",
    "            yield [[word for word, tag in tagged_sent]\n",
    "                   for tagged_sent in tagged_para]\n",
    "\n",
    "    def tagged_sents(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of sentences where each sentence is a list of\n",
    "        (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for tagged_para in self.tagged_paras(fileids, categories):\n",
    "            for tagged_sent in tagged_para:\n",
    "                yield tagged_sent\n",
    "                \n",
    "    def sents(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of sentences where each sentence is a list of\n",
    "        tokens.\n",
    "        \"\"\"\n",
    "        for tagged_sent in self.tagged_sents(fileids, categories):\n",
    "            yield [word for word, tag in tagged_sent]\n",
    "    \n",
    "    def tagged_words(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for sent in self.tagged_sents(fileids, categories):\n",
    "            for token, tag in sent:\n",
    "                yield token, tag\n",
    "\n",
    "    def words(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of tokens.\n",
    "        \"\"\"\n",
    "        for word, tag in self.tagged_words(fileids, categories):\n",
    "            yield word\n",
    "    \n",
    "    def describe(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Performs a single pass of the corpus and\n",
    "        returns a dictionary with a variety of metrics\n",
    "        concerning the state of the corpus.\n",
    "        \"\"\"\n",
    "        started = time.time()\n",
    "        \n",
    "        # Structures to perform counting.\n",
    "        counts  = nltk.FreqDist()\n",
    "        tokens  = nltk.FreqDist()\n",
    "\n",
    "        # Perform single pass over paragraphs, tokenize and count\n",
    "        for para in self.paras(fileids, categories):\n",
    "            counts['paras'] += 1\n",
    "            \n",
    "            for sent in para:\n",
    "                counts['sents'] += 1\n",
    "                \n",
    "                for word, tag in sent:\n",
    "                    counts['words'] += 1\n",
    "                    tokens[word] += 1\n",
    "        \n",
    "        # Compute the number of files and categories in the corpus\n",
    "        n_fileids = len(self._resolve(fileids, categories) or self.fileids())\n",
    "        n_topics  = len(self.categories(self._resolve(fileids, categories)))\n",
    "\n",
    "        # Return data structure with information\n",
    "        return {\n",
    "            'files':  n_fileids,\n",
    "            'topics': n_topics,\n",
    "            'paras':  counts['paras'],\n",
    "            'sents':  counts['sents'],\n",
    "            'words':  counts['words'],\n",
    "            'vocab':  len(tokens),\n",
    "            'lexdiv': counts['words'] / len(tokens),\n",
    "            'ppdoc':  counts['paras'] / n_fileids,\n",
    "            'sppar':  counts['sents'] / counts['paras'],\n",
    "            'secs':   time.time() - started,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2258e392-ed0d-4513-a1ac-af4dec197cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58,748 vocabulary 1,624,862 word count\n"
     ]
    }
   ],
   "source": [
    "corpus = PickledCorpusReader(root=CORPUS_ROOT.as_posix())\n",
    "words = collections.Counter(corpus.words())\n",
    "\n",
    "print(f\"{len(words.keys()):,} vocabulary {sum(words.values()):,} word count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0451ed36-33c4-4139-9c39-2fc1ad19a5f4",
   "metadata": {},
   "source": [
    "# Grammar-Based Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754674ea-265d-4d65-843c-3c1799eaf8b5",
   "metadata": {},
   "source": [
    "## Context-Free Grammars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cb46382-835f-4d86-baaf-303546791094",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAMMAR = \"\"\"\n",
    "    S -> NNP VP\n",
    "    VP -> V PP\n",
    "    PP -> P NP\n",
    "    NP -> DT N\n",
    "    NNP -> 'Gwen' | 'George'\n",
    "    V -> 'looks' | 'burns'\n",
    "    P -> 'in' | 'for'\n",
    "    DT -> 'the'\n",
    "    N -> 'castle' | 'ocean'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15b3f75a-6b63-4528-92dd-47ffc9af6fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = nltk.CFG.fromstring(GRAMMAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eb60453-9343-4aba-91ed-30cf5afbaafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 13 productions (start state = S)\n",
      "    S -> NNP VP\n",
      "    VP -> V PP\n",
      "    PP -> P NP\n",
      "    NP -> DT N\n",
      "    NNP -> 'Gwen'\n",
      "    NNP -> 'George'\n",
      "    V -> 'looks'\n",
      "    V -> 'burns'\n",
      "    P -> 'in'\n",
      "    P -> 'for'\n",
      "    DT -> 'the'\n",
      "    N -> 'castle'\n",
      "    N -> 'ocean'\n"
     ]
    }
   ],
   "source": [
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca2b5634-46eb-4546-a290-258cf5a5fa83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c409830e-9c7f-4b99-b8a3-992f3390e51a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[S -> NNP VP,\n",
       " VP -> V PP,\n",
       " PP -> P NP,\n",
       " NP -> DT N,\n",
       " NNP -> 'Gwen',\n",
       " NNP -> 'George',\n",
       " V -> 'looks',\n",
       " V -> 'burns',\n",
       " P -> 'in',\n",
       " P -> 'for',\n",
       " DT -> 'the',\n",
       " N -> 'castle',\n",
       " N -> 'ocean']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.productions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e7fc77-a07c-4bdf-9e60-5a1f1437f774",
   "metadata": {},
   "source": [
    "## Syntactic Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73c123ab-8936-4c34-b56d-e04b03f3e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chunk.regexp import RegexpParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08bbc2d8-cb2f-42c3-bfd0-7abf0a1c5fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAMMAR = r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}'\n",
    "chunker = RegexpParser(GRAMMAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdca5255-7e40-410a-bbfa-5bb18866cfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"\"\"\n",
    "    Dusty Baker proposed a simple solution to the Washington Nationals early-season bullpen \n",
    "    troubles Monday afternoon and it had nothing to do with his maligned group of relievers.\n",
    "\"\"\"\n",
    "tokens = nltk.word_tokenize(sent)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "chunked = chunker.parse(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1dcdff1-55ea-44ce-af86-a7c7e5fc6764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (KT Dusty/NNP Baker/NNP)\n",
      "  proposed/VBD\n",
      "  a/DT\n",
      "  (KT simple/JJ solution/NN)\n",
      "  to/TO\n",
      "  the/DT\n",
      "  (KT Washington/NNP Nationals/NNP)\n",
      "  (KT\n",
      "    early-season/JJ\n",
      "    bullpen/NN\n",
      "    troubles/NNS\n",
      "    Monday/NNP\n",
      "    afternoon/NN)\n",
      "  and/CC\n",
      "  it/PRP\n",
      "  had/VBD\n",
      "  (KT nothing/NN)\n",
      "  to/TO\n",
      "  do/VB\n",
      "  with/IN\n",
      "  his/PRP$\n",
      "  maligned/VBN\n",
      "  (KT group/NN of/IN relievers/NNS)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(chunked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab04e4ea-bd48-4730-b207-2e1bfc5240ec",
   "metadata": {},
   "source": [
    "## Extracting Keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0215e3ba-9ad6-47c5-96e9-b2a6523ae3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAMMAR = r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}'\n",
    "GOODTAGS = frozenset(['JJ','JJR','JJS','NN','NNP','NNS','NNPS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca5c9d1d-575e-4b16-8899-f04c4c8b7b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyphraseExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Wraps a PickledCorpusReader consisting of pos-tagged documents.\n",
    "    \"\"\"\n",
    "    def __init__(self, grammar=GRAMMAR):\n",
    "        self.grammar = GRAMMAR\n",
    "        self.chunker = nltk.RegexpParser(self.grammar)\n",
    "    \n",
    "    def normalize(self, sentence):\n",
    "        \"\"\"\n",
    "        Removes punctuation from a tokenized/tagged sentence and\n",
    "        lowercases words.\n",
    "        \"\"\"\n",
    "        return [(token.lower(), tag)\n",
    "                for (token, tag) in sentence\n",
    "                if not all(unicodedata.category(char).startswith('P')\n",
    "                           for char in token)]\n",
    "    \n",
    "    def extract_keyphrases(self, document):\n",
    "        \"\"\"\n",
    "        For a document, parse sentences using our chunker created by\n",
    "        our grammar, converting the parse tree into a tagged sequence.\n",
    "        Yields extracted phrases.\n",
    "        \"\"\"\n",
    "        for paragraph in document:\n",
    "            for sentence in paragraph:\n",
    "                sentence = self.normalize(sentence)\n",
    "                if not sentence:\n",
    "                    continue\n",
    "                chunks = nltk.tree2conlltags(\n",
    "                    self.chunker.parse(sentence)\n",
    "                )\n",
    "                keyphrases = [\n",
    "                    ' '.join(word for word, pos, chunk in group)\n",
    "                    for key, group in itertools.groupby(\n",
    "                        chunks, lambda term: term[-1] != 'O'\n",
    "                    ) if key\n",
    "                ]\n",
    "                for keyphrase in keyphrases:\n",
    "                    yield keyphrase\n",
    "    \n",
    "    def fit(self, documents, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        for document in documents:\n",
    "            yield list(self.extract_keyphrases(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f59f2b4-fe55-490d-ba8a-e07caf8ff844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lonely city', 'heart piercing wisdom', 'loneliness', 'laing', 'everyone', 'feast later', 'point', 'own hermetic existence in new york', 'danger', 'thankfully', 'lonely city', 'cry for connection', 'overcrowded overstimulated world', 'blueprint of urban loneliness', 'emotion', 'calls', 'city', 'npr jason heller', 'olivia laing', 'lonely city', 'exploration of loneliness', 'others experiences in new york city', 'rumpus', 'review', 'lonely city', 'related posts']\n",
      "CPU times: user 4.43 s, sys: 333 ms, total: 4.77 s\n",
      "Wall time: 7.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus = PickledCorpusReader(root=CORPUS_ROOT.as_posix())\n",
    "docs = corpus.docs()\n",
    "\n",
    "keyphrase_extractor = KeyphraseExtractor()\n",
    "keyphrases = list(keyphrase_extractor.fit_transform(docs))\n",
    "print(keyphrases[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188512be-0fea-42ea-8f71-2c25f473906c",
   "metadata": {},
   "source": [
    "## Extracting Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b6480d1-ade5-4a2c-8b49-11314a9ee603",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOODLABELS = frozenset(['PERSON', 'ORGANIZATION', 'FACILITY', 'GPE', 'GSP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22e7e8b9-52ba-4a56-be4f-7f7d64dca328",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, labels=GOODLABELS, **kwargs):\n",
    "        self.labels = labels\n",
    "\n",
    "    def get_entities(self, document):\n",
    "        return [\n",
    "            ' '.join(word.lower() for word, tag in tree)\n",
    "            for paragraph in document\n",
    "            for sentence in paragraph\n",
    "            for tree in nltk.ne_chunk(sentence)\n",
    "            if hasattr(tree, 'label')\n",
    "            and tree.label() in self.labels\n",
    "        ]\n",
    "\n",
    "    def fit(self, documents, labels=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        for document in documents:\n",
    "            yield self.get_entities(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22811337-441b-4ea2-822f-78ab18334ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lonely city', 'loneliness', 'laing', 'new york', 'lonely city', 'npr', 'jason heller', 'olivia laing', 'lonely city', 'new york city', 'rumpus', 'lonely city', 'related']\n",
      "CPU times: user 2min 22s, sys: 451 ms, total: 2min 23s\n",
      "Wall time: 2min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus = PickledCorpusReader(root=CORPUS_ROOT.as_posix())\n",
    "docs = corpus.docs()\n",
    "\n",
    "entity_extractor = EntityExtractor()\n",
    "entities = list(entity_extractor.fit_transform(docs))\n",
    "print(entities[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fc7747-2ab9-4204-8ad4-d93a862e6e1f",
   "metadata": {},
   "source": [
    "# n-Gram Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccb92b11-1df6-403e-b1da-c7917e448f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(words, n=2):\n",
    "    for idx in range(len(words)-n+1):\n",
    "        yield tuple(words[idx:idx+n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19df27c2-16af-48e4-b894-ef5aa2e7bdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\n",
    "    \"The\", \"reporters\", \"listened\", \"closely\", \"as\", \"the\", \"President\",\n",
    "    \"of\", \"the\", \"United\", \"States\", \"addressed\", \"the\", \"room\", \".\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf7c5da5-6658-4d6c-bcef-1f6d357fd7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The', 'reporters', 'listened')\n",
      "('reporters', 'listened', 'closely')\n",
      "('listened', 'closely', 'as')\n",
      "('closely', 'as', 'the')\n",
      "('as', 'the', 'President')\n",
      "('the', 'President', 'of')\n",
      "('President', 'of', 'the')\n",
      "('of', 'the', 'United')\n",
      "('the', 'United', 'States')\n",
      "('United', 'States', 'addressed')\n",
      "('States', 'addressed', 'the')\n",
      "('addressed', 'the', 'room')\n",
      "('the', 'room', '.')\n"
     ]
    }
   ],
   "source": [
    "for ngram in ngrams(words, n=3):\n",
    "    print(ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075cd1cf-baea-415f-a627-f93f1519a526",
   "metadata": {},
   "source": [
    "## An n-Gram-Aware CorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "997b4147-f9d2-4f41-8b23-ee3ec1ba9309",
   "metadata": {},
   "outputs": [],
   "source": [
    "LPAD_SYMBOL = '<s>'\n",
    "RPAD_SYMBOL = '</s>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "302b744f-6ee8-4ef9-8166-e50a7f4f36fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_ngrams = functools.partial(\n",
    "    nltk.ngrams,\n",
    "    pad_left=True, left_pad_symbol=LPAD_SYMBOL,\n",
    "    pad_right=True, right_pad_symbol=RPAD_SYMBOL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4de4dad4-7d1d-4742-9464-dda4e86eec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramPickledCorpusReader(PickledCorpusReader):\n",
    "    \n",
    "    def tagged_ngrams(self, n=2, fileids=None, categories=None):\n",
    "        for sent in self.tagged_sents(fileids, categories):\n",
    "            for ngram in nltk_ngrams(sent, n):\n",
    "                yield ngram\n",
    "    \n",
    "    def ngrams(self, n=2, fileids=None, categories=None):\n",
    "        for sent in self.sents(fileids, categories):\n",
    "            for ngram in nltk_ngrams(sent, n):\n",
    "                yield ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f50b479-df45-4274-9dd3-0a16e3267bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<s>', '<s>', '<s>', 'The')\n",
      "('<s>', '<s>', 'The', 'Lonely')\n",
      "('<s>', 'The', 'Lonely', 'City')\n",
      "('The', 'Lonely', 'City', 'bristles')\n",
      "('Lonely', 'City', 'bristles', 'with')\n",
      "('City', 'bristles', 'with', 'heart')\n",
      "('bristles', 'with', 'heart', '-')\n",
      "('with', 'heart', '-', 'piercing')\n",
      "('heart', '-', 'piercing', 'wisdom')\n",
      "('-', 'piercing', 'wisdom', '.')\n",
      "('piercing', 'wisdom', '.', '</s>')\n",
      "('wisdom', '.', '</s>', '</s>')\n",
      "('.', '</s>', '</s>', '</s>')\n",
      "('<s>', '<s>', '<s>', 'Loneliness')\n",
      "('<s>', '<s>', 'Loneliness', ',')\n",
      "('<s>', 'Loneliness', ',', 'according')\n",
      "('Loneliness', ',', 'according', 'to')\n",
      "(',', 'according', 'to', 'Laing')\n",
      "('according', 'to', 'Laing', ',')\n",
      "('to', 'Laing', ',', 'feels')\n",
      "CPU times: user 993 ms, sys: 535 ms, total: 1.53 s\n",
      "Wall time: 2min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus = NgramPickledCorpusReader(root=CORPUS_ROOT.as_posix())\n",
    "ngrams = corpus.ngrams(n=4)\n",
    "\n",
    "for ngram in list(ngrams)[:20]:\n",
    "    print(ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0edf47e-2f0d-4bed-94ac-4ad2386f88d6",
   "metadata": {},
   "source": [
    "## Choosing the Right n-Gram Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157ff500-678a-4d70-b408-74f0b4818845",
   "metadata": {},
   "source": [
    "## Significant Collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a456e738-dd57-4ce8-8b8b-dd598aa8ea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d618c88f-c8bf-4b64-958c-ebbb06597fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import QuadgramCollocationFinder\n",
    "from nltk.metrics.association import QuadgramAssocMeasures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b89a28ab-382b-4b63-aa35-99bd2f60145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_quadgrams(words, metric):\n",
    "    \"\"\"\n",
    "    Find and rank quadgrams from the supplied words using the given\n",
    "    association metric.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a collocation ranking utility from corpus words.\n",
    "    ngrams = QuadgramCollocationFinder.from_words(words)\n",
    "\n",
    "    # Rank collocations by an association metric\n",
    "    scored_df = pd.DataFrame(\n",
    "        data=ngrams.score_ngrams(metric),\n",
    "        columns=['collocation',\n",
    "                 f'score ({metric.__name__})']\n",
    "    )\n",
    "    \n",
    "    return scored_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0e974c5-5c58-4ea5-a1e0-3d07be9ccfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 1s, sys: 2.09 s, total: 3min 3s\n",
      "Wall time: 3min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus = PickledCorpusReader(root=CORPUS_ROOT.as_posix())\n",
    "\n",
    "scored_df = rank_quadgrams(\n",
    "    words=corpus.words(),\n",
    "    metric=QuadgramAssocMeasures.likelihood_ratio\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "011224b5-b0c7-43b2-a446-6b37374e5139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.6 s, sys: 29.1 ms, total: 1.62 s\n",
      "Wall time: 1.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Group quadgrams by first word\n",
    "scored_df['first_word'] = scored_df['collocation'].str[0]\n",
    "\n",
    "# Sort keyed quadgrams by strongest association\n",
    "scored_df.sort_values(\n",
    "    by=['first_word', 'score (likelihood_ratio)'],\n",
    "    ascending=False,\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "669c8550-6adf-4725-bb3f-36eb48cd09f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignificantCollocations(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 ngram_class=QuadgramCollocationFinder,\n",
    "                 metric=QuadgramAssocMeasures.pmi):\n",
    "        self.ngram_class = ngram_class\n",
    "        self.metric = metric\n",
    "        \n",
    "    def fit(self, docs, target):\n",
    "        ngrams = self.ngram_class.from_documents(docs)\n",
    "        self.scored_ = dict(ngrams.score_ngrams(self.metric))\n",
    "    \n",
    "    def transform(self, docs):\n",
    "        for doc in docs:\n",
    "            ngrams = self.ngram_class.from_words(doc)\n",
    "            yield {\n",
    "                ngram: self.scored_.get(ngram, 0.0)\n",
    "                for ngram in ngrams.nbest(QuadgramAssocMeasures.raw_freq, 50)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eaefd160-75ca-4058-9715-2d2799a15723",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7997f95b-a271-41cf-82c5-02671cabd8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    ('union', FeatureUnion([\n",
    "        ('ngrams', Pipeline([\n",
    "            ('sigcol', SignificantCollocations()),\n",
    "            ('dsigcol', DictVectorizer()),\n",
    "        ])),\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "    ])),\n",
    "    ('clf', SGDClassifier()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2902dd-8dca-4c8f-a2b0-e86dcbcdf220",
   "metadata": {
    "tags": []
   },
   "source": [
    "# n-Gram Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d9cb7c-6efb-475d-883a-d5dcf7ddf865",
   "metadata": {},
   "source": [
    "## Frequency and Conditional Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a8e6913-d59c-49b4-af77-48514b7124ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNKNOWN = '<UNK>'\n",
    "LPAD = '<s>'\n",
    "RPAD = '</s>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21797a29-6e2c-4a7f-8d8d-d208e2a349ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramCounter(object):\n",
    "    \"\"\"\n",
    "    The NgramCounter class counts ngrams given a vocabulary and ngram size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n, vocabulary, unknown=UNKNOWN):\n",
    "        \"\"\"\n",
    "        n is the size of the ngram\n",
    "        \"\"\"\n",
    "        if n < 1:\n",
    "            raise ValueError('ngram size must be greater than or equal to 1')\n",
    "\n",
    "        self.n = n\n",
    "        self.unknown = unknown\n",
    "        self.padding = {\n",
    "            'pad_left': True,\n",
    "            'pad_right': True,\n",
    "            'left_pad_symbol': LPAD,\n",
    "            'right_pad_symbol': RPAD\n",
    "        }\n",
    "\n",
    "        self.vocabulary = vocabulary\n",
    "        self.allgrams = collections.defaultdict(nltk.ConditionalFreqDist)\n",
    "        self.ngrams = nltk.FreqDist()\n",
    "        self.unigrams = nltk.FreqDist()\n",
    "    \n",
    "    def train_counts(self, training_text):\n",
    "        for sent in training_text:\n",
    "            checked_sent = map(self.check_against_vocab, sent)\n",
    "            sent_start = True\n",
    "            for ngram in self.to_ngrams(checked_sent):\n",
    "                # ngrams\n",
    "                self.ngrams[ngram] += 1\n",
    "                # unigrams\n",
    "                context, word = tuple(ngram[:-1]), ngram[-1]\n",
    "                if sent_start:\n",
    "                    for context_word in context:\n",
    "                        self.unigrams[context_word] += 1\n",
    "                    sent_start = False\n",
    "                self.unigrams[word] += 1\n",
    "                # allgrams\n",
    "                for window, ngram_order in enumerate(range(self.n, 1, -1)):\n",
    "                    context = context[window:]\n",
    "                    self.allgrams[ngram_order][context][word] += 1\n",
    "\n",
    "    def check_against_vocab(self, word):\n",
    "        return word if word in self.vocabulary else self.unknown\n",
    "\n",
    "    def to_ngrams(self, sequence):\n",
    "        \"\"\"\n",
    "        Wrapper for NLTK ngrams method\n",
    "        \"\"\"\n",
    "        return nltk.ngrams(sequence, self.n, **self.padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6b118e7-2168-4747-a46c-f8296a2c37da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ngrams(n, vocabulary, sentences):\n",
    "    counter = NgramCounter(n, vocabulary)\n",
    "    counter.train_counts(sentences)\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e3e20d8-f4bb-40d4-9e17-89f3e43b61f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = PickledCorpusReader(root=CORPUS_ROOT.as_posix())\n",
    "vocabulary = collections.Counter(corpus.words())\n",
    "sentences = list(corpus.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f3d1ae6-9b97-4c44-bffa-d3bb57f966bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.06 s, sys: 123 ms, total: 7.18 s\n",
      "Wall time: 7.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "counter = count_ngrams(3, vocabulary, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "844a377d-c71b-4438-8af5-ee35fa046609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'<s>': 149798, '</s>': 149798, ',': 68835, 'the': 65829, '.': 65400, 'to': 36590, 'of': 32149, 'and': 32017, 'a': 31716, 'in': 25062, ...})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9dd22b6d-d3a3-4f77-bcf0-8a1f2aef572d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({('.', '</s>', '</s>'): 56288, ('<s>', '<s>', 'The'): 7441, ('<s>', '<s>', '\"'): 3643, ('<s>', '<s>', '“'): 2699, ('.\"', '</s>', '</s>'): 2304, ('<s>', '<s>', 'It'): 2093, ('<s>', '<s>', 'But'): 1980, ('?', '</s>', '</s>'): 1951, ('<s>', '<s>', 'In'): 1936, ('said', '.', '</s>'): 1816, ...})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc33f871-c06f-4aed-8a52-f9f28449fd33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConditionalFreqDist with 574320 conditions>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.allgrams[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "081c7d45-2adc-44a2-bac7-894b1000551a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('!', '</s>'),\n",
       " ('!', 'For'),\n",
       " ('!', 'Kaley'),\n",
       " ('!', 'Lovebirds'),\n",
       " ('!', 'S'),\n",
       " ('!', 'Will'),\n",
       " ('!', 'duan'),\n",
       " ('!', 'wuick'),\n",
       " ('!', 'Â'),\n",
       " ('!!', '</s>')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(counter.allgrams[3].conditions())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68740739-97eb-4b82-a96c-51746ed84733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({\"'\": 3, 'Source': 1, 'and': 1, 'nominates': 1, 'in': 1, 'as': 1, 'said': 1, 'is': 1, 'who': 1, 'that': 1, ...})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.allgrams[3][('the', 'President')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6390e135-14ce-410e-89c5-ce22040e0aab",
   "metadata": {},
   "source": [
    "## Estimating Maximum Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d0acb38-acb5-4665-856b-499379f4ea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNgramModel(object):\n",
    "    \"\"\"\n",
    "    The BaseNgramModel creates an n-gram language model.\n",
    "    This base model is equivalent to a Maximum Likelihood Estimation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ngram_counter):\n",
    "        \"\"\"\n",
    "        BaseNgramModel is initialized with an NgramCounter.\n",
    "        \"\"\"\n",
    "        self.ngram_counter = ngram_counter\n",
    "        \n",
    "    def check_context(self, context):\n",
    "        \"\"\"\n",
    "        Ensures that the context is not longer than or equal to the model's\n",
    "        n-gram order.\n",
    "\n",
    "        Returns the context as a tuple.\n",
    "        \"\"\"\n",
    "        if len(context) >= self.ngram_counter.n:\n",
    "            raise ValueError(\"Context too long for this n-gram\")\n",
    "\n",
    "        return tuple(context)\n",
    "\n",
    "    def score(self, word, context):\n",
    "        \"\"\"\n",
    "        For a given string representation of a word, and a string word context,\n",
    "        returns the maximum likelihood score that the word will follow the\n",
    "        context.\n",
    "        \"\"\"\n",
    "        context = self.check_context(context)\n",
    "        return (self.ngram_counter\n",
    "                .allgrams[self.ngram_counter.n][context]\n",
    "                .freq(word))\n",
    "    \n",
    "    def logscore(self, word, context):\n",
    "        \"\"\"\n",
    "        For a given string representation of a word, and a word context,\n",
    "        computes the log probability of this word in this context.\n",
    "        \"\"\"\n",
    "        score = self.score(word, context)\n",
    "        if score == 0.0:\n",
    "            return float(\"-inf\")\n",
    "\n",
    "        return math.log(score, 2)\n",
    "    \n",
    "    def entropy(self, text):\n",
    "        \"\"\"\n",
    "        Calculate the approximate cross-entropy of the n-gram model for a\n",
    "        given text represented as a list of comma-separated strings.\n",
    "        This is the average log probability of each word in the text.\n",
    "        \"\"\"\n",
    "        normed_text = map(self.ngram_counter.check_against_vocab, text)\n",
    "        entropy = processed_ngrams = 0\n",
    "        for ngram in self.ngram_counter.to_ngrams(normed_text):\n",
    "            context, word = tuple(ngram[:-1]), ngram[-1]\n",
    "            entropy += self.logscore(word, context)\n",
    "            processed_ngrams += 1\n",
    "        return -entropy / processed_ngrams\n",
    "    \n",
    "    def perplexity(self, text):\n",
    "        \"\"\"\n",
    "        Given list of comma-separated strings, calculates the perplexity\n",
    "        of the text.\n",
    "        \"\"\"\n",
    "        return 2 ** self.entropy(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eed68852-16b3-4bec-bf6c-a52faefb0e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.18 s, sys: 145 ms, total: 7.33 s\n",
      "Wall time: 7.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trigram_model = BaseNgramModel(count_ngrams(3, vocabulary, sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61cc0704-f295-40e8-a6b3-65c3f7fd73f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.022697232272794028"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_model.score('I', ('<s>', '<s>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf80a2e6-a5e8-4c42-8bc9-c8f6d2763623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.461339805463796"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_model.logscore('I', ('<s>', '<s>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "944c7c69-e1d8-4379-9051-fb1f6d3e2e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8289621948876942"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_model.entropy(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92395a23-78aa-4386-a451-1aa14e187d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.552814082408281"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_model.perplexity(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "761e4d18-53f8-4b4e-be0f-f1bbf27db7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.2 s, sys: 471 ms, total: 13.7 s\n",
      "Wall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fivegram_model = BaseNgramModel(count_ngrams(5, vocabulary, sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b07642b5-3e0b-422d-bd47-02d90f150f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.552814082408281"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_model.perplexity(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5824e819-652b-4ecc-9c9f-95b755309ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.229342823734018"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fivegram_model.perplexity(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aea820-78df-4488-9e73-6c75a36ee1b7",
   "metadata": {},
   "source": [
    "## Unknown Words: Back-off and Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ad254d5-e6c0-4173-86fb-d2a0fa0a100f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddKNgramModel(BaseNgramModel):\n",
    "    \"\"\"\n",
    "    Provides Add-k-smoothed scores.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ngram_counter, k):\n",
    "        \"\"\"\n",
    "        Expects an input value, k, a number by which\n",
    "        to increment word counts during scoring.\n",
    "        \"\"\"\n",
    "        super().__init__(ngram_counter)\n",
    "        self.k = k\n",
    "        self.k_norm = len(self.ngram_counter.vocabulary) * k\n",
    "\n",
    "    def score(self, word, context):\n",
    "        \"\"\"\n",
    "        With Add-k-smoothing, the score is normalized with\n",
    "        a k value.\n",
    "        \"\"\"\n",
    "        context = self.check_context(context)\n",
    "        context_freqdist = (\n",
    "            self.ngram_counter\n",
    "            .allgrams[self.ngram_counter.n][context]\n",
    "        )\n",
    "        word_count = context_freqdist[word]\n",
    "        context_count = context_freqdist.N()\n",
    "        return (word_count + self.k) / \\\n",
    "               (context_count + self.k_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dcf5c1bc-d278-4a57-bb81-5a3283997879",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaplaceNgramModel(AddKNgramModel):\n",
    "    \"\"\"\n",
    "    Implements Laplace (add one) smoothing.\n",
    "    Laplace smoothing is the base case of Add-k smoothing,\n",
    "    with k set to 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, ngram_counter):\n",
    "        super().__init__(ngram_counter, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "080665f9-7a1f-4616-8198-94213aba8c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.84 s, sys: 138 ms, total: 7.98 s\n",
      "Wall time: 7.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "add_one_trigram_model = AddKNgramModel(count_ngrams(3, vocabulary, sentences), k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90440aa4-fec0-43cc-b2d5-6be40c8fb93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.14 s, sys: 160 ms, total: 8.3 s\n",
      "Wall time: 8.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "laplace_trigram_model = LaplaceNgramModel(count_ngrams(3, vocabulary, sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "022980cb-3425-4a94-a69e-30c91f237e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.702185606318513e-05"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 / len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "632b1f0b-8292-40aa-82f6-7d6f4cea0567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.702185606318513e-05"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_one_trigram_model.score('aaa', ('bbb', 'ccc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa6eafe4-9f32-4cbd-b487-82495c4db988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.702185606318513e-05"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laplace_trigram_model.score('aaa', ('bbb', 'ccc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e258e2-6290-4a08-8b16-3784d0d1cb6d",
   "metadata": {},
   "source": [
    "## Language Generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
