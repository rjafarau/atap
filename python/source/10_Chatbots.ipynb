{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd0f38bd-e765-4214-b473-85b0a27bf699",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5153fe70-7397-4d09-b0a4-2b092436cefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import abc\n",
    "import json\n",
    "import time\n",
    "import pprint\n",
    "import pathlib\n",
    "import logging\n",
    "import operator\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9af50c7-6e1b-4ffa-a7d7-63862f935fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import nltk\n",
    "import spacy\n",
    "import pytest\n",
    "import ipytest\n",
    "import inflect\n",
    "import humanize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbbc843d-93ba-4349-94b9-2a8bf325e2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf3d35d-5c48-42af-9ab3-de62f533a570",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b89fe81d-6c74-443f-a461-cfcde7dc3591",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = pathlib.Path('~/project').expanduser()\n",
    "DATA_DIR = PROJECT_DIR / 'data'\n",
    "SPACY_DATA = DATA_DIR / 'spacy_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9506603a-010b-4d8d-8afc-2e501c49d7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONVERSION_PATH = DATA_DIR / 'conversions.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cd3164c-e609-4016-b65f-acfec2214f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "COOKING_CORPUS_ROOT = DATA_DIR / 'cooking_corpus'\n",
    "COOKING_CORPUS_MINI_ROOT = DATA_DIR / 'cooking_corpus_mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79b984d9-e783-401d-89dd-74890b318681",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_ROOT = COOKING_CORPUS_MINI_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5655e8f-2582-4497-b773-f69113b465ab",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d44d79b-4e91-4f59-86f0-84835caeb0cb",
   "metadata": {},
   "source": [
    "## HTMLCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aeb1683-f506-4531-9954-678189a8af1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.api import CorpusReader\n",
    "from nltk.corpus.reader.api import CategorizedCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e12b229a-9e6a-4266-8cea-6e9356745c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from readability.readability import Unparseable\n",
    "from readability.readability import Document as Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c1a95c4-21c5-4204-98b1-cafd4cd8bbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"readability.readability\")\n",
    "logger.disabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10462cf4-da9d-4ff2-b710-6b7125cf4114",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_PATTERN = r'(?!\\.)[a-z_\\s]+/[\\w\\s\\d\\-]+\\.html'\n",
    "CAT_PATTERN = r'([a-z_\\s]+)/.*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c484eb1-ebd2-44d3-a807-f8f5baa27cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tags to extract as paragraphs from the HTML text\n",
    "TAGS = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'li']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b533507e-ff1c-4852-8cd9-e09ba83beb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HTMLCorpusReader(CategorizedCorpusReader, CorpusReader):\n",
    "    \"\"\"\n",
    "    A corpus reader for raw HTML documents to enable preprocessing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, fileids=DOC_PATTERN,\n",
    "                 word_tokenizer=nltk.WordPunctTokenizer(),\n",
    "                 sent_tokenizer=nltk.data.LazyLoader(\n",
    "                     'tokenizers/punkt/english.pickle'\n",
    "                 ),\n",
    "                 pos_tagger=nltk.PerceptronTagger(),\n",
    "                 tags=TAGS, encoding='latin-1', **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the corpus reader.  Categorization arguments\n",
    "        (``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to\n",
    "        the ``CategorizedCorpusReader`` constructor.  The remaining\n",
    "        arguments are passed to the ``CorpusReader`` constructor.\n",
    "        \"\"\"\n",
    "        # Add the default category pattern if not passed into the class.\n",
    "        if not any(key.startswith('cat_') for key in kwargs.keys()):\n",
    "            kwargs['cat_pattern'] = CAT_PATTERN\n",
    "        \n",
    "        # Initialize the NLTK corpus reader objects\n",
    "        CategorizedCorpusReader.__init__(self, kwargs)\n",
    "        CorpusReader.__init__(self, root, fileids, encoding)\n",
    "        \n",
    "        self._word_tokenizer = word_tokenizer\n",
    "        self._sent_tokenizer = sent_tokenizer\n",
    "        self._pos_tagger = pos_tagger\n",
    "        self._tags = tags\n",
    "        \n",
    "    def resolve(self, fileids, categories):\n",
    "        \"\"\"\n",
    "        Returns a list of fileids or categories depending on what is passed\n",
    "        to each internal corpus reader function. Implemented similarly to\n",
    "        the NLTK ``CategorizedPlaintextCorpusReader``.\n",
    "        \"\"\"\n",
    "        if fileids is not None and categories is not None:\n",
    "            raise ValueError(\"Specify fileids or categories, not both\")\n",
    "\n",
    "        if categories is not None:\n",
    "            return self.fileids(categories)\n",
    "        return fileids\n",
    "\n",
    "    def docs(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns the complete text of an HTML document, closing the document\n",
    "        after we are done reading it and yielding it in a memory safe fashion.\n",
    "        \"\"\"\n",
    "        # Resolve the fileids and the categories\n",
    "        fileids = self.resolve(fileids, categories)\n",
    "\n",
    "        # Create a generator, loading one document into memory at a time.\n",
    "        for path, encoding in self.abspaths(fileids, include_encoding=True):\n",
    "            with open(path, 'r', encoding=encoding) as f:\n",
    "                yield f.read()\n",
    "    \n",
    "    def html(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns the HTML content of each document, cleaning it using\n",
    "        the readability-lxml library.\n",
    "        \"\"\"\n",
    "        for doc in self.docs(fileids, categories):\n",
    "            try:\n",
    "                yield Paper(doc).summary()\n",
    "            except Unparseable as e:\n",
    "                print(\"Could not parse HTML: {}\".format(e))\n",
    "                continue\n",
    "\n",
    "    def paras(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Uses BeautifulSoup to parse the paragraphs from the HTML.\n",
    "        \"\"\"\n",
    "        for html in self.html(fileids, categories):\n",
    "            soup = bs4.BeautifulSoup(html, 'lxml')\n",
    "            for element in soup.find_all(self._tags):\n",
    "                yield element.text\n",
    "            soup.decompose()\n",
    "    \n",
    "    def titles(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Uses BeautifulSoup to identify titles from the\n",
    "        head tags within the HTML\n",
    "        \"\"\"\n",
    "        for doc in self.docs(fileids, categories):\n",
    "            soup = bs4.BeautifulSoup(doc, 'lxml')\n",
    "            try:\n",
    "                yield soup.title.text\n",
    "                soup.decompose()\n",
    "            except AttributeError as e:\n",
    "                continue\n",
    "    \n",
    "    def sents(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Uses the built in sentence tokenizer to extract sentences from the\n",
    "        paragraphs. Note that this method uses BeautifulSoup to parse HTML.\n",
    "        \"\"\"\n",
    "        for paragraph in self.paras(fileids, categories):\n",
    "            for sentence in self._sent_tokenizer.tokenize(paragraph):\n",
    "                yield sentence\n",
    "    \n",
    "    def words(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Uses the built in word tokenizer to extract tokens from sentences.\n",
    "        Note that this method uses BeautifulSoup to parse HTML content.\n",
    "        \"\"\"\n",
    "        for sentence in self.sents(fileids, categories):\n",
    "            for word in self._word_tokenizer.tokenize(sentence):\n",
    "                yield word\n",
    "    \n",
    "    def tokenize(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Segments, tokenizes, and tags a document in the corpus.\n",
    "        \"\"\"\n",
    "        for paragraph in self.paras(fileids, categories):\n",
    "            yield [\n",
    "                self._pos_tagger.tag(self._word_tokenizer.tokenize(sentence))\n",
    "                for sentence in self._sent_tokenizer.tokenize(paragraph)\n",
    "            ]\n",
    "    \n",
    "    def sizes(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a list of tuples, the fileid and size on disk of the file.\n",
    "        This function is used to detect oddly large files in the corpus.\n",
    "        \"\"\"\n",
    "        # Resolve the fileids and the categories\n",
    "        fileids = self.resolve(fileids, categories)\n",
    "\n",
    "        # Create a generator, getting every path and computing filesize\n",
    "        for path in self.abspaths(fileids):\n",
    "            yield os.path.getsize(path)\n",
    "    \n",
    "    def describe(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Performs a single pass of the corpus and returns a dictionary with a\n",
    "        variety of metrics concerning the state of the corpus.\n",
    "        \"\"\"\n",
    "        started = time.perf_counter()\n",
    "\n",
    "        # Structures to perform counting.\n",
    "        counts = nltk.FreqDist()\n",
    "        tokens = nltk.FreqDist()\n",
    "\n",
    "        # Perform single pass over paragraphs, tokenize and count\n",
    "        for para in self.paras(fileids, categories):\n",
    "            counts['paras'] += 1\n",
    "\n",
    "            for sent in self._sent_tokenizer.tokenize(para):\n",
    "                counts['sents'] += 1\n",
    "\n",
    "                for word in self._word_tokenizer.tokenize(sent):\n",
    "                    counts['words'] += 1\n",
    "                    tokens[word] += 1\n",
    "\n",
    "        # Compute the number of files and categories in the corpus\n",
    "        n_fileids = len(self.resolve(fileids, categories) or self.fileids())\n",
    "        n_categories = len(self.categories(self.resolve(fileids, categories)))\n",
    "\n",
    "        # Return data structure with information\n",
    "        return {\n",
    "            'files': n_fileids,\n",
    "            'categories': n_categories,\n",
    "            'paras': counts['paras'],\n",
    "            'sents': counts['sents'],\n",
    "            'words': counts['words'],\n",
    "            'vocab': len(tokens),\n",
    "            'lexdiv': counts['words'] / len(tokens),\n",
    "            'ppdoc': counts['paras'] / n_fileids,\n",
    "            'sppar': counts['sents'] / counts['paras'],\n",
    "            'secs': time.perf_counter() - started,\n",
    "        }\n",
    "    \n",
    "    def describes(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a string representation of the describe command.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            \"HTML corpus contains {files:,} files in {categories:,} categories.\\n\"\n",
    "            \"Structured as:\\n\"\n",
    "            \"    {paras:,} paragraphs ({ppdoc:0,.3f} mean paragraphs per file)\\n\"\n",
    "            \"    {sents:,} sentences ({sppar:0,.3f} mean sentences per paragraph).\\n\"\n",
    "            \"Word count of {words:,} with a vocabulary of {vocab:,} \"\n",
    "            \"({lexdiv:0,.3f} lexical diversity).\\n\"\n",
    "            \"Corpus scan took {secs:0,.3f} seconds.\"\n",
    "        ).format(**self.describe(fileids, categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bd810f5-a324-4f6c-9ce4-1730925b8bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML corpus contains 3 files in 1 categories.\n",
      "Structured as:\n",
      "    63 paragraphs (21.000 mean paragraphs per file)\n",
      "    180 sentences (2.857 mean sentences per paragraph).\n",
      "Word count of 3,000 with a vocabulary of 926 (3.240 lexical diversity).\n",
      "Corpus scan took 0.055 seconds.\n"
     ]
    }
   ],
   "source": [
    "corpus = HTMLCorpusReader(CORPUS_ROOT.as_posix())\n",
    "print(corpus.describes())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7d1cdb-ca13-418f-9e32-151d7c31ade7",
   "metadata": {},
   "source": [
    "## HTMLPickledCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1684f982-9adb-417d-b898-1e04615253d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PKL_PATTERN = r'(?!\\.)[a-z_\\s]+/[a-f0-9]+\\.pickle'\n",
    "CAT_PATTERN = r'([a-z_\\s]+)/.*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6314ec78-3f93-4907-b9bc-a3a5930983f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HTMLPickledCorpusReader(CategorizedCorpusReader, CorpusReader):\n",
    "    \n",
    "    def __init__(self, root, fileids=PKL_PATTERN, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the corpus reader.  Categorization arguments\n",
    "        (``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to\n",
    "        the ``CategorizedCorpusReader`` constructor.  The remaining arguments\n",
    "        are passed to the ``CorpusReader`` constructor.\n",
    "        \"\"\"\n",
    "        # Add the default category pattern if not passed into the class.\n",
    "        if not any(key.startswith('cat_') for key in kwargs.keys()):\n",
    "            kwargs['cat_pattern'] = CAT_PATTERN\n",
    "\n",
    "        CategorizedCorpusReader.__init__(self, kwargs)\n",
    "        CorpusReader.__init__(self, root, fileids)\n",
    "    \n",
    "    def resolve(self, fileids, categories):\n",
    "        \"\"\"\n",
    "        Returns a list of fileids or categories depending on what is passed\n",
    "        to each internal corpus reader function. This primarily bubbles up to\n",
    "        the high level ``docs`` method, but is implemented here similar to\n",
    "        the nltk ``CategorizedPlaintextCorpusReader``.\n",
    "        \"\"\"\n",
    "        if fileids is not None and categories is not None:\n",
    "            raise ValueError(\"Specify fileids or categories, not both\")\n",
    "\n",
    "        if categories is not None:\n",
    "            return self.fileids(categories)\n",
    "        return fileids\n",
    "    \n",
    "    def docs(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns the document loaded from a pickled object for every file in\n",
    "        the corpus. Similar to the BaleenCorpusReader, this uses a generator\n",
    "        to achive memory safe iteration.\n",
    "        \"\"\"\n",
    "        # Resolve the fileids and the categories\n",
    "        fileids = self.resolve(fileids, categories)\n",
    "\n",
    "        # Create a generator, loading one document into memory at a time.\n",
    "        for path in self.abspaths(fileids):\n",
    "            with open(path, 'rb') as f:\n",
    "                yield pickle.load(f)\n",
    "    \n",
    "    def titles(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Uses BeautifulSoup to identify titles from the\n",
    "        head tags within the HTML\n",
    "        \"\"\"\n",
    "        for doc in self.docs(fileids, categories):\n",
    "            yield doc['title']\n",
    "    \n",
    "    def tagged_paras(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of paragraphs where each paragraph is a list of\n",
    "        sentences, which is in turn a list of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for doc in self.docs(fileids, categories):\n",
    "            for tagged_para in doc['content']:\n",
    "                yield tagged_para\n",
    "    \n",
    "    def paras(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of paragraphs where each paragraph is a list of\n",
    "        sentences, which is in turn a list of tokens.\n",
    "        \"\"\"\n",
    "        for tagged_para in self.tagged_paras(fileids, categories):\n",
    "            yield [[word for word, tag in tagged_sent]\n",
    "                   for tagged_sent in tagged_para]\n",
    "    \n",
    "    def tagged_sents(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of sentences where each sentence is a list of\n",
    "        (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for tagged_para in self.tagged_paras(fileids, categories):\n",
    "            for tagged_sent in tagged_para:\n",
    "                yield tagged_sent\n",
    "                \n",
    "    def sents(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of sentences where each sentence is a list of\n",
    "        tokens.\n",
    "        \"\"\"\n",
    "        for tagged_sent in self.tagged_sents(fileids, categories):\n",
    "            yield [word for word, tag in tagged_sent]\n",
    "    \n",
    "    def tagged_words(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of (token, tag) tuples.\n",
    "        \"\"\"\n",
    "        for sent in self.tagged_sents(fileids, categories):\n",
    "            for token, tag in sent:\n",
    "                yield token, tag\n",
    "\n",
    "    def words(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a generator of tokens.\n",
    "        \"\"\"\n",
    "        for word, tag in self.tagged_words(fileids, categories):\n",
    "            yield word\n",
    "    \n",
    "    def describe(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Performs a single pass of the corpus and returns a dictionary with a\n",
    "        variety of metrics concerning the state of the corpus.\n",
    "        \"\"\"\n",
    "        started = time.perf_counter()\n",
    "\n",
    "        # Structures to perform counting.\n",
    "        counts = nltk.FreqDist()\n",
    "        tokens = nltk.FreqDist()\n",
    "        \n",
    "        # Perform single pass over paragraphs, tokenize and count\n",
    "        for para in self.tagged_paras(fileids, categories):\n",
    "            counts['paras'] += 1\n",
    "            \n",
    "            for sent in para:\n",
    "                counts['sents'] += 1\n",
    "                \n",
    "                for word, tag in sent:\n",
    "                    counts['words'] += 1\n",
    "                    tokens[word] += 1\n",
    "        \n",
    "        # Compute the number of files and categories in the corpus\n",
    "        n_fileids = len(self.resolve(fileids, categories) or self.fileids())\n",
    "        n_topics  = len(self.categories(self.resolve(fileids, categories)))\n",
    "        \n",
    "        # Return data structure with information\n",
    "        return {\n",
    "            'files': n_fileids,\n",
    "            'categories': n_categories,\n",
    "            'paras': counts['paras'],\n",
    "            'sents': counts['sents'],\n",
    "            'words': counts['words'],\n",
    "            'vocab': len(tokens),\n",
    "            'lexdiv': counts['words'] / len(tokens),\n",
    "            'ppdoc': counts['paras'] / n_fileids,\n",
    "            'sppar': counts['sents'] / counts['paras'],\n",
    "            'secs': time.perf_counter() - started,\n",
    "        }\n",
    "    \n",
    "    def describes(self, fileids=None, categories=None):\n",
    "        \"\"\"\n",
    "        Returns a string representation of the describe command.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            \"HTML corpus contains {files:,} files in {categories:,} categories.\\n\"\n",
    "            \"Structured as:\\n\"\n",
    "            \"    {paras:,} paragraphs ({ppdoc:0,.3f} mean paragraphs per file)\\n\"\n",
    "            \"    {sents:,} sentences ({sppar:0,.3f} mean sentences per paragraph).\\n\"\n",
    "            \"Word count of {words:,} with a vocabulary of {vocab:,} \"\n",
    "            \"({lexdiv:0,.3f} lexical diversity).\\n\"\n",
    "            \"Corpus scan took {secs:0,.3f} seconds.\"\n",
    "        ).format(**self.describe(fileids, categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c465026-fc94-4c0c-8d4d-df94dd080201",
   "metadata": {},
   "source": [
    "# Fundamentals of Conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ae18e7-ce07-4e73-bc44-674f0e7f52d1",
   "metadata": {},
   "source": [
    "## Dialog: A Brief Exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "646098b3-5ec5-4ee3-b4b6-f3a1e9f380c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dialog(abc.ABC):\n",
    "    \"\"\"\n",
    "    A dialog listens for utterances, parses and interprets them, then updates\n",
    "    its internal state. It can then formulate a response on demand.\n",
    "    \"\"\"\n",
    "    def listen(self, text, need_response=True, **kwargs):\n",
    "        \"\"\"\n",
    "        A text utterance is passed in and parsed. It is then passed to the\n",
    "        interpret method to determine how to respond. If a response is\n",
    "        requested, the respond method is used to generate a text response\n",
    "        based on the most recent input and the current Dialog state.\n",
    "        \"\"\"\n",
    "        # Parse the input\n",
    "        sents = self.parse(text)\n",
    "        \n",
    "        # Interpret the input\n",
    "        sents, confidence, kwargs = self.interpret(sents, **kwargs)\n",
    "        \n",
    "        # Determine the response\n",
    "        response = (self.respond(sents, confidence, **kwargs)\n",
    "                    if need_response else None)\n",
    "        \n",
    "        # Return initiative\n",
    "        return response, confidence\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def parse(self, text):\n",
    "        \"\"\"\n",
    "        Every dialog may need its own parsing strategy, some dialogs may need\n",
    "        dependency vs. constituency parses, others may simply require regular\n",
    "        expressions or chunkers.\n",
    "        \"\"\"\n",
    "        return []\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def interpret(self, sents, **kwargs):\n",
    "        \"\"\"\n",
    "        Interprets the utterance passed in as a list of parsed sentences,\n",
    "        updates the internal state of the dialog, computes a confidence of the\n",
    "        interpretation. May also return arguments specific to the response\n",
    "        mechanism.\n",
    "        \"\"\"\n",
    "        return sents, 0.0, kwargs\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def respond(self, sents, confidence, **kwargs):\n",
    "        \"\"\"\n",
    "        Creates a response given the input utterances and the current state of\n",
    "        the dialog, along with any arguments passed in from the listen or the\n",
    "        interpret methods.\n",
    "        \"\"\"\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad9c8cc-d517-4ce2-bee7-6bcd1e67a665",
   "metadata": {},
   "source": [
    "## Maintaining a Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "706b9cac-d987-45e2-b0e9-a222d1b3a9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConversation(Dialog, collections.abc.Sequence):\n",
    "    \"\"\"\n",
    "    This is the most simple version of a conversation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dialogs):\n",
    "        self._dialogs = dialogs\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self._dialogs[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._dialogs)\n",
    "    \n",
    "    def listen(self, text, need_response=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Simply return the best confidence response\n",
    "        \"\"\"        \n",
    "        responses = [dialog.listen(text, need_response, **kwargs)\n",
    "                     for dialog in self._dialogs]\n",
    "        \n",
    "        # Responses is a list of (response, confidence) pairs\n",
    "        return max(responses, key=operator.itemgetter(1))\n",
    "    \n",
    "    def parse(self, text):\n",
    "        \"\"\"\n",
    "        Returns parses for all internal dialogs for debugging\n",
    "        \"\"\"\n",
    "        return [dialog.parse(text)\n",
    "                for dialog in self._dialogs]\n",
    "    \n",
    "    def interpret(self, sents, **kwargs):\n",
    "        \"\"\"\n",
    "        Returns interpretations for all internal dialogs for debugging\n",
    "        \"\"\"\n",
    "        return [dialog.interpret(sents, **kwargs)\n",
    "                for dialog in self._dialogs]\n",
    "\n",
    "    def respond(self, sents, confidence, **kwargs):\n",
    "        \"\"\"\n",
    "        Returns responses for all internal dialogs for debugging\n",
    "        \"\"\"\n",
    "        return [dialog.respond(sents, confidence, **kwargs)\n",
    "                for dialog in self._dialogs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6eae5f-dea9-4d07-ae59-5dc25023cc49",
   "metadata": {},
   "source": [
    "# Rules for Polite Conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db900a7-f90d-4d0c-b1dc-3e62a98adb68",
   "metadata": {},
   "source": [
    "## Greetings and Salutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75e5ba64-c289-4005-93b8-f182005012ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Greeting(Dialog):\n",
    "    \"\"\"\n",
    "    Keeps track of the participants entering or leaving the conversation and\n",
    "    responds with appropriate salutations. This is an example of a rules based\n",
    "    system that keeps track of state and uses regular expressions and logic to\n",
    "    handle the dialog.\n",
    "    \"\"\"\n",
    "    \n",
    "    PATTERNS = {\n",
    "        'greeting': r'hello|hi|hey|good morning|good evening',\n",
    "        'introduction': r'my name is ([a-z\\-\\s]+)',\n",
    "        'goodbye': r'goodbye|bye|ttyl',\n",
    "        'rollcall': r'roll call|who\\'s here?'\n",
    "    }\n",
    "    \n",
    "    def __init__(self, participants=None):\n",
    "        # Participants is a map of user name to real name\n",
    "        self.participants = {}\n",
    "        \n",
    "        if participants is not None:\n",
    "            for participant in participants:\n",
    "                self.participants[participant] = None\n",
    "        \n",
    "        # Compile regular expressions\n",
    "        self._patterns = {\n",
    "            key: re.compile(pattern, re.I)\n",
    "            for key, pattern in self.PATTERNS.items()\n",
    "        }\n",
    "    \n",
    "    def parse(self, text):\n",
    "        \"\"\"\n",
    "        Applies all regular expressions to the text to find matches.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            key: match\n",
    "            for key, pattern in self._patterns.items()\n",
    "            if (match := pattern.search(text))\n",
    "            and match is not None\n",
    "        }\n",
    "    \n",
    "    def interpret(self, sents, **kwargs):\n",
    "        \"\"\"\n",
    "        Takes in parsed matches and determines if the message is an enter,\n",
    "        exit, or name change.\n",
    "        \"\"\"\n",
    "        # Can't do anything with no matches\n",
    "        if len(sents) == 0:\n",
    "            return sents, 0.0, kwargs\n",
    "\n",
    "        # Get username from the participants\n",
    "        user = kwargs.get('user', None)\n",
    "        \n",
    "        # Determine if an introduction has been made\n",
    "        if 'introduction' in sents:\n",
    "            # Get the name from the utterance\n",
    "            name = sents['introduction'].groups()[0]\n",
    "            user = user or name.lower()\n",
    "\n",
    "            # Determine if name has changed\n",
    "            if (user not in self.participants\n",
    "                or self.participants[user] != name):\n",
    "                kwargs['name_changed'] = True\n",
    "            \n",
    "            # Update the participants\n",
    "            self.participants[user] = name\n",
    "            kwargs['user'] = user\n",
    "            \n",
    "        # Determine if a greeting has been made\n",
    "        if 'greeting' in sents:\n",
    "            # If we don't have a name for the user\n",
    "            if user not in self.participants:\n",
    "                kwargs['request_introduction'] = True\n",
    "\n",
    "        # Determine if goodbye has been made\n",
    "        if 'goodbye' in sents and user is not None:\n",
    "            # Remove participant\n",
    "            self.participants.pop(user)\n",
    "            kwargs.pop('user', None)\n",
    "        \n",
    "        # If we've seen anything we're looking for, we're pretty confident\n",
    "        return sents, 1.0, kwargs\n",
    "\n",
    "    def respond(self, sents, confidence, **kwargs):\n",
    "        \"\"\"\n",
    "        Gives a greeting or a goodbye depending on what's appropriate.\n",
    "        \"\"\"\n",
    "        if confidence == 0:\n",
    "            return None\n",
    "        \n",
    "        name = self.participants.get(kwargs.get('user', None), None)\n",
    "        name_changed = kwargs.get('name_changed', False)\n",
    "        request_introduction = kwargs.get('request_introduction', False)\n",
    "        \n",
    "        if 'greeting' in sents or 'introduction' in sents:\n",
    "            if request_introduction:\n",
    "                return \"Hello, what is your name?\"\n",
    "            else:\n",
    "                return \"Hello, {}!\".format(name)\n",
    "            \n",
    "        if 'goodbye' in sents:\n",
    "            return \"Talk to you later!\"\n",
    "        \n",
    "        if 'rollcall' in sents:\n",
    "            people = list(self.participants.values())\n",
    "            \n",
    "            if len(people) > 1:\n",
    "                roster = \", \".join(people[:-1])\n",
    "                roster += \" and {}.\".format(people[-1])\n",
    "                return \"Currently in the conversation are \" + roster\n",
    "            elif len(people) == 1:\n",
    "                return \"It's just you and me right now, {}.\".format(name)\n",
    "            else:\n",
    "                return \"So lonely in here by myself ... wait who is that?\"\n",
    "\n",
    "        raise Exception(\n",
    "            \"expected response to be returned, but could not find rule\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb3bcb60-60ad-4e3c-bda1-fcb16ada484a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, what is your name?\n",
      "Hello, Jake!\n",
      "It's just you and me right now, Jake.\n",
      "Talk to you later!\n"
     ]
    }
   ],
   "source": [
    "dialog = Greeting()\n",
    "# `listen` returns (response, confidence) tuples; just print the response\n",
    "print(dialog.listen(\"Hello!\", user=\"jakevp321\")[0])\n",
    "print(dialog.listen(\"my name is Jake\", user=\"jakevp321\")[0])\n",
    "print(dialog.listen(\"Roll call!\", user=\"jakevp321\")[0])\n",
    "print(dialog.listen(\"Have to go, goodbye!\", user=\"jakevp321\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fafe6a2f-d33c-4a77-9e91-161de740dc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, what is your name?\n",
      "Hello, Jill!\n",
      "It's just you and me right now, None.\n"
     ]
    }
   ],
   "source": [
    "dialog = Greeting()\n",
    "print(dialog.listen(\"hey\", user=\"jillmonger\")[0])\n",
    "print(dialog.listen(\"my name is Jill.\", user=\"jillmonger\")[0])\n",
    "print(dialog.listen(\"who's here?\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e48f17-fde7-42b1-9afa-1de65b099525",
   "metadata": {},
   "source": [
    "## Handling Miscommunication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a20ad25-f248-4185-a56a-ab2bb9554778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
      "platform linux -- Python 3.8.11, pytest-7.4.0, pluggy-1.2.0 -- /home/python/.local/share/virtualenvs/project-onCnT2CZ/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/python/project/source\n",
      "plugins: anyio-3.7.1\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 3 items\n",
      "\n",
      "t_ec61538c48d54aa1bbeb5f53a73d723c.py::TestBaseClasses::test_dialog_abc[Gobbledeguk] \u001b[32mPASSED\u001b[0m\u001b[32m  [ 33%]\u001b[0m\n",
      "t_ec61538c48d54aa1bbeb5f53a73d723c.py::TestBaseClasses::test_dialog_abc[Gibberish] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 66%]\u001b[0m\n",
      "t_ec61538c48d54aa1bbeb5f53a73d723c.py::TestBaseClasses::test_dialog_abc[Wingdings] \u001b[32mPASSED\u001b[0m\u001b[32m    [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m =========================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "class TestBaseClasses(object):\n",
    "    \"\"\"\n",
    "    Tests for the Dialog class\n",
    "    \"\"\"\n",
    "    \n",
    "    @pytest.mark.parametrize(\"text\", [\n",
    "        \"Gobbledeguk\", \"Gibberish\", \"Wingdings\"\n",
    "    ])\n",
    "    def test_dialog_abc(self, text):\n",
    "        \"\"\"\n",
    "        Test the Dialog ABC and the listen method\n",
    "        \"\"\"\n",
    "        class SampleDialog(Dialog):\n",
    "            \n",
    "            def parse(self, text):\n",
    "                return []\n",
    "\n",
    "            def interpret(self, sents):\n",
    "                return sents, 0.0, {}\n",
    "            \n",
    "            def respond(self, sents, confidence):\n",
    "                return None\n",
    "\n",
    "        sample = SampleDialog()\n",
    "        reply, confidence = sample.listen(text)\n",
    "        assert confidence == 0.0\n",
    "        assert reply is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48264dbc-32c5-4cae-b716-3b9a5778c6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
      "platform linux -- Python 3.8.11, pytest-7.4.0, pluggy-1.2.0 -- /home/python/.local/share/virtualenvs/project-onCnT2CZ/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/python/project/source\n",
      "plugins: anyio-3.7.1\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 8 items\n",
      "\n",
      "t_ec61538c48d54aa1bbeb5f53a73d723c.py::TestGreetingDialog::test_greeting_intro[w/ user-Hello!] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n",
      "t_ec61538c48d54aa1bbeb5f53a73d723c.py::TestGreetingDialog::test_greeting_intro[w/ user-hello] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "t_ec61538c48d54aa1bbeb5f53a73d723c.py::TestGreetingDialog::test_greeting_intro[w/ user-hey] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n",
      "t_ec61538c48d54aa1bbeb5f53a73d723c.py::TestGreetingDialog::test_greeting_intro[w/ user-hi] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "t_ec61538c48d54aa1bbeb5f53a73d723c.py::TestGreetingDialog::test_greeting_intro[w/o user-Hello!] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n",
      "t_ec61538c48d54aa1bbeb5f53a73d723c.py::TestGreetingDialog::test_greeting_intro[w/o user-hello] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "t_ec61538c48d54aa1bbeb5f53a73d723c.py::TestGreetingDialog::test_greeting_intro[w/o user-hey] \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n",
      "t_ec61538c48d54aa1bbeb5f53a73d723c.py::TestGreetingDialog::test_greeting_intro[w/o user-hi] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================================== \u001b[32m\u001b[1m8 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m =========================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "class TestGreetingDialog(object):\n",
    "    \"\"\"\n",
    "    Test expected input and responses for the Greeting dialog\n",
    "    \"\"\"\n",
    "\n",
    "    @pytest.mark.parametrize(\"text\", [\"Hello!\", \"hello\", 'hey', 'hi'])\n",
    "    @pytest.mark.parametrize(\"user\", [ \"jay\", None], ids=[\"w/ user\", \"w/o user\"])\n",
    "    def test_greeting_intro(self, user, text):\n",
    "        \"\"\"\n",
    "        Test that an initial greeting requests an introduction\n",
    "        \"\"\"\n",
    "        g = Greeting()\n",
    "        reply, confidence = g.listen(text, user=user)\n",
    "        assert confidence == 1.0\n",
    "        assert reply is not None\n",
    "        assert reply == \"Hello, what is your name?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78995a6d-b49f-493e-8865-c975baba7d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
      "platform linux -- Python 3.8.11, pytest-7.4.0, pluggy-1.2.0 -- /home/python/.local/share/virtualenvs/project-onCnT2CZ/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/python/project/source\n",
      "plugins: anyio-3.7.1\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 4 items\n",
      "\n",
      "t_ec61538c48d54aa1bbeb5f53a73d723c.py::TestGreetingDialog::test_initial_intro[w/ user-My name is Jake] \u001b[33mXPASS\u001b[0m\u001b[33m [ 25%]\u001b[0m\n",
      "t_ec61538c48d54aa1bbeb5f53a73d723c.py::TestGreetingDialog::test_initial_intro[w/ user-Hello, I'm Jake.] \u001b[33mXFAIL\u001b[0m\u001b[33m [ 50%]\u001b[0m\n",
      "t_ec61538c48d54aa1bbeb5f53a73d723c.py::TestGreetingDialog::test_initial_intro[w/o user-My name is Jake] \u001b[33mXPASS\u001b[0m\u001b[33m [ 75%]\u001b[0m\n",
      "t_ec61538c48d54aa1bbeb5f53a73d723c.py::TestGreetingDialog::test_initial_intro[w/o user-Hello, I'm Jake.] \u001b[33mXFAIL\u001b[0m\u001b[33m [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m================================== \u001b[33m\u001b[1m2 xfailed\u001b[0m, \u001b[33m\u001b[1m2 xpassed\u001b[0m\u001b[33m in 0.11s\u001b[0m\u001b[33m ===================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -vv\n",
    "\n",
    "class TestGreetingDialog(object):\n",
    "    \"\"\"\n",
    "    Test expected input and responses for the Greeting dialog\n",
    "    \"\"\"\n",
    "\n",
    "    @pytest.mark.xfail(reason=\"a case that must be handled\")\n",
    "    @pytest.mark.parametrize(\"text\", [\"My name is Jake\", \"Hello, I'm Jake.\"])\n",
    "    @pytest.mark.parametrize(\"user\", [\"jkm\", None], ids=[\"w/ user\", \"w/o user\"])\n",
    "    def test_initial_intro(self, user, text):\n",
    "        \"\"\"\n",
    "        Test an initial introduction without greeting\n",
    "        \"\"\"\n",
    "        g = Greeting()\n",
    "        reply, confidence = g.listen(text, user=user)\n",
    "        assert confidence == 1.0\n",
    "        assert reply is not None\n",
    "        assert reply == \"Hello, Jake!\"\n",
    "\n",
    "        if user is None:\n",
    "            user = 'jake'\n",
    "\n",
    "        assert user in g.participants\n",
    "        assert g.participants[user] == 'Jake'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59278665-eb5d-499a-9f9c-9c4e5e15956a",
   "metadata": {},
   "source": [
    "# Entertaining Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fdc554-6d8b-4f79-8d8f-ddeca1029954",
   "metadata": {},
   "source": [
    "## Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b7f5d8d-682e-4296-bf4a-643923ee3d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_nlp = spacy.load(SPACY_DATA / 'en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ed9b2eb-cd51-4e7d-bbe6-0b900c49e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_displacy_tree(sent):\n",
    "    doc = spacy_nlp(sent)\n",
    "    spacy.displacy.render(doc, style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd269ab1-88ba-44b9-ba85-61b22c7d7ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"072f4568bcd143d8b17a8812631f80d1-0\" class=\"displacy\" width=\"1275\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">How</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">SCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">many</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">teaspoons</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">are</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">tablespoon?</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-072f4568bcd143d8b17a8812631f80d1-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-072f4568bcd143d8b17a8812631f80d1-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-072f4568bcd143d8b17a8812631f80d1-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-072f4568bcd143d8b17a8812631f80d1-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-072f4568bcd143d8b17a8812631f80d1-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-072f4568bcd143d8b17a8812631f80d1-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-072f4568bcd143d8b17a8812631f80d1-0-3\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-072f4568bcd143d8b17a8812631f80d1-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,179.0 L753.0,167.0 737.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-072f4568bcd143d8b17a8812631f80d1-0-4\" stroke-width=\"2px\" d=\"M945,177.0 C945,89.5 1095.0,89.5 1095.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-072f4568bcd143d8b17a8812631f80d1-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,179.0 L937,167.0 953,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-072f4568bcd143d8b17a8812631f80d1-0-5\" stroke-width=\"2px\" d=\"M770,177.0 C770,2.0 1100.0,2.0 1100.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-072f4568bcd143d8b17a8812631f80d1-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1100.0,179.0 L1108.0,167.0 1092.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_displacy_tree('How many teaspoons are in a tablespoon?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb72001d-77ed-4795-8eec-b7023c927e48",
   "metadata": {},
   "source": [
    "## Constituency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b8d067d-c83f-4e0f-98f6-50f78287ec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tree(sent):\n",
    "    \"\"\"\n",
    "    Get the SpaCy dependency tree structure\n",
    "    :param sent: string\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    doc = spacy_nlp(sent)\n",
    "    pprint.pprint(doc.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cba7855f-4bc7-4d9c-8f23-0a126827fccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ents': [],\n",
      " 'sents': [{'end': 39, 'start': 0}],\n",
      " 'text': 'How many teaspoons are in a tablespoon?',\n",
      " 'tokens': [{'dep': 'advmod',\n",
      "             'end': 3,\n",
      "             'head': 1,\n",
      "             'id': 0,\n",
      "             'lemma': 'how',\n",
      "             'morph': '',\n",
      "             'pos': 'SCONJ',\n",
      "             'start': 0,\n",
      "             'tag': 'WRB'},\n",
      "            {'dep': 'amod',\n",
      "             'end': 8,\n",
      "             'head': 2,\n",
      "             'id': 1,\n",
      "             'lemma': 'many',\n",
      "             'morph': 'Degree=Pos',\n",
      "             'pos': 'ADJ',\n",
      "             'start': 4,\n",
      "             'tag': 'JJ'},\n",
      "            {'dep': 'nsubj',\n",
      "             'end': 18,\n",
      "             'head': 3,\n",
      "             'id': 2,\n",
      "             'lemma': 'teaspoon',\n",
      "             'morph': 'Number=Plur',\n",
      "             'pos': 'NOUN',\n",
      "             'start': 9,\n",
      "             'tag': 'NNS'},\n",
      "            {'dep': 'ROOT',\n",
      "             'end': 22,\n",
      "             'head': 3,\n",
      "             'id': 3,\n",
      "             'lemma': 'be',\n",
      "             'morph': 'Mood=Ind|Tense=Pres|VerbForm=Fin',\n",
      "             'pos': 'AUX',\n",
      "             'start': 19,\n",
      "             'tag': 'VBP'},\n",
      "            {'dep': 'prep',\n",
      "             'end': 25,\n",
      "             'head': 3,\n",
      "             'id': 4,\n",
      "             'lemma': 'in',\n",
      "             'morph': '',\n",
      "             'pos': 'ADP',\n",
      "             'start': 23,\n",
      "             'tag': 'IN'},\n",
      "            {'dep': 'det',\n",
      "             'end': 27,\n",
      "             'head': 6,\n",
      "             'id': 5,\n",
      "             'lemma': 'a',\n",
      "             'morph': 'Definite=Ind|PronType=Art',\n",
      "             'pos': 'DET',\n",
      "             'start': 26,\n",
      "             'tag': 'DT'},\n",
      "            {'dep': 'pobj',\n",
      "             'end': 38,\n",
      "             'head': 4,\n",
      "             'id': 6,\n",
      "             'lemma': 'tablespoon',\n",
      "             'morph': 'Number=Sing',\n",
      "             'pos': 'NOUN',\n",
      "             'start': 28,\n",
      "             'tag': 'NN'},\n",
      "            {'dep': 'punct',\n",
      "             'end': 39,\n",
      "             'head': 3,\n",
      "             'id': 7,\n",
      "             'lemma': '?',\n",
      "             'morph': 'PunctType=Peri',\n",
      "             'pos': 'PUNCT',\n",
      "             'start': 38,\n",
      "             'tag': '.'}]}\n"
     ]
    }
   ],
   "source": [
    "spacy_tree('How many teaspoons are in a tablespoon?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed84c0d2-30da-4428-8301-004161f1f6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_spacy_tree(sent):\n",
    "    \"\"\"\n",
    "    Visually inspect the SpaCy dependency tree with nltk.tree\n",
    "    :param sent: string\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    doc = spacy_nlp(sent)\n",
    "    \n",
    "    def token_format(token):\n",
    "        return \"_\".join([token.orth_, token.tag_, token.dep_])\n",
    "\n",
    "    def to_nltk_tree(node):\n",
    "        if node.n_lefts + node.n_rights > 0:\n",
    "            return nltk.Tree(token_format(node),\n",
    "                             [to_nltk_tree(child)\n",
    "                              for child in node.children])\n",
    "        else:\n",
    "            return token_format(node)\n",
    "\n",
    "    return [to_nltk_tree(sent.root) for sent in doc.sents][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71bf60b3-bf85-4fd3-9af1-0e558357880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = nltk_spacy_tree('How many teaspoons are in a tablespoon?')\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed82b72-ce01-47f9-aaa8-f5b4ac6198d3",
   "metadata": {},
   "source": [
    "## Question Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4a87593b-5a3b-40fd-bb75-5fe81798678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_type(sent):\n",
    "    \"\"\"\n",
    "    Try to identify whether the question is about measurements,\n",
    "    recipes, or not a question.\n",
    "    :param sent: string\n",
    "    :return: str response type\n",
    "    \"\"\"\n",
    "    doc = spacy_nlp(sent)\n",
    "    \n",
    "    noun_tags = {'NN', 'NNS', 'NNP', 'NNPS'}\n",
    "    nouns = [token.orth_\n",
    "             for sent in doc.sents\n",
    "             for token in sent\n",
    "             if token.tag_ in noun_tags]\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            # Find wh-adjective and wh-adverb phrases\n",
    "            if token.tag_ == 'WRB':\n",
    "                if token.nbor().tag_ == 'JJ':\n",
    "                    return (\"quantity\", nouns)\n",
    "            # Find wh-noun phrases\n",
    "            elif token.tag_ == 'WP':\n",
    "                # Use pre-trained clusters to return recipes\n",
    "                return (\"recipe\", nouns)\n",
    "    # Todo: try to be conversational using our n-gram language generator?\n",
    "    return (\"default\", nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a58e3604-1959-4ab2-b33a-57f5473d8533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('quantity', ['teaspoons', 'tablespoon'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_type('How many teaspoons are in a tablespoon?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ec764a-4e22-4b86-9e04-2d6d0fc7c1af",
   "metadata": {},
   "source": [
    "## From Tablespoons to Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21315f05-c0a5-405f-8908-0cbd2ab98851",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Converter(Dialog):\n",
    "    \"\"\"\n",
    "    Answers questions about converting units\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, conversion_path=CONVERSION_PATH):\n",
    "        with open(conversion_path, 'r') as f:\n",
    "            self.metrics = json.load(f)\n",
    "        self.inflect = inflect.engine()\n",
    "        self.stemmer = nltk.SnowballStemmer('english')\n",
    "        self.parser = spacy.load(SPACY_DATA / 'en_core_web_sm')\n",
    "    \n",
    "    def parse(self, text):\n",
    "        parse = self.parser(text)\n",
    "        return parse\n",
    "    \n",
    "    def interpret(self, sents, **kwargs):\n",
    "        measures = []\n",
    "        confidence = 0\n",
    "        results = {}\n",
    "        # Make sure there are wh-adverb phrases\n",
    "        if 'WRB' in [token.tag_\n",
    "                     for sent in sents.sents\n",
    "                     for token in sent]:\n",
    "            # If so, increment confidence & traverse sents\n",
    "            confidence += .2\n",
    "            for sent in sents.sents:\n",
    "                for token in sent:\n",
    "                    # Store nouns as target measures\n",
    "                    if token.tag_ in ['NN', 'NNS']:\n",
    "                        measures.append(token.orth_)\n",
    "                    # Store numbers as target quantities\n",
    "                    elif token.tag_ in ['CD']:\n",
    "                        results['quantity'] = token.orth_\n",
    "        \n",
    "            # If both source and destination measures are provided...\n",
    "            if len(measures) == 2:\n",
    "                confidence += .4\n",
    "                # Stem source and dest to remove pluralization\n",
    "                results['dst'], results['src'] = (\n",
    "                    tuple(map(self.stemmer.stem, measures))\n",
    "                )\n",
    "                \n",
    "                # Check to see if they correspond to our lookup table\n",
    "                if results['src'] in self.metrics:\n",
    "                    confidence += .2\n",
    "                    if results['dst'] in self.metrics[results['src']]:\n",
    "                        confidence += .2\n",
    "                        \n",
    "        return results, confidence, kwargs\n",
    "    \n",
    "    def convert(self, src, dst, quantity=1.0):\n",
    "        \"\"\"\n",
    "        Converts from the source unit to the dest unit for the given quantity\n",
    "        of the source unit.\n",
    "        \"\"\"\n",
    "        # Check that we can convert\n",
    "        if dst not in self.metrics:\n",
    "            raise KeyError(f\"cannot convert to '{dst}' units\")\n",
    "        if src not in self.metrics[dst]:\n",
    "            raise KeyError(f\"cannot convert from '{src}' to '{dst}'\")\n",
    "        \n",
    "        return self.metrics[dst][src] * float(quantity), src, dst\n",
    "    \n",
    "    def round(self, num):\n",
    "        num = round(float(num), 4)\n",
    "        return int(num) if num.is_integer() else num\n",
    "    \n",
    "    def pluralize(self, noun, num):\n",
    "        return self.inflect.plural_noun(noun, num)\n",
    "    \n",
    "    def numericalize(self, amt):\n",
    "        if 1e2 < amt < 1e6:\n",
    "            return humanize.intcomma(int(amt))\n",
    "        elif amt >= 1e6:\n",
    "            return humanize.intword(int(amt))\n",
    "        elif isinstance(amt, int) or amt.is_integer():\n",
    "            return humanize.apnumber(int(amt))\n",
    "        else:\n",
    "            return humanize.fractional(amt)\n",
    "    \n",
    "    def respond(self, sents, confidence, **kwargs):\n",
    "        \"\"\"\n",
    "        Response makes use of the humanize and inflect libraries to produce\n",
    "        much more human understandable results.\n",
    "        \"\"\"\n",
    "        if confidence < .5:\n",
    "            return \"I'm sorry, I don't know that one.\"\n",
    "        \n",
    "        try:\n",
    "            quantity = sents.get('quantity', 1)\n",
    "            amount, src, dst = self.convert(**sents)\n",
    "            \n",
    "            # Perform numeric rounding\n",
    "            amount = self.round(amount)\n",
    "            quantity = self.round(quantity)\n",
    "            \n",
    "            # Pluralize\n",
    "            src = self.pluralize(src, quantity)\n",
    "            dst = self.pluralize(dst, amount)\n",
    "            verb = self.inflect.plural_verb('is', amount)\n",
    "            \n",
    "            # Numericalize\n",
    "            quantity = self.numericalize(quantity)\n",
    "            amount = self.numericalize(amount)\n",
    "            \n",
    "            return f'There {verb} {amount} {dst} in {quantity} {src}.'\n",
    "        \n",
    "        except KeyError as e:\n",
    "            return \"I'm sorry I {}\".format(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0093f704-39ad-4349-87ed-4838b496a964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('There are 16 cups in one gallon.', 1.0)\n",
      "('There are 1/8 gallons in two cups.', 1.0)\n",
      "('There are 16 tablespoons in one cup.', 1.0)\n",
      "('There are 160 tablespoons in 10 cups.', 1.0)\n",
      "('There are 1/3 tablespoons in one teaspoon.', 1.0)\n"
     ]
    }
   ],
   "source": [
    "dialog = Converter()\n",
    "print(dialog.listen(\"How many cups are in a gallon?\"))\n",
    "print(dialog.listen(\"How many gallons are in 2 cups?\"))\n",
    "print(dialog.listen(\"How many tablespoons are in a cup?\"))\n",
    "print(dialog.listen(\"How many tablespoons are in 10 cups?\"))\n",
    "print(dialog.listen(\"How many tablespoons are in a teaspoon?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff1c052-9a4a-478e-863b-dc5861a5c300",
   "metadata": {},
   "source": [
    "# Learning to Help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04126e12-b9c9-4a85-abe7-218c98544d76",
   "metadata": {},
   "source": [
    "## Being Neighborly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ac94f2-0f0b-49a8-87f5-159ce3ccbf1f",
   "metadata": {},
   "source": [
    "## Offering Recommendations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
