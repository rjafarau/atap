{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78829259-0341-4bb1-81c1-b05938c87764",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc5d9a83-2e75-46c5-8598-203866f12329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c226d4db-86c6-42f3-8885-9eacabe4a8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45025ce1-f9a7-4594-b174-e5c4eb1d3295",
   "metadata": {},
   "source": [
    "# Words in Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29d41ca3-44ba-411b-aed8-76f05cb85371",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"The elephant sneezed at the sight of potatoes.\",\n",
    "    \"Bats can see via echolocation. See the bat sight sneeze!\",\n",
    "    \"Wondering, she opened the door to the studio.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54e4cdee-308a-436f-bae3-993962d2cfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    stem = nltk.stem.SnowballStemmer('english')\n",
    "    text = text.lower()\n",
    "\n",
    "    for token in nltk.word_tokenize(text):\n",
    "        if token in string.punctuation: continue\n",
    "        yield stem.stem(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fafd5c-1f35-45b6-9a2d-451292bbcc25",
   "metadata": {},
   "source": [
    "## Frequency Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f19f1d-1556-4ab8-a7fd-4b945a7f1ac9",
   "metadata": {},
   "source": [
    "### With NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9775b160-996e-49ab-ab82-52bd862d051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_frequency_vectorize(corpus):\n",
    "\n",
    "    # The NLTK frequency vectorize method\n",
    "    from collections import defaultdict\n",
    "\n",
    "    def vectorize(doc):\n",
    "        features = defaultdict(int)\n",
    "\n",
    "        for token in tokenize(doc):\n",
    "            features[token] += 1\n",
    "\n",
    "        return features\n",
    "\n",
    "    return map(vectorize, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85178798-c94a-4a2a-b8b5-0a4a64a88ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[defaultdict(int,\n",
       "             {'the': 2,\n",
       "              'eleph': 1,\n",
       "              'sneez': 1,\n",
       "              'at': 1,\n",
       "              'sight': 1,\n",
       "              'of': 1,\n",
       "              'potato': 1}),\n",
       " defaultdict(int,\n",
       "             {'bat': 2,\n",
       "              'can': 1,\n",
       "              'see': 2,\n",
       "              'via': 1,\n",
       "              'echoloc': 1,\n",
       "              'the': 1,\n",
       "              'sight': 1,\n",
       "              'sneez': 1}),\n",
       " defaultdict(int,\n",
       "             {'wonder': 1,\n",
       "              'she': 1,\n",
       "              'open': 1,\n",
       "              'the': 2,\n",
       "              'door': 1,\n",
       "              'to': 1,\n",
       "              'studio': 1})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk_frequency_vectorize(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454d71e2-c167-4494-9815-80e2f13eef5f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### In Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3a79eb2-b2dd-42b4-b355-16e219f9978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_frequency_vectorize(corpus):\n",
    "    # The Scikit-Learn frequency vectorize method\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    return vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e42c8b38-9bd0-403b-927d-7616607db85e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 2, 0, 0, 0],\n",
       "        [0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 2, 0, 1, 1, 0, 0, 1, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 2, 1, 0, 1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_frequency_vectorize(corpus).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826e49f4-e2df-4613-9349-640f2513bace",
   "metadata": {},
   "source": [
    "### The Gensim way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d4323ee-357f-43b3-b4b0-c391b36e28ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gensim_frequency_vectorize(corpus):\n",
    "    # The Gensim frequency vectorize method\n",
    "    import gensim\n",
    "    \n",
    "    tokenized_corpus = [list(tokenize(doc)) for doc in corpus]\n",
    "    id2word = gensim.corpora.Dictionary(tokenized_corpus)\n",
    "    return [id2word.doc2bow(doc) for doc in tokenized_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1033483-1008-4f35-90f0-87a3dfd70289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 2)],\n",
       " [(4, 1), (5, 1), (6, 1), (7, 2), (8, 1), (9, 1), (10, 2), (11, 1)],\n",
       " [(6, 2), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1)]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_frequency_vectorize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec7dd40-b88a-470f-82bd-92c610952031",
   "metadata": {},
   "source": [
    "## One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cb3682-652c-4cd6-b4d5-39f4e89c0e22",
   "metadata": {},
   "source": [
    "### With NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4401486-36bc-4b4f-92d7-e0f2869b2838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_one_hot_vectorize(corpus):\n",
    "    # The NLTK one hot vectorize method\n",
    "    def vectorize(doc):\n",
    "        return {\n",
    "            token: True\n",
    "            for token in tokenize(doc)\n",
    "        }\n",
    "\n",
    "    return map(vectorize, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a328c6e-0257-4393-83da-635b04afb831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'the': True,\n",
       "  'eleph': True,\n",
       "  'sneez': True,\n",
       "  'at': True,\n",
       "  'sight': True,\n",
       "  'of': True,\n",
       "  'potato': True},\n",
       " {'bat': True,\n",
       "  'can': True,\n",
       "  'see': True,\n",
       "  'via': True,\n",
       "  'echoloc': True,\n",
       "  'the': True,\n",
       "  'sight': True,\n",
       "  'sneez': True},\n",
       " {'wonder': True,\n",
       "  'she': True,\n",
       "  'open': True,\n",
       "  'the': True,\n",
       "  'door': True,\n",
       "  'to': True,\n",
       "  'studio': True}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk_one_hot_vectorize(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b7df00-bdef-4d5a-8565-14daad577b46",
   "metadata": {},
   "source": [
    "### In Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16e18ffe-16ab-4285-8b13-d98bbd2f24ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_one_hot_vectorize_v0(corpus):\n",
    "    # The Sklearn one hot vectorize method\n",
    "\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.preprocessing import Binarizer\n",
    "\n",
    "    freq    = CountVectorizer()\n",
    "    vectors = freq.fit_transform(corpus)\n",
    "    \n",
    "    onehot  = Binarizer()\n",
    "    vectors = onehot.fit_transform(vectors.toarray())\n",
    "\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "510aaf58-ed5a-4a67-b1ff-d1d576c30ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_one_hot_vectorize_v0(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc855c52-e1a5-48b4-8aaa-94e693efd894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124 µs ± 878 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "sklearn_one_hot_vectorize_v0(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a95d6b9-1558-4038-a83e-fec05a6b376e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_one_hot_vectorize_v1(corpus):\n",
    "    # The Sklearn one hot vectorize method\n",
    "\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "    freq    = CountVectorizer(binary=True)\n",
    "    vectors = freq.fit_transform(corpus)\n",
    "\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "536e173b-343c-46ef-8a7e-7e14a2fd66e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0],\n",
       "        [0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_one_hot_vectorize_v1(corpus).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34dfc32a-d8e5-4ab3-bbbf-2813f6eaf27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.7 µs ± 430 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "sklearn_one_hot_vectorize_v1(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f7c98f-b721-4285-b884-a9a28f3bc96c",
   "metadata": {},
   "source": [
    "### The Gensim way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3391441-e944-4b1c-bc45-b3f9befceff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gensim_one_hot_vectorize(corpus):\n",
    "    # The Gensim one hot vectorize method\n",
    "    import gensim\n",
    "    import numpy as np\n",
    "\n",
    "    corpus  = [list(tokenize(doc)) for doc in corpus]\n",
    "    id2word = gensim.corpora.Dictionary(corpus)\n",
    "\n",
    "    corpus  = [\n",
    "        [(token[0], 1) for token in id2word.doc2bow(doc)]\n",
    "        for doc in corpus\n",
    "    ]\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e59362e-4828-46ff-813b-7147a9226e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)],\n",
       " [(4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1)],\n",
       " [(6, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1)]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_one_hot_vectorize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33de0241-63f5-4397-88b5-64392c68c0fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Term Frequency–Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e653020-9898-45cd-b1d7-c729489208c1",
   "metadata": {},
   "source": [
    "### With NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c769df73-91ad-4b03-96e6-1a536bd8cf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tfidf_vectorize(corpus):\n",
    "\n",
    "    from nltk.text import TextCollection\n",
    "\n",
    "    corpus = [list(tokenize(doc)) for doc in corpus]\n",
    "    texts = TextCollection(corpus)\n",
    "\n",
    "    for doc in corpus:\n",
    "        yield {\n",
    "            term: texts.tf_idf(term, doc)\n",
    "            for term in doc\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec650f83-a9db-46db-b8a2-22fcb14dc3b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'the': 0.0,\n",
       "  'eleph': 0.13732653608351372,\n",
       "  'sneez': 0.05068313851352055,\n",
       "  'at': 0.13732653608351372,\n",
       "  'sight': 0.05068313851352055,\n",
       "  'of': 0.13732653608351372,\n",
       "  'potato': 0.13732653608351372},\n",
       " {'bat': 0.21972245773362198,\n",
       "  'can': 0.10986122886681099,\n",
       "  'see': 0.21972245773362198,\n",
       "  'via': 0.10986122886681099,\n",
       "  'echoloc': 0.10986122886681099,\n",
       "  'the': 0.0,\n",
       "  'sight': 0.04054651081081644,\n",
       "  'sneez': 0.04054651081081644},\n",
       " {'wonder': 0.13732653608351372,\n",
       "  'she': 0.13732653608351372,\n",
       "  'open': 0.13732653608351372,\n",
       "  'the': 0.0,\n",
       "  'door': 0.13732653608351372,\n",
       "  'to': 0.13732653608351372,\n",
       "  'studio': 0.13732653608351372}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk_tfidf_vectorize(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d40ace-e4ad-40d7-bd1a-ddb242132c39",
   "metadata": {},
   "source": [
    "### In Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "062508dd-b0e8-4479-a696-1cc070ba13ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_tfidf_vectorize(corpus):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    tfidf = TfidfVectorizer()\n",
    "    return tfidf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "963a38e4-f1c7-456b-8f9f-e21714d74a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.37867627, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.37867627, 0.37867627, 0.        , 0.37867627,\n",
       "         0.        , 0.        , 0.28799306, 0.        , 0.37867627,\n",
       "         0.        , 0.44730461, 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.30251368, 0.30251368, 0.30251368, 0.        ,\n",
       "         0.30251368, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.60502736, 0.        , 0.23006945, 0.30251368, 0.        ,\n",
       "         0.        , 0.17866945, 0.        , 0.30251368, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.36772387,\n",
       "         0.        , 0.        , 0.        , 0.36772387, 0.        ,\n",
       "         0.        , 0.36772387, 0.        , 0.        , 0.        ,\n",
       "         0.36772387, 0.43436728, 0.36772387, 0.        , 0.36772387]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_tfidf_vectorize(corpus).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507acdb1-afc8-4b92-8452-7d14051c17fb",
   "metadata": {},
   "source": [
    "### The Gensim way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9626e241-da3f-4407-8a54-d3f75762522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gensim_tfidf_vectorize(corpus):\n",
    "    import gensim\n",
    "\n",
    "    corpus  = [list(tokenize(doc)) for doc in corpus]\n",
    "    lexicon = gensim.corpora.Dictionary(corpus)\n",
    "\n",
    "    tfidf   = gensim.models.TfidfModel(dictionary=lexicon, normalize=True)\n",
    "    vectors = [tfidf[lexicon.doc2bow(vector)] for vector in corpus]\n",
    "\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "001f67a6-449a-4f16-9575-4324d8e7fdb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0.4837965208957426),\n",
       "  (1, 0.4837965208957426),\n",
       "  (2, 0.4837965208957426),\n",
       "  (3, 0.4837965208957426),\n",
       "  (4, 0.17855490118826325),\n",
       "  (5, 0.17855490118826325)],\n",
       " [(4, 0.10992597952954358),\n",
       "  (5, 0.10992597952954358),\n",
       "  (7, 0.5956913654963344),\n",
       "  (8, 0.2978456827481672),\n",
       "  (9, 0.2978456827481672),\n",
       "  (10, 0.5956913654963344),\n",
       "  (11, 0.2978456827481672)],\n",
       " [(12, 0.408248290463863),\n",
       "  (13, 0.408248290463863),\n",
       "  (14, 0.408248290463863),\n",
       "  (15, 0.408248290463863),\n",
       "  (16, 0.408248290463863),\n",
       "  (17, 0.408248290463863)]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_tfidf_vectorize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0505fe-f455-4225-a4e4-1bfab3e5ffb8",
   "metadata": {},
   "source": [
    "## Distributed Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394ac4d4-d4c8-482d-bbe2-10bf8d25aab4",
   "metadata": {},
   "source": [
    "### The Gensim way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2aab5621-f8c5-43d9-a41b-25a6fc0d84a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gensim_doc2vec_vectorize(corpus):\n",
    "    from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "\n",
    "    corpus = [list(tokenize(doc)) for doc in corpus]\n",
    "    docs   = [\n",
    "        TaggedDocument(words, ['d{}'.format(idx)])\n",
    "        for idx, words in enumerate(corpus)\n",
    "    ]\n",
    "    model = Doc2Vec(docs, vector_size=5, min_count=0)\n",
    "    return model.dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac218f39-7376-40e3-b3c7-3c6667c27031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.10468323 -0.11976369 -0.19807316  0.17087035  0.0711579 ]\n"
     ]
    }
   ],
   "source": [
    "print(gensim_doc2vec_vectorize(corpus)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
