{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5480bb5-acbe-430f-a4b4-4cfdb4b82fa3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bec33aa4-712e-423a-acf3-a50e839279b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86e170e1-4576-436b-8dcc-e7d834557663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reader import HTMLPickledCorpusReader\n",
    "from transformer import TextNormalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d390e503-e63f-4e76-8266-a14ead12c8f4",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b799eee5-5b54-4b95-8a28-63ad2d96ff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = pathlib.Path('~/project').expanduser()\n",
    "DATA_DIR = PROJECT_DIR / 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc71cda2-22fa-45bc-9eeb-b2472c6e5b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_ROOT = DATA_DIR / 'sample'\n",
    "HOBBIES_ROOT = DATA_DIR / 'hobbies'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d6ac72-ce84-4ace-be1f-0455798d2068",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa22368-75aa-4d1a-bc6b-b6e5e76d11f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## HTMLPickledCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f04ee5d8-b6ee-4174-b8ae-7386bfa42d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML corpus contains 2,538 files in 12 categories.\n",
      "Structured as:\n",
      "    43,922 paragraphs (17.306 mean paragraphs per file)\n",
      "    74,899 sentences (1.705 mean sentences per paragraph).\n",
      "Word count of 1,624,862 with a vocabulary of 58,748 (27.658 lexical diversity).\n",
      "Corpus scan took 1.508 seconds.\n"
     ]
    }
   ],
   "source": [
    "corpus = HTMLPickledCorpusReader(SAMPLE_ROOT.as_posix())\n",
    "print(corpus.describes())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d4be76-b96d-45da-bce7-f5cadff29b76",
   "metadata": {},
   "source": [
    "## TextNormalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee3eda57-d186-4c36-8e72-807a132188ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = HTMLPickledCorpusReader(SAMPLE_ROOT.as_posix())\n",
    "normalizer = TextNormalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1c27456-1460-49e0-a788-d91c87e273a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2538"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(normalizer.fit_transform(corpus.docs())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fc78af-cee0-47a3-9571-f20c0cdaf21e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Python Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fb1306-583f-4aea-97aa-76465ec7f9f3",
   "metadata": {},
   "source": [
    "## Running Tasks in Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fbf5d8e-2767-41e4-8e0d-96c58e0ee51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mp_train import sequential, parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b24ab74c-1706-4034-b5cd-8527301af86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginning sequential tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MainProcess 2024-03-03 18:54:51 naive bayes training took 31.92 seconds with an average score of 0.459\n",
      "MainProcess 2024-03-03 18:56:21 logistic regression training took 90.45 seconds with an average score of 0.570\n",
      "MainProcess 2024-03-03 18:57:24 multilayer perceptron training took 63.13 seconds with an average score of 0.556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total sequential fit time: 185.51 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"beginning sequential tasks\")\n",
    "_, delta = sequential(SAMPLE_ROOT.as_posix())\n",
    "print(\"total sequential fit time: {:0.2f} seconds\".format(delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ff4d3e-44a8-479a-b125-9fb43bd489fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginning parallel tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fit_naive_bayes 2024-03-03 18:58:12 naive bayes training took 48.19 seconds with an average score of 0.459\n",
      "fit_multilayer_perceptron 2024-03-03 18:58:53 multilayer perceptron training took 88.74 seconds with an average score of 0.572\n",
      "fit_logistic_regression 2024-03-03 18:59:17 logistic regression training took 112.90 seconds with an average score of 0.570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parallel fit time: 112.93 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"beginning parallel tasks\")\n",
    "_, delta = parallel(SAMPLE_ROOT.as_posix())\n",
    "print(\"total parallel fit time: {:0.2f} seconds\".format(delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa1a273-d0de-474a-bb7e-670a96e9a273",
   "metadata": {},
   "source": [
    "## Process Pools and Queues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e241b80d-60c8-4e60-b7df-2d4e3eadf940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcpi import mcpi_sequential, mcpi_parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31719252-571a-4811-bc55-c995c6da270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efec3f9b-01a3-4894-90ab-8292423c2ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential pi: 3.1421316 in 1.56 seconds\n"
     ]
    }
   ],
   "source": [
    "pi, delta = mcpi_sequential(N)\n",
    "print(\"sequential pi: {} in {:0.2f} seconds\".format(pi, delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7701f28-0dc0-44f4-98e1-44274f396b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parallel pi: 3.14214 in 0.34 seconds\n"
     ]
    }
   ],
   "source": [
    "pi, delta = mcpi_parallel(N)\n",
    "print(\"parallel pi: {} in {:0.2f} seconds\".format(pi, delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd4b172-8a19-4548-a20e-8212fe753b1b",
   "metadata": {},
   "source": [
    "## Parallel Corpus Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99c6c70d-7a40-482d-89e2-7c44e02b6b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessor import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7f7f220-64e6-4594-8403-a92649eb9315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Preprocessor(object):\n",
      "    \"\"\"\n",
      "    The preprocessor wraps a corpus object (usually a `HTMLCorpusReader`)\n",
      "    and manages the stateful tokenization and part of speech tagging into a\n",
      "    directory that is stored in a format that can be read by the\n",
      "    `HTMLPickledCorpusReader`. This format is more compact and necessarily\n",
      "    removes a variety of fields from the document that are stored in the JSON\n",
      "    representation dumped from the Mongo database. This format however is more\n",
      "    easily accessed for common parsing activity.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, corpus, target=None, **kwargs):\n",
      "        \"\"\"\n",
      "        The corpus is the `HTMLCorpusReader` to preprocess and pickle.\n",
      "        The target is the directory on disk to output the pickled corpus to.\n",
      "        \"\"\"\n",
      "        self.corpus = corpus\n",
      "        self.target = target\n",
      "\n",
      "    @property\n",
      "    def target(self):\n",
      "        return self._target\n",
      "\n",
      "    @target.setter\n",
      "    def target(self, path):\n",
      "        if path is not None:\n",
      "            # Normalize the path and make it absolute\n",
      "            path = os.path.expanduser(path)\n",
      "            path = os.path.expandvars(path)\n",
      "            path = os.path.abspath(path)\n",
      "\n",
      "            if os.path.exists(path):\n",
      "                if not os.path.isdir(path):\n",
      "                    raise ValueError(\n",
      "                        \"Please supply a directory to write preprocessed data to.\"\n",
      "                    )\n",
      "\n",
      "        self._target = path\n",
      "\n",
      "    def titles(self, fileids=None, categories=None):\n",
      "        \"\"\"\n",
      "        Helper function to access the titles of the corpus\n",
      "        \"\"\"\n",
      "        return self.corpus.titles(fileids, categories)\n",
      "\n",
      "    def fileids(self, fileids=None, categories=None):\n",
      "        \"\"\"\n",
      "        Helper function to access the fileids of the corpus\n",
      "        \"\"\"\n",
      "        fileids = self.corpus.resolve(fileids, categories)\n",
      "        return fileids if fileids else self.corpus.fileids()\n",
      "\n",
      "    def abspath(self, fileid):\n",
      "        \"\"\"\n",
      "        Returns the absolute path to the target fileid from the corpus fileid.\n",
      "        \"\"\"\n",
      "        # Find the directory, relative from the corpus root.\n",
      "        parent = os.path.relpath(\n",
      "            os.path.dirname(self.corpus.abspath(fileid)), self.corpus.root\n",
      "        )\n",
      "\n",
      "        # Compute the name part\n",
      "        name = str(uuid.uuid4())\n",
      "\n",
      "        # Create the pickle file extension\n",
      "        basename = name + '.pickle'\n",
      "\n",
      "        # Return the path to the file relative to the target.\n",
      "        return os.path.normpath(os.path.join(self.target, parent, basename))\n",
      "\n",
      "    def replicate(self, source):\n",
      "        \"\"\"\n",
      "        Directly copies all files in the source directory to the root of the\n",
      "        target directory (does not maintain subdirectory structures). Used to\n",
      "        copy over metadata files from the root of the corpus to the target.\n",
      "        \"\"\"\n",
      "        names = [\n",
      "            name for name in os.listdir(source)\n",
      "            if not name.startswith('.')\n",
      "        ]\n",
      "\n",
      "        # Filter out directories and copy files\n",
      "        for name in names:\n",
      "            src = os.path.abspath(os.path.join(source, name))\n",
      "            dst = os.path.abspath(os.path.join(self.target, name))\n",
      "\n",
      "            if os.path.isfile(src):\n",
      "                shutil.copy(src, dst)\n",
      "\n",
      "    def tokenize(self, fileid):\n",
      "        \"\"\"\n",
      "        Segments, tokenizes, and tags a document in the corpus. Returns a\n",
      "        generator of paragraphs, which are lists of sentences, which in turn\n",
      "        are lists of part of speech tagged words.\n",
      "        \"\"\"\n",
      "        return self.corpus.tokenize(fileids=fileid)\n",
      "\n",
      "    def process(self, fileid):\n",
      "        \"\"\"\n",
      "        For a single file does the following preprocessing work:\n",
      "            1. Checks the location on disk to make sure no errors occur.\n",
      "            2. Gets all paragraphs for the given text.\n",
      "            3. Segments the paragraphs with the sent_tokenizer\n",
      "            4. Tokenizes the sentences with the wordpunct_tokenizer\n",
      "            5. Tags the sentences using the default pos_tagger\n",
      "            6. Writes the document as a pickle to the target location.\n",
      "        This method is called multiple times from the transform runner.\n",
      "        \"\"\"\n",
      "        # Compute the outpath to write the file to.\n",
      "        target = self.abspath(fileid)\n",
      "        parent = os.path.dirname(target)\n",
      "\n",
      "        # Make sure the directory exists\n",
      "        if not os.path.exists(parent):\n",
      "            os.makedirs(parent)\n",
      "\n",
      "        # Make sure that the parent is a directory and not a file\n",
      "        if not os.path.isdir(parent):\n",
      "            raise ValueError(\n",
      "                \"Please supply a directory to write preprocessed data to.\"\n",
      "            )\n",
      "\n",
      "        # Create a data structure for the pickle\n",
      "        document = {\n",
      "            'title': list(self.titles(fileid)),\n",
      "            'content': list(self.tokenize(fileid))\n",
      "        }\n",
      "\n",
      "        # Open and serialize the pickle to disk\n",
      "        with open(target, 'wb') as f:\n",
      "            pickle.dump(document, f, pickle.HIGHEST_PROTOCOL)\n",
      "\n",
      "        # Clean up the document\n",
      "        del document\n",
      "\n",
      "        # Return the target fileid\n",
      "        return target\n",
      "\n",
      "    def transform(self, fileids=None, categories=None):\n",
      "        \"\"\"\n",
      "        Transform the wrapped corpus, writing out the segmented, tokenized,\n",
      "        and part of speech tagged corpus as a pickle to the target directory.\n",
      "        This method will also directly copy files that are in the corpus.root\n",
      "        directory that are not matched by the corpus.fileids().\n",
      "        \"\"\"\n",
      "        # Make the target directory if it doesn't already exist\n",
      "        if not os.path.exists(self.target):\n",
      "            os.makedirs(self.target)\n",
      "\n",
      "        # First shutil.copy anything in the root directory.\n",
      "        self.replicate(self.corpus.root)\n",
      "\n",
      "        # Create a multiprocessing pool\n",
      "        with mp.Pool() as pool:\n",
      "            return pool.map(\n",
      "                self.process,\n",
      "                self.fileids(fileids, categories)\n",
      "            )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(Preprocessor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6108f911-825e-4515-a6a7-8a269dafa059",
   "metadata": {},
   "source": [
    "# Cluster Computing with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ffea90-9615-4367-b73f-e71c7dad7507",
   "metadata": {},
   "source": [
    "## Anatomy of a Spark Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff3c65f2-7085-4a23-963f-0fb25f335427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sc_template import confugure_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a745d1c8-0e75-4e5e-a09f-d5161e99bd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = 'My Spark Application'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25ef49b8-82ea-4982-9983-59ef354b4ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/24 21:30:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc, spark = confugure_spark(APP_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05defab8-5891-45b3-a872-3e52a8acf432",
   "metadata": {},
   "source": [
    "## Distributing the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f04e3c6-ba62-45e3-9dda-69ffb6e71238",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = sc.wholeTextFiles((HOBBIES_ROOT / '*' / '*.txt').as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b716554-e218-47a1-a3fb-fb3f3538c669",
   "metadata": {},
   "source": [
    "## RDD Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b9ed855-ce5e-4f12-af45-ca18b2567fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/opt/spark/bin/spark-submit sc_bigramcount.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc96f9b3-0acb-4f4b-87d8-3f0c4552e594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sc_bigramcount import count_labels, count_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c922342-5575-43a8-b9e7-4f0ec5ee5221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label      Count\n",
      "-------  -------\n",
      "books         72\n",
      "cinema       100\n",
      "gaming       128\n",
      "sports       118\n",
      "cooking       30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "count_labels(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ae1faae-3aee-4169-b1d3-8c50a6ae81ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique bigrams: 138204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(_1=Row(_1='From', _2='to'), _2=1)\n"
     ]
    }
   ],
   "source": [
    "count_bigrams(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a918e78e-8937-469c-ba0b-24e160f79eb0",
   "metadata": {},
   "source": [
    "## NLP with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f63d381-530b-4eb2-a727-482640411574",
   "metadata": {},
   "source": [
    "### From Scikit-Learn to MLLib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd7d133-8b7f-4837-a7d0-d756bf8028b8",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58af85c9-8cd0-423a-a80c-2e392318c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/opt/spark/bin/spark-submit sc_vectorization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3d0cf5e-9c13-4de3-9c8d-d9045858dde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sc_vectorization import load_corpus, make_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63c5f752-de12-4637-b92c-9a5492e679b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = load_corpus(\n",
    "    sc, spark,\n",
    "    path=(HOBBIES_ROOT / '*' / '*.txt').as_posix()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30443c5c-b86b-4db4-9eb7-7495043826ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "vectorizer = make_vectorizer()\n",
    "vectorizer = vectorizer.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "146df882-12a6-4e03-b234-4f55d228c23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b3964fe-7a35-411c-bbfe-0f5bc7d8f1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|label|                text|              tokens|     filtered_tokens|           frequency|               tfidf|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|books|\\r\\n\\r\\nFrom \\n\\n...|[, , , , from, , ...|[, , , , , , , , ...|(4096,[38,71,106,...|(4096,[38,71,106,...|\n",
      "|books|The Lonely City b...|[the, lonely, cit...|[lonely, city bri...|(4096,[89,132,156...|(4096,[89,132,156...|\n",
      "|books|\\n\\n\\n\\nRelated P...|[, , , , related,...|[, , , , related,...|(4096,[445,2545,3...|(4096,[445,2545,3...|\n",
      "|books|The first story i...|[the, first, stor...|[first, story, sa...|(4096,[3,27,31,57...|(4096,[3,27,31,57...|\n",
      "|books|by Sonny Liew\\n\\n...|[by, sonny, liew,...|[sonny, liew, , h...|(4096,[315,480,53...|(4096,[315,480,53...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectors.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8162958d-6776-477a-b662-2b818671d3e0",
   "metadata": {},
   "source": [
    "### Text clustering with MLLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f1da1d5-582a-4168-a8f3-3bf50f15f855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/opt/spark/bin/spark-submit sc_clustering.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e056c908-4590-41b0-84a5-d987f510482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sc_clustering import load_corpus, make_clusterer, evaluate_clusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1745dc7f-b643-4644-b3cb-d3ecd9caaec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = load_corpus(\n",
    "    sc, spark,\n",
    "    path=(HOBBIES_ROOT / '*' / '*.txt').as_posix()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0237d0e3-78ad-4764-8d51-9d8cc4bd5fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/24 21:31:07 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "clusterer = make_clusterer()\n",
    "clusterer = clusterer.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4c408d6-f394-4bce-b8db-473e7ccf2166",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clusterer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b362b89c-57cc-4d5f-b034-537bc9a04d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cluster    Size  Terms\n",
      "---------  ------  -------------------------------------------------------------\n",
      "        0       4  piled game,\" dispassion, jimmy (wal) saoirse pitch,\n",
      "        1      10  superwoman. relegation\" grazer bookend arrest, gaimon,\n",
      "        2      30  and 360 tendon. bookend --  toeing\n",
      "        3     110  and toeing in banana-point that 360 for\n",
      "        4      65  and who that in ex-wife banana-point toeing\n",
      "        5      92  that who and water—as lionsgate) tow. banana-point\n",
      "        6      82  who that lionsgate) society’s pseudonym water—as acceptance.\n",
      "        7      34  society’s usefully, lionsgate) three-year trash. pseudonym he\n",
      "        8       3  culminating, and 1880 lyndon -- until, pelicans,\n",
      "        9      18  the and to a of in\n",
      "Sum of square distance to center: 2.697\n",
      "Silhouette with squared euclidean distance: 0.270\n"
     ]
    }
   ],
   "source": [
    "evaluate_clusterer(clusterer, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a48883-5804-46a3-bf2e-03a9be1272d4",
   "metadata": {},
   "source": [
    "### Text classification with MLLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d119640-88aa-4f2b-81c8-380817fde093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/opt/spark/bin/spark-submit sc_classification.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07856e8c-eaab-4893-ba2f-a7c97c88a22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sc_classification import load_corpus, make_classifier, evaluate_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53f36d33-bd50-49dd-82cd-497e018695d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = load_corpus(\n",
    "    sc, spark,\n",
    "    path=(HOBBIES_ROOT / '*' / '*.txt').as_posix()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d29024a0-2f72-42ec-8ebd-856ba99f38cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = corpus.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9abe7930-155d-4a55-b56d-acd723b8bbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = make_classifier()\n",
    "classifier = classifier.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d7a5c9ed-8890-4aa7-bd00-9acd62b6ec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3c8fb1f-0f24-4182-8341-a9b0837c05cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+\n",
      "|prediction|indexedLabel|               tfidf|\n",
      "+----------+------------+--------------------+\n",
      "|       0.0|         3.0|(4096,[54,126,172...|\n",
      "|       0.0|         3.0|(4096,[445,2545,3...|\n",
      "|       0.0|         3.0|(4096,[196,726,76...|\n",
      "|       0.0|         3.0|(4096,[38,71,106,...|\n",
      "|       0.0|         3.0|(4096,[32,55,64,1...|\n",
      "+----------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Accuracy = 0.284\n"
     ]
    }
   ],
   "source": [
    "evaluate_classifier(classifier, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959a3013-a82f-4cf8-8edc-ada3ec66ec1a",
   "metadata": {},
   "source": [
    "### Local fit, global evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f488235-7c30-484f-a69a-9a4e6c2da3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/opt/spark/bin/spark-submit sc_sklearn_sample_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "edb67df3-1129-4753-8e27-48a0f0b61016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2eb82cac-6986-448b-bac5-b1039280e680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sc_sklearn_sample_model import (\n",
    "    load_corpus,\n",
    "    make_vectorizer,\n",
    "    make_accuracy_closure\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33481d1d-40a6-4acc-baf1-fc29ae45ac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = load_corpus(\n",
    "    sc, spark,\n",
    "    path=(HOBBIES_ROOT / '*' / '*.txt').as_posix()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7e87c19-47bd-4c09-83bd-a18ae53be412",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = make_vectorizer()\n",
    "vectorizer = vectorizer.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "879754b4-fcec-4fed-acbd-89ded165eac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d756f709-1381-4297-b92c-7f6146934687",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = (\n",
    "    vectors\n",
    "    .sample(withReplacement=False,\n",
    "            fraction=0.1,\n",
    "            seed=42)\n",
    "    .collect()\n",
    ")\n",
    "X = [row['tfidf'] for row in sample]\n",
    "y = [row['label'] for row in sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "15bd4f21-ef2a-4600-b6b7-20cd60c2128a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AdaBoostClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AdaBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>AdaBoostClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "AdaBoostClassifier()"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = AdaBoostClassifier()\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d6edd1c5-aff2-45bf-b299-bac94e96b188",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = sc.broadcast(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f19e1c5c-85a7-4c26-8e37-892012c111c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = sc.accumulator(0)\n",
    "incorrect = sc.accumulator(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d070b956-1192-49c1-aa45-288aa3090c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = make_accuracy_closure(clf, incorrect, correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cd5b5ea6-5788-4733-a936-256a8ac334a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "vectors.foreachPartition(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8790edfe-69ff-4854-9711-46bb4a2e2702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global accuracy of model was 0.695\n"
     ]
    }
   ],
   "source": [
    "accuracy = correct.value / (correct.value + incorrect.value)\n",
    "print(f'Global accuracy of model was {accuracy:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
