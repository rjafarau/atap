{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5480bb5-acbe-430f-a4b4-4cfdb4b82fa3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bec33aa4-712e-423a-acf3-a50e839279b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86e170e1-4576-436b-8dcc-e7d834557663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reader import HTMLPickledCorpusReader\n",
    "from transformer import TextNormalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d390e503-e63f-4e76-8266-a14ead12c8f4",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b799eee5-5b54-4b95-8a28-63ad2d96ff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = pathlib.Path('~/project').expanduser()\n",
    "DATA_DIR = PROJECT_DIR / 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc71cda2-22fa-45bc-9eeb-b2472c6e5b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_ROOT = DATA_DIR / 'sample'\n",
    "HOBBIES_ROOT = DATA_DIR / 'hobbies'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d6ac72-ce84-4ace-be1f-0455798d2068",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa22368-75aa-4d1a-bc6b-b6e5e76d11f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## HTMLPickledCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f04ee5d8-b6ee-4174-b8ae-7386bfa42d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML corpus contains 2,538 files in 12 categories.\n",
      "Structured as:\n",
      "    43,922 paragraphs (17.306 mean paragraphs per file)\n",
      "    74,899 sentences (1.705 mean sentences per paragraph).\n",
      "Word count of 1,624,862 with a vocabulary of 58,748 (27.658 lexical diversity).\n",
      "Corpus scan took 1.652 seconds.\n"
     ]
    }
   ],
   "source": [
    "corpus = HTMLPickledCorpusReader(SAMPLE_ROOT.as_posix())\n",
    "print(corpus.describes())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d4be76-b96d-45da-bce7-f5cadff29b76",
   "metadata": {},
   "source": [
    "## TextNormalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee3eda57-d186-4c36-8e72-807a132188ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = HTMLPickledCorpusReader(SAMPLE_ROOT.as_posix())\n",
    "normalizer = TextNormalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1c27456-1460-49e0-a788-d91c87e273a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2538"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(normalizer.fit_transform(corpus.docs())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fc78af-cee0-47a3-9571-f20c0cdaf21e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Python Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fb1306-583f-4aea-97aa-76465ec7f9f3",
   "metadata": {},
   "source": [
    "## Running Tasks in Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fbf5d8e-2767-41e4-8e0d-96c58e0ee51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mp_train import sequential, parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b24ab74c-1706-4034-b5cd-8527301af86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginning sequential tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MainProcess 2024-03-03 18:54:51 naive bayes training took 31.92 seconds with an average score of 0.459\n",
      "MainProcess 2024-03-03 18:56:21 logistic regression training took 90.45 seconds with an average score of 0.570\n",
      "MainProcess 2024-03-03 18:57:24 multilayer perceptron training took 63.13 seconds with an average score of 0.556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total sequential fit time: 185.51 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"beginning sequential tasks\")\n",
    "_, delta = sequential(SAMPLE_ROOT.as_posix())\n",
    "print(\"total sequential fit time: {:0.2f} seconds\".format(delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ff4d3e-44a8-479a-b125-9fb43bd489fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginning parallel tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fit_naive_bayes 2024-03-03 18:58:12 naive bayes training took 48.19 seconds with an average score of 0.459\n",
      "fit_multilayer_perceptron 2024-03-03 18:58:53 multilayer perceptron training took 88.74 seconds with an average score of 0.572\n",
      "fit_logistic_regression 2024-03-03 18:59:17 logistic regression training took 112.90 seconds with an average score of 0.570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parallel fit time: 112.93 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"beginning parallel tasks\")\n",
    "_, delta = parallel(SAMPLE_ROOT.as_posix())\n",
    "print(\"total parallel fit time: {:0.2f} seconds\".format(delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa1a273-d0de-474a-bb7e-670a96e9a273",
   "metadata": {},
   "source": [
    "## Process Pools and Queues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e241b80d-60c8-4e60-b7df-2d4e3eadf940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcpi import mcpi_sequential, mcpi_parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31719252-571a-4811-bc55-c995c6da270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efec3f9b-01a3-4894-90ab-8292423c2ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential pi: 3.1421316 in 1.56 seconds\n"
     ]
    }
   ],
   "source": [
    "pi, delta = mcpi_sequential(N)\n",
    "print(\"sequential pi: {} in {:0.2f} seconds\".format(pi, delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7701f28-0dc0-44f4-98e1-44274f396b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parallel pi: 3.14214 in 0.34 seconds\n"
     ]
    }
   ],
   "source": [
    "pi, delta = mcpi_parallel(N)\n",
    "print(\"parallel pi: {} in {:0.2f} seconds\".format(pi, delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd4b172-8a19-4548-a20e-8212fe753b1b",
   "metadata": {},
   "source": [
    "## Parallel Corpus Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99c6c70d-7a40-482d-89e2-7c44e02b6b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessor import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7f7f220-64e6-4594-8403-a92649eb9315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Preprocessor(object):\n",
      "    \"\"\"\n",
      "    The preprocessor wraps a corpus object (usually a `HTMLCorpusReader`)\n",
      "    and manages the stateful tokenization and part of speech tagging into a\n",
      "    directory that is stored in a format that can be read by the\n",
      "    `HTMLPickledCorpusReader`. This format is more compact and necessarily\n",
      "    removes a variety of fields from the document that are stored in the JSON\n",
      "    representation dumped from the Mongo database. This format however is more\n",
      "    easily accessed for common parsing activity.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, corpus, target=None, **kwargs):\n",
      "        \"\"\"\n",
      "        The corpus is the `HTMLCorpusReader` to preprocess and pickle.\n",
      "        The target is the directory on disk to output the pickled corpus to.\n",
      "        \"\"\"\n",
      "        self.corpus = corpus\n",
      "        self.target = target\n",
      "\n",
      "    @property\n",
      "    def target(self):\n",
      "        return self._target\n",
      "\n",
      "    @target.setter\n",
      "    def target(self, path):\n",
      "        if path is not None:\n",
      "            # Normalize the path and make it absolute\n",
      "            path = os.path.expanduser(path)\n",
      "            path = os.path.expandvars(path)\n",
      "            path = os.path.abspath(path)\n",
      "\n",
      "            if os.path.exists(path):\n",
      "                if not os.path.isdir(path):\n",
      "                    raise ValueError(\n",
      "                        \"Please supply a directory to write preprocessed data to.\"\n",
      "                    )\n",
      "\n",
      "        self._target = path\n",
      "\n",
      "    def titles(self, fileids=None, categories=None):\n",
      "        \"\"\"\n",
      "        Helper function to access the titles of the corpus\n",
      "        \"\"\"\n",
      "        return self.corpus.titles(fileids, categories)\n",
      "\n",
      "    def fileids(self, fileids=None, categories=None):\n",
      "        \"\"\"\n",
      "        Helper function to access the fileids of the corpus\n",
      "        \"\"\"\n",
      "        fileids = self.corpus.resolve(fileids, categories)\n",
      "        return fileids if fileids else self.corpus.fileids()\n",
      "\n",
      "    def abspath(self, fileid):\n",
      "        \"\"\"\n",
      "        Returns the absolute path to the target fileid from the corpus fileid.\n",
      "        \"\"\"\n",
      "        # Find the directory, relative from the corpus root.\n",
      "        parent = os.path.relpath(\n",
      "            os.path.dirname(self.corpus.abspath(fileid)), self.corpus.root\n",
      "        )\n",
      "\n",
      "        # Compute the name part\n",
      "        name = str(uuid.uuid4())\n",
      "\n",
      "        # Create the pickle file extension\n",
      "        basename = name + '.pickle'\n",
      "\n",
      "        # Return the path to the file relative to the target.\n",
      "        return os.path.normpath(os.path.join(self.target, parent, basename))\n",
      "\n",
      "    def replicate(self, source):\n",
      "        \"\"\"\n",
      "        Directly copies all files in the source directory to the root of the\n",
      "        target directory (does not maintain subdirectory structures). Used to\n",
      "        copy over metadata files from the root of the corpus to the target.\n",
      "        \"\"\"\n",
      "        names = [\n",
      "            name for name in os.listdir(source)\n",
      "            if not name.startswith('.')\n",
      "        ]\n",
      "\n",
      "        # Filter out directories and copy files\n",
      "        for name in names:\n",
      "            src = os.path.abspath(os.path.join(source, name))\n",
      "            dst = os.path.abspath(os.path.join(self.target, name))\n",
      "\n",
      "            if os.path.isfile(src):\n",
      "                shutil.copy(src, dst)\n",
      "\n",
      "    def tokenize(self, fileid):\n",
      "        \"\"\"\n",
      "        Segments, tokenizes, and tags a document in the corpus. Returns a\n",
      "        generator of paragraphs, which are lists of sentences, which in turn\n",
      "        are lists of part of speech tagged words.\n",
      "        \"\"\"\n",
      "        return self.corpus.tokenize(fileids=fileid)\n",
      "\n",
      "    def process(self, fileid):\n",
      "        \"\"\"\n",
      "        For a single file does the following preprocessing work:\n",
      "            1. Checks the location on disk to make sure no errors occur.\n",
      "            2. Gets all paragraphs for the given text.\n",
      "            3. Segments the paragraphs with the sent_tokenizer\n",
      "            4. Tokenizes the sentences with the wordpunct_tokenizer\n",
      "            5. Tags the sentences using the default pos_tagger\n",
      "            6. Writes the document as a pickle to the target location.\n",
      "        This method is called multiple times from the transform runner.\n",
      "        \"\"\"\n",
      "        # Compute the outpath to write the file to.\n",
      "        target = self.abspath(fileid)\n",
      "        parent = os.path.dirname(target)\n",
      "\n",
      "        # Make sure the directory exists\n",
      "        if not os.path.exists(parent):\n",
      "            os.makedirs(parent)\n",
      "\n",
      "        # Make sure that the parent is a directory and not a file\n",
      "        if not os.path.isdir(parent):\n",
      "            raise ValueError(\n",
      "                \"Please supply a directory to write preprocessed data to.\"\n",
      "            )\n",
      "\n",
      "        # Create a data structure for the pickle\n",
      "        document = {\n",
      "            'title': list(self.titles(fileid)),\n",
      "            'content': list(self.tokenize(fileid))\n",
      "        }\n",
      "\n",
      "        # Open and serialize the pickle to disk\n",
      "        with open(target, 'wb') as f:\n",
      "            pickle.dump(document, f, pickle.HIGHEST_PROTOCOL)\n",
      "\n",
      "        # Clean up the document\n",
      "        del document\n",
      "\n",
      "        # Return the target fileid\n",
      "        return target\n",
      "\n",
      "    def transform(self, fileids=None, categories=None):\n",
      "        \"\"\"\n",
      "        Transform the wrapped corpus, writing out the segmented, tokenized,\n",
      "        and part of speech tagged corpus as a pickle to the target directory.\n",
      "        This method will also directly copy files that are in the corpus.root\n",
      "        directory that are not matched by the corpus.fileids().\n",
      "        \"\"\"\n",
      "        # Make the target directory if it doesn't already exist\n",
      "        if not os.path.exists(self.target):\n",
      "            os.makedirs(self.target)\n",
      "\n",
      "        # First shutil.copy anything in the root directory.\n",
      "        self.replicate(self.corpus.root)\n",
      "\n",
      "        # Create a multiprocessing pool\n",
      "        with mp.Pool() as pool:\n",
      "            return pool.map(\n",
      "                self.process,\n",
      "                self.fileids(fileids, categories)\n",
      "            )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(Preprocessor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6108f911-825e-4515-a6a7-8a269dafa059",
   "metadata": {},
   "source": [
    "# Cluster Computing with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ffea90-9615-4367-b73f-e71c7dad7507",
   "metadata": {},
   "source": [
    "## Anatomy of a Spark Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff3c65f2-7085-4a23-963f-0fb25f335427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sc_template import confugure_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a745d1c8-0e75-4e5e-a09f-d5161e99bd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = 'My Spark Application'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25ef49b8-82ea-4982-9983-59ef354b4ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/22 00:01:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc, spark = confugure_spark(APP_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05defab8-5891-45b3-a872-3e52a8acf432",
   "metadata": {},
   "source": [
    "## Distributing the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f04e3c6-ba62-45e3-9dda-69ffb6e71238",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = sc.wholeTextFiles((HOBBIES_ROOT / '*' / '*.txt').as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b716554-e218-47a1-a3fb-fb3f3538c669",
   "metadata": {},
   "source": [
    "## RDD Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b9ed855-ce5e-4f12-af45-ca18b2567fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/opt/spark/bin/spark-submit sc_bigramcount.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc96f9b3-0acb-4f4b-87d8-3f0c4552e594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sc_bigramcount import count_labels, count_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c922342-5575-43a8-b9e7-4f0ec5ee5221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label      Count\n",
      "-------  -------\n",
      "books         72\n",
      "cinema       100\n",
      "gaming       128\n",
      "sports       118\n",
      "cooking       30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "count_labels(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ae1faae-3aee-4169-b1d3-8c50a6ae81ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique bigrams: 138204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(_1=Row(_1='From', _2='to'), _2=1)\n"
     ]
    }
   ],
   "source": [
    "count_bigrams(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a918e78e-8937-469c-ba0b-24e160f79eb0",
   "metadata": {},
   "source": [
    "## NLP with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f63d381-530b-4eb2-a727-482640411574",
   "metadata": {},
   "source": [
    "### From Scikit-Learn to MLLib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd7d133-8b7f-4837-a7d0-d756bf8028b8",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58af85c9-8cd0-423a-a80c-2e392318c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/opt/spark/bin/spark-submit sc_vectorization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3d0cf5e-9c13-4de3-9c8d-d9045858dde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sc_vectorization import load_corpus, make_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63c5f752-de12-4637-b92c-9a5492e679b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = load_corpus(\n",
    "    sc, spark,\n",
    "    path=(HOBBIES_ROOT / '*' / '*.txt').as_posix()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30443c5c-b86b-4db4-9eb7-7495043826ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "vectorizer = make_vectorizer()\n",
    "vectorizer = vectorizer.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "146df882-12a6-4e03-b234-4f55d228c23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b3964fe-7a35-411c-bbfe-0f5bc7d8f1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|label|                text|              tokens|     filtered_tokens|           frequency|               tfidf|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|books|\\r\\n\\r\\nFrom \\n\\n...|[, , , , from, , ...|[, , , , , , , , ...|(4096,[38,71,106,...|(4096,[38,71,106,...|\n",
      "|books|The Lonely City b...|[the, lonely, cit...|[lonely, city bri...|(4096,[89,132,156...|(4096,[89,132,156...|\n",
      "|books|\\n\\n\\n\\nRelated P...|[, , , , related,...|[, , , , related,...|(4096,[445,2545,3...|(4096,[445,2545,3...|\n",
      "|books|The first story i...|[the, first, stor...|[first, story, sa...|(4096,[3,27,31,57...|(4096,[3,27,31,57...|\n",
      "|books|by Sonny Liew\\n\\n...|[by, sonny, liew,...|[sonny, liew, , h...|(4096,[315,480,53...|(4096,[315,480,53...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectors.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8162958d-6776-477a-b662-2b818671d3e0",
   "metadata": {},
   "source": [
    "### Text clustering with MLLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f1da1d5-582a-4168-a8f3-3bf50f15f855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/opt/spark/bin/spark-submit sc_clustering.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e056c908-4590-41b0-84a5-d987f510482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sc_clustering import load_corpus, make_clusterer, evaluate_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1745dc7f-b643-4644-b3cb-d3ecd9caaec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = load_corpus(\n",
    "    sc, spark,\n",
    "    path=(HOBBIES_ROOT / '*' / '*.txt').as_posix()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0237d0e3-78ad-4764-8d51-9d8cc4bd5fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/22 00:02:08 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "clusterer = make_clusterer()\n",
    "clusterer = clusterer.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4c408d6-f394-4bce-b8db-473e7ccf2166",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clusterer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b362b89c-57cc-4d5f-b034-537bc9a04d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cluster    Size  Terms\n",
      "---------  ------  ----------------------------------------------------------------------------\n",
      "        0       3  glass-cutting mvp friday's and ($3,000) sportsbooks storage\n",
      "        1      18  the and to a of in\n",
      "        2      34  warrior,\" teases more wear—it's blogcast@playstation.sony.com spotrac. loot,\n",
      "        3      74  loot, that backing 12th warrior,\" more (58),\n",
      "        4      73  and transformative backing glow, loot, was 12th\n",
      "        5      91  loot, backing that 12th and record. glow,\n",
      "        6     107  and record. 12th in mcilroy-6 countered. for\n",
      "        7      34  record. countered. and statutes 12th territories? factory\n",
      "        8      10  on-loan  territories? countered. hype statutes imperium,\n",
      "        9       4  yung paused. lab’s 10-on-10 lines. saqlain splatoon\n",
      "Sum of square distance to center: 2.687\n",
      "Silhouette with squared euclidean distance: 0.264\n"
     ]
    }
   ],
   "source": [
    "evaluate_clustering(clusterer, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a48883-5804-46a3-bf2e-03a9be1272d4",
   "metadata": {},
   "source": [
    "### Text classification with MLLib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959a3013-a82f-4cf8-8edc-ada3ec66ec1a",
   "metadata": {},
   "source": [
    "### Local fit, global evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
