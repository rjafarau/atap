{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5480bb5-acbe-430f-a4b4-4cfdb4b82fa3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bec33aa4-712e-423a-acf3-a50e839279b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86e170e1-4576-436b-8dcc-e7d834557663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reader import HTMLPickledCorpusReader\n",
    "from transformer import TextNormalizer, identity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d390e503-e63f-4e76-8266-a14ead12c8f4",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b799eee5-5b54-4b95-8a28-63ad2d96ff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = pathlib.Path('~/project').expanduser()\n",
    "DATA_DIR = PROJECT_DIR / 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc71cda2-22fa-45bc-9eeb-b2472c6e5b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_ROOT = DATA_DIR / 'sample'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d6ac72-ce84-4ace-be1f-0455798d2068",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa22368-75aa-4d1a-bc6b-b6e5e76d11f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## HTMLPickledCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f04ee5d8-b6ee-4174-b8ae-7386bfa42d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML corpus contains 2,538 files in 12 categories.\n",
      "Structured as:\n",
      "    43,922 paragraphs (17.306 mean paragraphs per file)\n",
      "    74,899 sentences (1.705 mean sentences per paragraph).\n",
      "Word count of 1,624,862 with a vocabulary of 58,748 (27.658 lexical diversity).\n",
      "Corpus scan took 2.002 seconds.\n"
     ]
    }
   ],
   "source": [
    "corpus = HTMLPickledCorpusReader(CORPUS_ROOT.as_posix())\n",
    "print(corpus.describes())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d4be76-b96d-45da-bce7-f5cadff29b76",
   "metadata": {},
   "source": [
    "## TextNormalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee3eda57-d186-4c36-8e72-807a132188ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = HTMLPickledCorpusReader(CORPUS_ROOT.as_posix())\n",
    "normalizer = TextNormalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1c27456-1460-49e0-a788-d91c87e273a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2538"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(normalizer.fit_transform(corpus.docs())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fc78af-cee0-47a3-9571-f20c0cdaf21e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Python Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fb1306-583f-4aea-97aa-76465ec7f9f3",
   "metadata": {},
   "source": [
    "## Running Tasks in Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fbf5d8e-2767-41e4-8e0d-96c58e0ee51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mp_train import sequential, parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b24ab74c-1706-4034-b5cd-8527301af86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginning sequential tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MainProcess 2024-03-03 18:54:51 naive bayes training took 31.92 seconds with an average score of 0.459\n",
      "MainProcess 2024-03-03 18:56:21 logistic regression training took 90.45 seconds with an average score of 0.570\n",
      "MainProcess 2024-03-03 18:57:24 multilayer perceptron training took 63.13 seconds with an average score of 0.556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total sequential fit time: 185.51 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"beginning sequential tasks\")\n",
    "_, delta = sequential(CORPUS_ROOT.as_posix())\n",
    "print(\"total sequential fit time: {:0.2f} seconds\".format(delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ff4d3e-44a8-479a-b125-9fb43bd489fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginning parallel tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fit_naive_bayes 2024-03-03 18:58:12 naive bayes training took 48.19 seconds with an average score of 0.459\n",
      "fit_multilayer_perceptron 2024-03-03 18:58:53 multilayer perceptron training took 88.74 seconds with an average score of 0.572\n",
      "fit_logistic_regression 2024-03-03 18:59:17 logistic regression training took 112.90 seconds with an average score of 0.570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parallel fit time: 112.93 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"beginning parallel tasks\")\n",
    "_, delta = parallel(CORPUS_ROOT.as_posix())\n",
    "print(\"total parallel fit time: {:0.2f} seconds\".format(delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa1a273-d0de-474a-bb7e-670a96e9a273",
   "metadata": {},
   "source": [
    "## Process Pools and Queues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e241b80d-60c8-4e60-b7df-2d4e3eadf940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcpi import mcpi_sequential, mcpi_parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31719252-571a-4811-bc55-c995c6da270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efec3f9b-01a3-4894-90ab-8292423c2ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential pi: 3.1421316 in 1.56 seconds\n"
     ]
    }
   ],
   "source": [
    "pi, delta = mcpi_sequential(N)\n",
    "print(\"sequential pi: {} in {:0.2f} seconds\".format(pi, delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7701f28-0dc0-44f4-98e1-44274f396b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parallel pi: 3.14214 in 0.34 seconds\n"
     ]
    }
   ],
   "source": [
    "pi, delta = mcpi_parallel(N)\n",
    "print(\"parallel pi: {} in {:0.2f} seconds\".format(pi, delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd4b172-8a19-4548-a20e-8212fe753b1b",
   "metadata": {},
   "source": [
    "## Parallel Corpus Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99c6c70d-7a40-482d-89e2-7c44e02b6b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessor import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7f7f220-64e6-4594-8403-a92649eb9315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Preprocessor(object):\n",
      "    \"\"\"\n",
      "    The preprocessor wraps a corpus object (usually a `HTMLCorpusReader`)\n",
      "    and manages the stateful tokenization and part of speech tagging into a\n",
      "    directory that is stored in a format that can be read by the\n",
      "    `HTMLPickledCorpusReader`. This format is more compact and necessarily\n",
      "    removes a variety of fields from the document that are stored in the JSON\n",
      "    representation dumped from the Mongo database. This format however is more\n",
      "    easily accessed for common parsing activity.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, corpus, target=None, **kwargs):\n",
      "        \"\"\"\n",
      "        The corpus is the `HTMLCorpusReader` to preprocess and pickle.\n",
      "        The target is the directory on disk to output the pickled corpus to.\n",
      "        \"\"\"\n",
      "        self.corpus = corpus\n",
      "        self.target = target\n",
      "\n",
      "    @property\n",
      "    def target(self):\n",
      "        return self._target\n",
      "\n",
      "    @target.setter\n",
      "    def target(self, path):\n",
      "        if path is not None:\n",
      "            # Normalize the path and make it absolute\n",
      "            path = os.path.expanduser(path)\n",
      "            path = os.path.expandvars(path)\n",
      "            path = os.path.abspath(path)\n",
      "\n",
      "            if os.path.exists(path):\n",
      "                if not os.path.isdir(path):\n",
      "                    raise ValueError(\n",
      "                        \"Please supply a directory to write preprocessed data to.\"\n",
      "                    )\n",
      "\n",
      "        self._target = path\n",
      "\n",
      "    def titles(self, fileids=None, categories=None):\n",
      "        \"\"\"\n",
      "        Helper function to access the titles of the corpus\n",
      "        \"\"\"\n",
      "        return self.corpus.titles(fileids, categories)\n",
      "\n",
      "    def fileids(self, fileids=None, categories=None):\n",
      "        \"\"\"\n",
      "        Helper function to access the fileids of the corpus\n",
      "        \"\"\"\n",
      "        fileids = self.corpus.resolve(fileids, categories)\n",
      "        return fileids if fileids else self.corpus.fileids()\n",
      "\n",
      "    def abspath(self, fileid):\n",
      "        \"\"\"\n",
      "        Returns the absolute path to the target fileid from the corpus fileid.\n",
      "        \"\"\"\n",
      "        # Find the directory, relative from the corpus root.\n",
      "        parent = os.path.relpath(\n",
      "            os.path.dirname(self.corpus.abspath(fileid)), self.corpus.root\n",
      "        )\n",
      "\n",
      "        # Compute the name part\n",
      "        name = str(uuid.uuid4())\n",
      "\n",
      "        # Create the pickle file extension\n",
      "        basename = name + '.pickle'\n",
      "\n",
      "        # Return the path to the file relative to the target.\n",
      "        return os.path.normpath(os.path.join(self.target, parent, basename))\n",
      "\n",
      "    def replicate(self, source):\n",
      "        \"\"\"\n",
      "        Directly copies all files in the source directory to the root of the\n",
      "        target directory (does not maintain subdirectory structures). Used to\n",
      "        copy over metadata files from the root of the corpus to the target.\n",
      "        \"\"\"\n",
      "        names = [\n",
      "            name for name in os.listdir(source)\n",
      "            if not name.startswith('.')\n",
      "        ]\n",
      "\n",
      "        # Filter out directories and copy files\n",
      "        for name in names:\n",
      "            src = os.path.abspath(os.path.join(source, name))\n",
      "            dst = os.path.abspath(os.path.join(self.target, name))\n",
      "\n",
      "            if os.path.isfile(src):\n",
      "                shutil.copy(src, dst)\n",
      "\n",
      "    def tokenize(self, fileid):\n",
      "        \"\"\"\n",
      "        Segments, tokenizes, and tags a document in the corpus. Returns a\n",
      "        generator of paragraphs, which are lists of sentences, which in turn\n",
      "        are lists of part of speech tagged words.\n",
      "        \"\"\"\n",
      "        return self.corpus.tokenize(fileids=fileid)\n",
      "\n",
      "    def process(self, fileid):\n",
      "        \"\"\"\n",
      "        For a single file does the following preprocessing work:\n",
      "            1. Checks the location on disk to make sure no errors occur.\n",
      "            2. Gets all paragraphs for the given text.\n",
      "            3. Segments the paragraphs with the sent_tokenizer\n",
      "            4. Tokenizes the sentences with the wordpunct_tokenizer\n",
      "            5. Tags the sentences using the default pos_tagger\n",
      "            6. Writes the document as a pickle to the target location.\n",
      "        This method is called multiple times from the transform runner.\n",
      "        \"\"\"\n",
      "        # Compute the outpath to write the file to.\n",
      "        target = self.abspath(fileid)\n",
      "        parent = os.path.dirname(target)\n",
      "\n",
      "        # Make sure the directory exists\n",
      "        if not os.path.exists(parent):\n",
      "            os.makedirs(parent)\n",
      "\n",
      "        # Make sure that the parent is a directory and not a file\n",
      "        if not os.path.isdir(parent):\n",
      "            raise ValueError(\n",
      "                \"Please supply a directory to write preprocessed data to.\"\n",
      "            )\n",
      "\n",
      "        # Create a data structure for the pickle\n",
      "        document = {\n",
      "            'title': list(self.titles(fileid)),\n",
      "            'content': list(self.tokenize(fileid))\n",
      "        }\n",
      "\n",
      "        # Open and serialize the pickle to disk\n",
      "        with open(target, 'wb') as f:\n",
      "            pickle.dump(document, f, pickle.HIGHEST_PROTOCOL)\n",
      "\n",
      "        # Clean up the document\n",
      "        del document\n",
      "\n",
      "        # Return the target fileid\n",
      "        return target\n",
      "\n",
      "    def transform(self, fileids=None, categories=None):\n",
      "        \"\"\"\n",
      "        Transform the wrapped corpus, writing out the segmented, tokenized,\n",
      "        and part of speech tagged corpus as a pickle to the target directory.\n",
      "        This method will also directly copy files that are in the corpus.root\n",
      "        directory that are not matched by the corpus.fileids().\n",
      "        \"\"\"\n",
      "        # Make the target directory if it doesn't already exist\n",
      "        if not os.path.exists(self.target):\n",
      "            os.makedirs(self.target)\n",
      "\n",
      "        # First shutil.copy anything in the root directory.\n",
      "        self.replicate(self.corpus.root)\n",
      "\n",
      "        # Create a multiprocessing pool\n",
      "        with mp.Pool() as pool:\n",
      "            return pool.map(\n",
      "                self.process,\n",
      "                self.fileids(fileids, categories)\n",
      "            )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(Preprocessor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6108f911-825e-4515-a6a7-8a269dafa059",
   "metadata": {},
   "source": [
    "# Cluster Computing with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ffea90-9615-4367-b73f-e71c7dad7507",
   "metadata": {},
   "source": [
    "## Anatomy of a Spark Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05defab8-5891-45b3-a872-3e52a8acf432",
   "metadata": {},
   "source": [
    "## Distributing the Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b716554-e218-47a1-a3fb-fb3f3538c669",
   "metadata": {},
   "source": [
    "## RDD Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a918e78e-8937-469c-ba0b-24e160f79eb0",
   "metadata": {},
   "source": [
    "## NLP with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f63d381-530b-4eb2-a727-482640411574",
   "metadata": {},
   "source": [
    "### From Scikit-Learn to MLLib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd7d133-8b7f-4837-a7d0-d756bf8028b8",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8162958d-6776-477a-b662-2b818671d3e0",
   "metadata": {},
   "source": [
    "### Text clustering with MLLib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a48883-5804-46a3-bf2e-03a9be1272d4",
   "metadata": {},
   "source": [
    "### Text classification with MLLib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959a3013-a82f-4cf8-8edc-ada3ec66ec1a",
   "metadata": {},
   "source": [
    "### Local fit, global evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
