{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78829259-0341-4bb1-81c1-b05938c87764",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc5d9a83-2e75-46c5-8598-203866f12329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c226d4db-86c6-42f3-8885-9eacabe4a8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45025ce1-f9a7-4594-b174-e5c4eb1d3295",
   "metadata": {},
   "source": [
    "# Words in Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29d41ca3-44ba-411b-aed8-76f05cb85371",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"The elephant sneezed at the sight of potatoes.\",\n",
    "    \"Bats can see via echolocation. See the bat sight sneeze!\",\n",
    "    \"Wondering, she opened the door to the studio.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54e4cdee-308a-436f-bae3-993962d2cfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    stem = nltk.stem.SnowballStemmer('english')\n",
    "    text = text.lower()\n",
    "\n",
    "    for token in nltk.word_tokenize(text):\n",
    "        if token in string.punctuation: continue\n",
    "        yield stem.stem(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fafd5c-1f35-45b6-9a2d-451292bbcc25",
   "metadata": {},
   "source": [
    "## Frequency Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f19f1d-1556-4ab8-a7fd-4b945a7f1ac9",
   "metadata": {},
   "source": [
    "### With NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9775b160-996e-49ab-ab82-52bd862d051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_frequency_vectorize(corpus):\n",
    "\n",
    "    # The NLTK frequency vectorize method\n",
    "    from collections import defaultdict\n",
    "\n",
    "    def vectorize(doc):\n",
    "        features = defaultdict(int)\n",
    "\n",
    "        for token in tokenize(doc):\n",
    "            features[token] += 1\n",
    "\n",
    "        return features\n",
    "\n",
    "    return map(vectorize, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85178798-c94a-4a2a-b8b5-0a4a64a88ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[defaultdict(int,\n",
       "             {'the': 2,\n",
       "              'eleph': 1,\n",
       "              'sneez': 1,\n",
       "              'at': 1,\n",
       "              'sight': 1,\n",
       "              'of': 1,\n",
       "              'potato': 1}),\n",
       " defaultdict(int,\n",
       "             {'bat': 2,\n",
       "              'can': 1,\n",
       "              'see': 2,\n",
       "              'via': 1,\n",
       "              'echoloc': 1,\n",
       "              'the': 1,\n",
       "              'sight': 1,\n",
       "              'sneez': 1}),\n",
       " defaultdict(int,\n",
       "             {'wonder': 1,\n",
       "              'she': 1,\n",
       "              'open': 1,\n",
       "              'the': 2,\n",
       "              'door': 1,\n",
       "              'to': 1,\n",
       "              'studio': 1})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk_frequency_vectorize(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454d71e2-c167-4494-9815-80e2f13eef5f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### In Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3a79eb2-b2dd-42b4-b355-16e219f9978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_frequency_vectorize(corpus):\n",
    "    # The Scikit-Learn frequency vectorize method\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    return vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e42c8b38-9bd0-403b-927d-7616607db85e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 2, 0, 0, 0],\n",
       "        [0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 2, 0, 1, 1, 0, 0, 1, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 2, 1, 0, 1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_frequency_vectorize(corpus).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826e49f4-e2df-4613-9349-640f2513bace",
   "metadata": {},
   "source": [
    "### The Gensim way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d4323ee-357f-43b3-b4b0-c391b36e28ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gensim_frequency_vectorize(corpus):\n",
    "    # The Gensim frequency vectorize method\n",
    "    import gensim\n",
    "    \n",
    "    tokenized_corpus = [list(tokenize(doc)) for doc in corpus]\n",
    "    id2word = gensim.corpora.Dictionary(tokenized_corpus)\n",
    "    return [id2word.doc2bow(doc) for doc in tokenized_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1033483-1008-4f35-90f0-87a3dfd70289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 2)],\n",
       " [(4, 1), (5, 1), (6, 1), (7, 2), (8, 1), (9, 1), (10, 2), (11, 1)],\n",
       " [(6, 2), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1)]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_frequency_vectorize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec7dd40-b88a-470f-82bd-92c610952031",
   "metadata": {},
   "source": [
    "## One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cb3682-652c-4cd6-b4d5-39f4e89c0e22",
   "metadata": {},
   "source": [
    "### With NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4401486-36bc-4b4f-92d7-e0f2869b2838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_one_hot_vectorize(corpus):\n",
    "    # The NLTK one hot vectorize method\n",
    "    def vectorize(doc):\n",
    "        return {\n",
    "            token: True\n",
    "            for token in tokenize(doc)\n",
    "        }\n",
    "\n",
    "    return map(vectorize, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a328c6e-0257-4393-83da-635b04afb831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'the': True,\n",
       "  'eleph': True,\n",
       "  'sneez': True,\n",
       "  'at': True,\n",
       "  'sight': True,\n",
       "  'of': True,\n",
       "  'potato': True},\n",
       " {'bat': True,\n",
       "  'can': True,\n",
       "  'see': True,\n",
       "  'via': True,\n",
       "  'echoloc': True,\n",
       "  'the': True,\n",
       "  'sight': True,\n",
       "  'sneez': True},\n",
       " {'wonder': True,\n",
       "  'she': True,\n",
       "  'open': True,\n",
       "  'the': True,\n",
       "  'door': True,\n",
       "  'to': True,\n",
       "  'studio': True}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk_one_hot_vectorize(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b7df00-bdef-4d5a-8565-14daad577b46",
   "metadata": {},
   "source": [
    "### In Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16e18ffe-16ab-4285-8b13-d98bbd2f24ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_one_hot_vectorize_v0(corpus):\n",
    "    # The Sklearn one hot vectorize method\n",
    "\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.preprocessing import Binarizer\n",
    "\n",
    "    freq    = CountVectorizer()\n",
    "    vectors = freq.fit_transform(corpus)\n",
    "    \n",
    "    onehot  = Binarizer()\n",
    "    vectors = onehot.fit_transform(vectors.toarray())\n",
    "\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "510aaf58-ed5a-4a67-b1ff-d1d576c30ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_one_hot_vectorize_v0(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc855c52-e1a5-48b4-8aaa-94e693efd894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124 µs ± 878 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "sklearn_one_hot_vectorize_v0(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a95d6b9-1558-4038-a83e-fec05a6b376e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_one_hot_vectorize_v1(corpus):\n",
    "    # The Sklearn one hot vectorize method\n",
    "\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "    freq    = CountVectorizer(binary=True)\n",
    "    vectors = freq.fit_transform(corpus)\n",
    "\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "536e173b-343c-46ef-8a7e-7e14a2fd66e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0],\n",
       "        [0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_one_hot_vectorize_v1(corpus).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34dfc32a-d8e5-4ab3-bbbf-2813f6eaf27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.7 µs ± 430 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "sklearn_one_hot_vectorize_v1(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f7c98f-b721-4285-b884-a9a28f3bc96c",
   "metadata": {},
   "source": [
    "### The Gensim way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3391441-e944-4b1c-bc45-b3f9befceff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gensim_one_hot_vectorize(corpus):\n",
    "    # The Gensim one hot vectorize method\n",
    "    import gensim\n",
    "    import numpy as np\n",
    "\n",
    "    corpus  = [list(tokenize(doc)) for doc in corpus]\n",
    "    id2word = gensim.corpora.Dictionary(corpus)\n",
    "\n",
    "    corpus  = [\n",
    "        [(token[0], 1) for token in id2word.doc2bow(doc)]\n",
    "        for doc in corpus\n",
    "    ]\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e59362e-4828-46ff-813b-7147a9226e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)],\n",
       " [(4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1)],\n",
       " [(6, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1)]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_one_hot_vectorize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33de0241-63f5-4397-88b5-64392c68c0fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Term Frequency–Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e653020-9898-45cd-b1d7-c729489208c1",
   "metadata": {},
   "source": [
    "### With NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c769df73-91ad-4b03-96e6-1a536bd8cf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tfidf_vectorize(corpus):\n",
    "\n",
    "    from nltk.text import TextCollection\n",
    "\n",
    "    corpus = [list(tokenize(doc)) for doc in corpus]\n",
    "    texts = TextCollection(corpus)\n",
    "\n",
    "    for doc in corpus:\n",
    "        yield {\n",
    "            term: texts.tf_idf(term, doc)\n",
    "            for term in doc\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec650f83-a9db-46db-b8a2-22fcb14dc3b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'the': 0.0,\n",
       "  'eleph': 0.13732653608351372,\n",
       "  'sneez': 0.05068313851352055,\n",
       "  'at': 0.13732653608351372,\n",
       "  'sight': 0.05068313851352055,\n",
       "  'of': 0.13732653608351372,\n",
       "  'potato': 0.13732653608351372},\n",
       " {'bat': 0.21972245773362198,\n",
       "  'can': 0.10986122886681099,\n",
       "  'see': 0.21972245773362198,\n",
       "  'via': 0.10986122886681099,\n",
       "  'echoloc': 0.10986122886681099,\n",
       "  'the': 0.0,\n",
       "  'sight': 0.04054651081081644,\n",
       "  'sneez': 0.04054651081081644},\n",
       " {'wonder': 0.13732653608351372,\n",
       "  'she': 0.13732653608351372,\n",
       "  'open': 0.13732653608351372,\n",
       "  'the': 0.0,\n",
       "  'door': 0.13732653608351372,\n",
       "  'to': 0.13732653608351372,\n",
       "  'studio': 0.13732653608351372}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk_tfidf_vectorize(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d40ace-e4ad-40d7-bd1a-ddb242132c39",
   "metadata": {},
   "source": [
    "### In Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "062508dd-b0e8-4479-a696-1cc070ba13ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_tfidf_vectorize(corpus):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    tfidf = TfidfVectorizer()\n",
    "    return tfidf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "963a38e4-f1c7-456b-8f9f-e21714d74a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.37867627, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.37867627, 0.37867627, 0.        , 0.37867627,\n",
       "         0.        , 0.        , 0.28799306, 0.        , 0.37867627,\n",
       "         0.        , 0.44730461, 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.30251368, 0.30251368, 0.30251368, 0.        ,\n",
       "         0.30251368, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.60502736, 0.        , 0.23006945, 0.30251368, 0.        ,\n",
       "         0.        , 0.17866945, 0.        , 0.30251368, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.36772387,\n",
       "         0.        , 0.        , 0.        , 0.36772387, 0.        ,\n",
       "         0.        , 0.36772387, 0.        , 0.        , 0.        ,\n",
       "         0.36772387, 0.43436728, 0.36772387, 0.        , 0.36772387]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_tfidf_vectorize(corpus).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507acdb1-afc8-4b92-8452-7d14051c17fb",
   "metadata": {},
   "source": [
    "### The Gensim way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9626e241-da3f-4407-8a54-d3f75762522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gensim_tfidf_vectorize(corpus):\n",
    "    import gensim\n",
    "\n",
    "    corpus  = [list(tokenize(doc)) for doc in corpus]\n",
    "    lexicon = gensim.corpora.Dictionary(corpus)\n",
    "\n",
    "    tfidf   = gensim.models.TfidfModel(dictionary=lexicon, normalize=True)\n",
    "    vectors = [tfidf[lexicon.doc2bow(vector)] for vector in corpus]\n",
    "\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "001f67a6-449a-4f16-9575-4324d8e7fdb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0.4837965208957426),\n",
       "  (1, 0.4837965208957426),\n",
       "  (2, 0.4837965208957426),\n",
       "  (3, 0.4837965208957426),\n",
       "  (4, 0.17855490118826325),\n",
       "  (5, 0.17855490118826325)],\n",
       " [(4, 0.10992597952954358),\n",
       "  (5, 0.10992597952954358),\n",
       "  (7, 0.5956913654963344),\n",
       "  (8, 0.2978456827481672),\n",
       "  (9, 0.2978456827481672),\n",
       "  (10, 0.5956913654963344),\n",
       "  (11, 0.2978456827481672)],\n",
       " [(12, 0.408248290463863),\n",
       "  (13, 0.408248290463863),\n",
       "  (14, 0.408248290463863),\n",
       "  (15, 0.408248290463863),\n",
       "  (16, 0.408248290463863),\n",
       "  (17, 0.408248290463863)]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_tfidf_vectorize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0505fe-f455-4225-a4e4-1bfab3e5ffb8",
   "metadata": {},
   "source": [
    "## Distributed Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394ac4d4-d4c8-482d-bbe2-10bf8d25aab4",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### The Gensim way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2aab5621-f8c5-43d9-a41b-25a6fc0d84a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gensim_doc2vec_vectorize(corpus):\n",
    "    from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "\n",
    "    corpus = [list(tokenize(doc)) for doc in corpus]\n",
    "    docs   = [\n",
    "        TaggedDocument(words, ['d{}'.format(idx)])\n",
    "        for idx, words in enumerate(corpus)\n",
    "    ]\n",
    "    model = Doc2Vec(docs, vector_size=5, min_count=0)\n",
    "    return model.dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac218f39-7376-40e3-b3c7-3c6667c27031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.10468323 -0.11976369 -0.19807316  0.17087035  0.0711579 ]\n"
     ]
    }
   ],
   "source": [
    "print(gensim_doc2vec_vectorize(corpus)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aca610d-93a9-4f25-ad5c-164e9d1dd006",
   "metadata": {},
   "source": [
    "# The Scikit-Learn API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1b414a-371e-4a92-a056-33a8838fe936",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The BaseEstimator Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "955d830a-2d0c-41ba-bcc8-e9a59d6949dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class Estimator(BaseEstimator):\n",
    "    \n",
    "    def fit(self, X, y=None): \n",
    "        \"\"\"\n",
    "        Accept input data, X, and optional target data, y. Returns self.\n",
    "        \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Accept input data, X and return a vector of predictions for each row.\n",
    "        \"\"\"\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffb9152-e146-40e1-8229-c35faa14a000",
   "metadata": {},
   "source": [
    "## Extending TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b884144-ddf5-4365-8270-90bede7604b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class Transfomer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Learn how to transform data based on input data, X.\n",
    "        \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform X into a new dataset, Xprime and return it.\n",
    "        \"\"\"\n",
    "        return Xprime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d88a82-d2d7-4d52-9e99-3b467a49b2be",
   "metadata": {},
   "source": [
    "### Creating a custom Gensim vectorization transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c02b679-ef9b-4579-a20a-1ba694a70a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.matutils import sparse2full\n",
    "\n",
    "class GensimVectorizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, path=None):\n",
    "        self.path = path\n",
    "        self.id2word = None\n",
    "\n",
    "        self.load()\n",
    "\n",
    "    def load(self):\n",
    "        if os.path.exists(self.path):\n",
    "            self.id2word = Dictionary.load(self.path)\n",
    "\n",
    "    def save(self):\n",
    "        self.id2word.save(self.path)\n",
    "\n",
    "    def fit(self, documents, labels=None):\n",
    "        self.id2word = Dictionary(documents)\n",
    "        self.save()\n",
    "\n",
    "    def transform(self, documents):\n",
    "        for document in documents:\n",
    "            docvec = self.id2word.doc2bow(document)\n",
    "            yield sparse2full(docvec, len(self.id2word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4493e859-bb34-403e-b2a0-a67a6ce688ba",
   "metadata": {},
   "source": [
    "### Creating a custom text normalization transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2b7ecda-9178-48b3-83d9-9c1016fdc079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c72ebecd-31b9-43e3-bf94-c00ce7fa39b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, language='english'):\n",
    "        self.stopwords  = set(sw.words(language))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def is_punct(self, token):\n",
    "        return all(\n",
    "            unicodedata.category(char).startswith('P') for char in token\n",
    "        )\n",
    "\n",
    "    def is_stopword(self, token):\n",
    "        return token.lower() in self.stopwords\n",
    "\n",
    "    def normalize(self, document):\n",
    "        return [\n",
    "            self.lemmatize(token, tag).lower()\n",
    "            for paragraph in document\n",
    "            for sentence in paragraph\n",
    "            for (token, tag) in sentence\n",
    "            if not self.is_punct(token) and not self.is_stopword(token)\n",
    "        ]\n",
    "\n",
    "    def lemmatize(self, token, pos_tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(pos_tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        for document in documents:\n",
    "            yield self.normalize(document[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8439baca-5de2-47a6-b323-12ccb171c96a",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8905e8-ecab-44fd-9a0c-64ca80a213d8",
   "metadata": {},
   "source": [
    "## Pipeline Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e13ea3-7008-4115-a538-f3f6f3e44f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = Pipeline([\n",
    "    ('normalizer', TextNormalizer()),\n",
    "    ('vectorizer', GensimVectorizer()),\n",
    "    ('bayes', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c1e9ba-9da7-4846-8e5f-98add55460b1",
   "metadata": {},
   "source": [
    "## Grid Search for Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed2ab77-45ef-4fc5-82c0-e076b01d5822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "search = GridSearchCV(model, param_grid={\n",
    "    'count__analyzer': ['word', 'char', 'char_wb'],\n",
    "    'count__ngram_range': [(1,1), (1,2), (1,3), (1,4), (1,5), (2,3)],\n",
    "    'onehot__threshold': [0.0, 1.0, 2.0, 3.0],\n",
    "    'bayes__alpha': [0.0, 1.0],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a85723-0424-438a-a2ca-3657c53a4967",
   "metadata": {},
   "source": [
    "## Enriching Feature Extraction with Feature Unions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433bd003-f245-4edc-83b4-5b4bf9cb91a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = Pipeline([\n",
    "    ('parser', HTMLParser()),\n",
    "    ('text_union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "            ('entity_feature', Pipeline([\n",
    "                ('entity_extractor', EntityExtractor()),\n",
    "                ('entity_vect', CountVectorizer()),\n",
    "            ])),\n",
    "            ('keyphrase_feature', Pipeline([\n",
    "                ('keyphrase_extractor', KeyphraseExtractor()),\n",
    "                ('keyphrase_vect', TfidfVectorizer()),\n",
    "            ])),\n",
    "        ],\n",
    "        transformer_weights={\n",
    "            'entity_feature': 0.6,\n",
    "            'keyphrase_feature': 0.2,\n",
    "        }\n",
    "    )),\n",
    "    ('clf', LogisticRegression()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c989ab64-0867-4b97-98bc-f3fa76bd3c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
